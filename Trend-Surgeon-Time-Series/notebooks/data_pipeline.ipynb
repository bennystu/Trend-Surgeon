{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb89729",
   "metadata": {},
   "source": [
    "Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc8e45",
   "metadata": {},
   "source": [
    "# üìä **Trend Surgeon ‚Äî Full Feature Engineering Pipeline**\n",
    "\n",
    "This document describes the **end-to-end data pipeline** used to construct the final, fully aligned, leakage-safe feature dataset for multi-step recursive forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "# üß± **1. Inputs**\n",
    "\n",
    "| Parameter         | Description                                           |\n",
    "| ----------------- | ----------------------------------------------------- |\n",
    "| `TARGET_TICKER`   | Ticker being predicted (e.g. `\"PPH\"`)                 |\n",
    "| `SUPPORT_TICKERS` | Related tickers used as features                      |\n",
    "| `START_DATE`      | Historical start date                                 |\n",
    "| `END_DATE`        | Historical end date                                   |\n",
    "| `HORIZON`         | Number of forecast steps used in recursive prediction |\n",
    "\n",
    "**All data is downloaded directly from Yahoo Finance.**\n",
    "\n",
    "---\n",
    "\n",
    "# üóÉÔ∏è **2. Data Download & Cleaning (Step A)**\n",
    "\n",
    "1. Download OHLCV for target + support tickers.\n",
    "2. Flatten MultiIndex columns.\n",
    "3. Drop `\"Adj Close\"` columns.\n",
    "4. Identify earliest date where **all tickers** have complete OHLCV.\n",
    "5. Trim dataset to only fully-overlapping period.\n",
    "\n",
    "Result:\n",
    "**A clean base dataframe of synchronized OHLCV across all tickers.**\n",
    "\n",
    "---\n",
    "\n",
    "# üîë **3. Feature Registry (Step B)**\n",
    "\n",
    "A global dictionary that stores feature alignment rules:\n",
    "\n",
    "```python\n",
    "feature_registry = {\n",
    "    \"PPH_Return_1d\": \"shift_1\",\n",
    "    \"day_of_week\": \"no_shift\",\n",
    "    \"is_cpi_day\": \"shift_plus_k\",\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Allowed rules:\n",
    "\n",
    "| Rule             | Meaning                                           |\n",
    "| ---------------- | ------------------------------------------------- |\n",
    "| `\"shift_1\"`      | Feature uses OHLCV(t) and must be shifted to t‚àí1  |\n",
    "| `\"no_shift\"`     | Feature known before t (calendar, lagged)         |\n",
    "| `\"shift_plus_k\"` | Future event feature, expanded to t+1 ‚Ä¶ t+HORIZON |\n",
    "\n",
    "This guarantees **explicit control** of temporal alignment and prevents leakage.\n",
    "\n",
    "---\n",
    "\n",
    "# üìà **4. Per-Ticker Technical Features (Step C)**\n",
    "\n",
    "Generated for **each** ticker:\n",
    "\n",
    "### Momentum\n",
    "\n",
    "* Returns (1d, 5d, 10d, 20d)\n",
    "* SMA-10, SMA-50, EMA-20, EMA-50\n",
    "* MACD, MACD signal, MACD histogram\n",
    "* RSI-14\n",
    "* Stochastics (%K, %D)\n",
    "\n",
    "### Volatility\n",
    "\n",
    "* Rolling vol-20\n",
    "* Parkinson\n",
    "* Garman-Klass\n",
    "* Bollinger Band width\n",
    "\n",
    "### Volume\n",
    "\n",
    "* Volume ROC\n",
    "* Volume Z-score\n",
    "* OBV\n",
    "\n",
    "### Entropy\n",
    "\n",
    "* Rolling return energy\n",
    "\n",
    "All of these depend on OHLCV(t) ‚Üí\n",
    "**registered with `\"shift_1\"`**\n",
    "\n",
    "---\n",
    "\n",
    "# üîó **5. Cross-Ticker Features (Step D)**\n",
    "\n",
    "For each support ticker:\n",
    "\n",
    "* Price ratio: `ticker_Close / target_Close`\n",
    "* RS-20: 20-day relative strength\n",
    "* 60-day rolling correlation\n",
    "\n",
    "Also depend on OHLCV(t) ‚Üí\n",
    "**registered `\"shift_1\"`**\n",
    "\n",
    "---\n",
    "\n",
    "# üß¨ **6. PCA Latent Factors (Step E)**\n",
    "\n",
    "PCA on all tickers‚Äô 1-day returns produces:\n",
    "\n",
    "* `PCA_1`\n",
    "* `PCA_2`\n",
    "* `PCA_3`\n",
    "\n",
    "Captures market-level latent structure.\n",
    "Depends on returns(t) ‚Üí\n",
    "**registered `\"shift_1\"`**\n",
    "\n",
    "---\n",
    "\n",
    "# üìÖ **7. Calendar & Macro-Event Features (Step F)**\n",
    "\n",
    "### Calendar Basics\n",
    "\n",
    "`day_of_week`, `day_of_month`, `month`, `quarter`,\n",
    "`is_month_end`, `is_month_start`, `is_year_end`\n",
    "‚Üí **registered `\"no_shift\"`**\n",
    "\n",
    "### Holidays\n",
    "\n",
    "`is_holiday_adjacent`\n",
    "‚Üí **`\"no_shift\"`**\n",
    "\n",
    "### OPEX Week\n",
    "\n",
    "‚Üí **`\"no_shift\"`**\n",
    "\n",
    "### FOMC Week\n",
    "\n",
    "‚Üí **`\"no_shift\"`**\n",
    "\n",
    "### Macro-Events\n",
    "\n",
    "Day-level & week-level flags:\n",
    "\n",
    "* `is_cpi_day`\n",
    "* `is_nfp_day`\n",
    "* `is_ppi_day`\n",
    "* `is_gdp_day`\n",
    "* ‚Ä¶and their `_week` variants\n",
    "\n",
    "‚Üí **registered `\"shift_plus_k\"`**\n",
    "\n",
    "Later expanded into:\n",
    "\n",
    "```\n",
    "is_cpi_day_t+1\n",
    "is_cpi_day_t+2\n",
    "...\n",
    "is_cpi_day_t+HORIZON\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üïí **8. Shift Engine (Step G)**\n",
    "\n",
    "Applies temporal alignment **after** all features are created:\n",
    "\n",
    "### `\"shift_1\"`\n",
    "\n",
    "‚Üí `df[col] = df[col].shift(1)`\n",
    "\n",
    "### `\"no_shift\"`\n",
    "\n",
    "‚Üí unchanged\n",
    "\n",
    "### `\"shift_plus_k\"`\n",
    "\n",
    "‚Üí autocreates k=1..HORIZON shifted columns:\n",
    "\n",
    "```\n",
    "col_t+1 = df[col].shift(1)\n",
    "col_t+2 = df[col].shift(2)\n",
    "...\n",
    "col_t+HORIZON = df[col].shift(HORIZON)\n",
    "```\n",
    "\n",
    "Guarantees **zero leakage** and proper alignment for recursive multi-step forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "# üìú **9. Markdown Documentation Generator (Step H)**\n",
    "\n",
    "Optional step that exports a table:\n",
    "\n",
    "| Feature | Rule | Resulting Columns |\n",
    "| ------- | ---- | ----------------- |\n",
    "\n",
    "Uses registry + horizon to show how each feature is aligned.\n",
    "\n",
    "Pure documentation ‚Äî not used by code.\n",
    "\n",
    "---\n",
    "\n",
    "# üß© **10. Master Entry Function**\n",
    "\n",
    "All pieces are tied together in:\n",
    "\n",
    "```python\n",
    "build_feature_dataset(\n",
    "    target=\"PPH\",\n",
    "    support_tickers=[...],\n",
    "    start_date=\"2011-01-04\",\n",
    "    end_date=\"2025-12-01\",\n",
    "    horizon=30\n",
    ")\n",
    "```\n",
    "\n",
    "What it does:\n",
    "\n",
    "1. Download & clean data\n",
    "2. Trim to earliest common date\n",
    "3. Run technicals\n",
    "4. Run cross-ticker features\n",
    "5. Run PCA\n",
    "6. Add calendar + macro events\n",
    "7. Apply shift engine\n",
    "8. (Optional) write markdown\n",
    "9. Return final feature matrix\n",
    "\n",
    "---\n",
    "\n",
    "# üì¶ **Final Output**\n",
    "\n",
    "A fully aligned, leakage-safe dataframe with:\n",
    "\n",
    "* Target ticker\n",
    "* Support tickers\n",
    "* All technical features\n",
    "* Cross-ticker features\n",
    "* PCA factors\n",
    "* Calendar features\n",
    "* Holiday effects\n",
    "* Macro event ladders (t+1 ‚Üí t+HORIZON)\n",
    "* Proper shifts applied\n",
    "* Documentation table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72445f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5976a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP A ‚Äî RAW DATA PREPARATION WITH SAFE WINDOW EXPANSION\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _compute_safe_window(start_date, end_date, min_history=20, horizon=30):\n",
    "    \"\"\"\n",
    "    Expands the user's requested date range so that:\n",
    "      - rolling windows (SMA50, corr60, etc.) have enough initial history\n",
    "      - shift_plus_k has future coverage\n",
    "    \"\"\"\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end   = pd.to_datetime(end_date)\n",
    "\n",
    "    safe_start = start - pd.Timedelta(days=int(min_history * 2.5))\n",
    "    safe_end   = end + pd.Timedelta(days=horizon + 10)\n",
    "\n",
    "    return safe_start, safe_end\n",
    "\n",
    "\n",
    "def download_and_prepare_data(target, support_tickers,\n",
    "                              start_date, end_date,\n",
    "                              min_history=20, horizon=30):\n",
    "    \"\"\"\n",
    "    Combines:\n",
    "      ‚úî Your original OHLCV preparation logic\n",
    "      ‚úî Automatic expansion of the date window\n",
    "      ‚úî Clear validation errors if the requested date range is unavailable\n",
    "      ‚úî Full safe dataset for all later steps\n",
    "    \"\"\"\n",
    "\n",
    "    tickers = [target] + list(support_tickers)\n",
    "    logger.info(f\"Downloading data for: {tickers}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1. Compute expanded safe window\n",
    "    # ------------------------------------------------------------\n",
    "    safe_start, safe_end = _compute_safe_window(start_date, end_date,\n",
    "                                                min_history, horizon)\n",
    "\n",
    "    logger.info(f\"Safe fetch window: {safe_start.date()} ‚Üí {safe_end.date()}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2. Download from yfinance inside safe window\n",
    "    # ------------------------------------------------------------\n",
    "    raw = yf.download(\n",
    "        tickers=tickers,\n",
    "        start=safe_start.strftime(\"%Y-%m-%d\"),\n",
    "        end=safe_end.strftime(\"%Y-%m-%d\"),\n",
    "        auto_adjust=False,\n",
    "        group_by=\"ticker\",\n",
    "        progress=False\n",
    "    )\n",
    "\n",
    "    if raw.empty:\n",
    "        raise ValueError(\"‚ùå yfinance returned no data.\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3. Flatten columns\n",
    "    # ------------------------------------------------------------\n",
    "    raw.columns = [f\"{t}_{f}\" for t, f in raw.columns]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4. Drop Adj Close if any\n",
    "    # ------------------------------------------------------------\n",
    "    raw = raw[[c for c in raw.columns if \"Adj_Close\" not in c and \"Adj Close\" not in c]]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 5. Ensure datetime index\n",
    "    # ------------------------------------------------------------\n",
    "    if \"date\" in raw.columns:\n",
    "        raw[\"date\"] = pd.to_datetime(raw[\"date\"], errors=\"coerce\")\n",
    "        raw = raw.set_index(\"date\")\n",
    "\n",
    "    raw.index = pd.to_datetime(raw.index)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 6. Validate that requested range is covered\n",
    "    # ------------------------------------------------------------\n",
    "    earliest = raw.index.min()\n",
    "    latest   = raw.index.max()\n",
    "\n",
    "    req_start = pd.to_datetime(start_date)\n",
    "    req_end   = pd.to_datetime(end_date)\n",
    "\n",
    "    if earliest > req_start:\n",
    "        raise ValueError(\n",
    "            f\"‚ùå Requested start {req_start.date()} is too early.\\n\"\n",
    "            f\"   Earliest fetched data is {earliest.date()}\"\n",
    "        )\n",
    "\n",
    "    if latest < req_end:\n",
    "        raise ValueError(\n",
    "            f\"‚ùå Requested end {req_end.date()} is too late.\\n\"\n",
    "            f\"   Latest fetched data is {latest.date()}\"\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 7. DO NOT trim early ‚Äî keep full safe window\n",
    "    #    Feature generation needs the earlier history!\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    logger.info(f\"Data prepared. Shape after safe fetch: {raw.shape}\")\n",
    "\n",
    "    return raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271af16",
   "metadata": {},
   "source": [
    "**Test of Step A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461366e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data for: ['PPH', 'XPH', 'IHE', 'IBB', 'XBI', 'XLV', 'VHT', 'SPY', 'VIXY']\n",
      "Safe fetch window: 2010-11-15 ‚Üí 2026-01-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Running Step A ‚Äî download_and_prepare_data\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data prepared. Shape after safe fetch: (3786, 45)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> DONE. Output summary:\n",
      "\n",
      "Shape: (3786, 45)\n",
      "\n",
      "Index dtype: datetime64[ns]\n",
      "\n",
      "Columns: ['VIXY_Open', 'VIXY_High', 'VIXY_Low', 'VIXY_Close', 'VIXY_Volume', 'XPH_Open', 'XPH_High', 'XPH_Low', 'XPH_Close', 'XPH_Volume', 'XBI_Open', 'XBI_High', 'XBI_Low', 'XBI_Close', 'XBI_Volume', 'VHT_Open', 'VHT_High', 'VHT_Low', 'VHT_Close', 'VHT_Volume'] ...\n",
      "\n",
      "\n",
      "DataFrame info():\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3786 entries, 2010-11-15 to 2025-12-03\n",
      "Data columns (total 45 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   VIXY_Open    3752 non-null   float64\n",
      " 1   VIXY_High    3752 non-null   float64\n",
      " 2   VIXY_Low     3752 non-null   float64\n",
      " 3   VIXY_Close   3752 non-null   float64\n",
      " 4   VIXY_Volume  3752 non-null   float64\n",
      " 5   XPH_Open     3786 non-null   float64\n",
      " 6   XPH_High     3786 non-null   float64\n",
      " 7   XPH_Low      3786 non-null   float64\n",
      " 8   XPH_Close    3786 non-null   float64\n",
      " 9   XPH_Volume   3786 non-null   int64  \n",
      " 10  XBI_Open     3786 non-null   float64\n",
      " 11  XBI_High     3786 non-null   float64\n",
      " 12  XBI_Low      3786 non-null   float64\n",
      " 13  XBI_Close    3786 non-null   float64\n",
      " 14  XBI_Volume   3786 non-null   int64  \n",
      " 15  VHT_Open     3786 non-null   float64\n",
      " 16  VHT_High     3786 non-null   float64\n",
      " 17  VHT_Low      3786 non-null   float64\n",
      " 18  VHT_Close    3786 non-null   float64\n",
      " 19  VHT_Volume   3786 non-null   int64  \n",
      " 20  SPY_Open     3786 non-null   float64\n",
      " 21  SPY_High     3786 non-null   float64\n",
      " 22  SPY_Low      3786 non-null   float64\n",
      " 23  SPY_Close    3786 non-null   float64\n",
      " 24  SPY_Volume   3786 non-null   int64  \n",
      " 25  IHE_Open     3786 non-null   float64\n",
      " 26  IHE_High     3786 non-null   float64\n",
      " 27  IHE_Low      3786 non-null   float64\n",
      " 28  IHE_Close    3786 non-null   float64\n",
      " 29  IHE_Volume   3786 non-null   int64  \n",
      " 30  IBB_Open     3786 non-null   float64\n",
      " 31  IBB_High     3786 non-null   float64\n",
      " 32  IBB_Low      3786 non-null   float64\n",
      " 33  IBB_Close    3786 non-null   float64\n",
      " 34  IBB_Volume   3786 non-null   int64  \n",
      " 35  XLV_Open     3786 non-null   float64\n",
      " 36  XLV_High     3786 non-null   float64\n",
      " 37  XLV_Low      3786 non-null   float64\n",
      " 38  XLV_Close    3786 non-null   float64\n",
      " 39  XLV_Volume   3786 non-null   int64  \n",
      " 40  PPH_Open     3786 non-null   float64\n",
      " 41  PPH_High     3786 non-null   float64\n",
      " 42  PPH_Low      3786 non-null   float64\n",
      " 43  PPH_Close    3786 non-null   float64\n",
      " 44  PPH_Volume   3786 non-null   int64  \n",
      "dtypes: float64(37), int64(8)\n",
      "memory usage: 1.3 MB\n",
      "None\n",
      "\n",
      "Head:\n",
      "\n",
      "            VIXY_Open  VIXY_High  VIXY_Low  VIXY_Close  VIXY_Volume  \\\n",
      "Date                                                                  \n",
      "2010-11-15        NaN        NaN       NaN         NaN          NaN   \n",
      "2010-11-16        NaN        NaN       NaN         NaN          NaN   \n",
      "2010-11-17        NaN        NaN       NaN         NaN          NaN   \n",
      "2010-11-18        NaN        NaN       NaN         NaN          NaN   \n",
      "2010-11-19        NaN        NaN       NaN         NaN          NaN   \n",
      "\n",
      "             XPH_Open   XPH_High    XPH_Low  XPH_Close  XPH_Volume  ...  \\\n",
      "Date                                                                ...   \n",
      "2010-11-15  22.385000  22.580000  22.379999  22.450001       50800  ...   \n",
      "2010-11-16  22.325001  22.389999  22.155001  22.245001      142200  ...   \n",
      "2010-11-17  22.250000  22.355000  22.215000  22.275000       96800  ...   \n",
      "2010-11-18  22.580000  22.700001  22.510000  22.625000      151800  ...   \n",
      "2010-11-19  22.655001  22.655001  22.459999  22.625000       95200  ...   \n",
      "\n",
      "             XLV_Open   XLV_High    XLV_Low  XLV_Close  XLV_Volume   PPH_Open  \\\n",
      "Date                                                                            \n",
      "2010-11-15  31.040001  31.110001  30.889999  30.940001     4181900  32.330002   \n",
      "2010-11-16  30.740000  30.840000  30.400000  30.490000    12265400  31.924999   \n",
      "2010-11-17  30.540001  30.660000  30.480000  30.540001     5810200  31.875000   \n",
      "2010-11-18  30.719999  31.040001  30.629999  30.969999     5721600  31.940001   \n",
      "2010-11-19  30.940001  31.020000  30.820000  31.000000     5078100  32.055000   \n",
      "\n",
      "             PPH_High    PPH_Low  PPH_Close  PPH_Volume  \n",
      "Date                                                     \n",
      "2010-11-15  32.330002  32.105000  32.145000      862400  \n",
      "2010-11-16  32.040001  31.625000  31.735001      866800  \n",
      "2010-11-17  31.910000  31.730000  31.735001      678600  \n",
      "2010-11-18  32.255001  31.905001  32.160000      427200  \n",
      "2010-11-19  32.150002  32.014999  32.145000      544000  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TEST STEP A: RAW DATA PREP\n",
    "# ============================\n",
    "\n",
    "print(\">>> Running Step A ‚Äî download_and_prepare_data\\n\")\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "TARGET = \"PPH\"\n",
    "SUPPORT = [\n",
    "    \"XPH\", \"IHE\", \"IBB\", \"XBI\", \"XLV\", \"VHT\", \"SPY\", \"VIXY\"\n",
    "]\n",
    "START = \"2011-01-04\"\n",
    "END   = \"2025-12-01\"\n",
    "\n",
    "# --- RUN STEP A ---\n",
    "df_raw = download_and_prepare_data(\n",
    "    target=TARGET,\n",
    "    support_tickers=SUPPORT,\n",
    "    start_date=START,\n",
    "    end_date=END\n",
    ")\n",
    "\n",
    "print(\"\\n>>> DONE. Output summary:\\n\")\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "print(\"\\nIndex dtype:\", df_raw.index.dtype)\n",
    "print(\"\\nColumns:\", list(df_raw.columns)[:20], \"...\\n\")\n",
    "print(\"\\nDataFrame info():\\n\")\n",
    "print(df_raw.info())\n",
    "print(\"\\nHead:\\n\")\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5432d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP A.5 ‚Äî EARLY CLEANING OF RAW DATA\n",
    "# ============================================================\n",
    "\n",
    "def clean_raw_ohlcv(df, tickers):\n",
    "    \"\"\"\n",
    "    Early cleaning applied immediately after downloading and trimming OHLCV data.\n",
    "    Ensures stable inputs for feature generation.\n",
    "\n",
    "    Steps:\n",
    "    - Drop duplicate index entries\n",
    "    - Ensure index is sorted\n",
    "    - Replace infinite values\n",
    "    - Forward-fill *only* OHLCV values to handle non-trading gaps\n",
    "    - Remove rows where some tickers have partial OHLCV (e.g. trading halts)\n",
    "    - Enforce numeric dtypes\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 1. Remove duplicate dates & sort index\n",
    "    # --------------------------------------------\n",
    "    df = df[~df.index.duplicated(keep='first')].sort_index()\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 2. Replace infinite values\n",
    "    # --------------------------------------------\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 3. Forward-fill ONLY OHLCV fields across non-trading gaps\n",
    "    # --------------------------------------------\n",
    "    ohlcv_cols = []\n",
    "    for t in tickers:\n",
    "        for field in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
    "            col = f\"{t}_{field}\"\n",
    "            if col in df.columns:\n",
    "                ohlcv_cols.append(col)\n",
    "\n",
    "    # Forward-fill OHLCV missing values (safe for market closures)\n",
    "    df[ohlcv_cols] = df[ohlcv_cols].ffill()\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 4. Remove rows where *not all* tickers have full OHLCV\n",
    "    # --------------------------------------------\n",
    "    mask = df[ohlcv_cols].notna().all(axis=1)\n",
    "    df = df[mask]\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 5. Enforce numeric dtype for OHLCV columns\n",
    "    # --------------------------------------------\n",
    "    for col in ohlcv_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 6. Remove any row that still contains NaNs in OHLCV\n",
    "    # --------------------------------------------\n",
    "    df = df.dropna(subset=ohlcv_cols, how=\"any\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd84c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# STEP A.5 ‚Äî AUTOMATED TEST BLOCK FOR clean_raw_ohlcv\n",
    "# =====================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def test_step_A5(df_raw, target, support_tickers):\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"üîç TESTING STEP A.5 ‚Äî CLEAN RAW OHLCV\")\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    tickers = [target] + support_tickers\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Run the cleaning step\n",
    "    # ---------------------------------------------------------\n",
    "    df_clean = clean_raw_ohlcv(df_raw, tickers=tickers)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Index must be datetime, sorted, unique\n",
    "    # ---------------------------------------------------------\n",
    "    assert isinstance(df_clean.index, pd.DatetimeIndex), \\\n",
    "        \"‚ùå Index must remain DatetimeIndex\"\n",
    "\n",
    "    assert df_clean.index.is_monotonic_increasing, \\\n",
    "        \"‚ùå Index must be sorted after cleaning\"\n",
    "\n",
    "    assert df_clean.index.is_unique, \\\n",
    "        \"‚ùå Duplicate dates detected after cleaning\"\n",
    "\n",
    "    print(\"‚úì Index: datetime, sorted, unique\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Collect expected OHLCV columns\n",
    "    # ---------------------------------------------------------\n",
    "    expected_cols = []\n",
    "    for t in tickers:\n",
    "        for field in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
    "            expected_cols.append(f\"{t}_{field}\")\n",
    "\n",
    "    # Ensure all expected columns exist\n",
    "    for col in expected_cols:\n",
    "        assert col in df_clean.columns, f\"‚ùå Missing OHLCV column: {col}\"\n",
    "\n",
    "    print(f\"‚úì All expected OHLCV columns present ({len(expected_cols)} columns)\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. OHLCV must contain only numeric values\n",
    "    # ---------------------------------------------------------\n",
    "    for col in expected_cols:\n",
    "        assert pd.api.types.is_numeric_dtype(df_clean[col]), \\\n",
    "            f\"‚ùå OHLCV column {col} is not numeric dtype\"\n",
    "\n",
    "    print(\"‚úì All OHLCV columns are numeric\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. No NaNs allowed in OHLCV after cleanup\n",
    "    # ---------------------------------------------------------\n",
    "    na_counts = df_clean[expected_cols].isna().sum()\n",
    "    assert na_counts.sum() == 0, \\\n",
    "        f\"‚ùå NaNs remain in OHLCV after clean_raw_ohlcv:\\n{na_counts[na_counts > 0]}\"\n",
    "\n",
    "    print(\"‚úì No NaNs in OHLCV after cleanup\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 5. No infinite values\n",
    "    # ---------------------------------------------------------\n",
    "    assert not np.isinf(df_clean[expected_cols].values).any(), \\\n",
    "        \"‚ùå Infinite values remain in OHLCV\"\n",
    "\n",
    "    print(\"‚úì No infinite values in OHLCV\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 6. Forward-fill must not alter the first row\n",
    "    # ---------------------------------------------------------\n",
    "    for col in expected_cols:\n",
    "        assert df_clean[col].iloc[0] == df_raw[col].loc[df_clean.index[0]], \\\n",
    "            f\"‚ùå First-row OHLCV changed for column {col}; unsafe ffill\"\n",
    "\n",
    "    print(\"‚úì Forward-fill did not alter first valid OHLCV row\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 7. No partial rows allowed (every ticker must have all 5 fields)\n",
    "    # ---------------------------------------------------------\n",
    "    row_validity = df_clean[expected_cols].notna().all(axis=1)\n",
    "    assert row_validity.all(), \\\n",
    "        \"‚ùå Some rows still have incomplete OHLCV\"\n",
    "\n",
    "    print(\"‚úì All rows contain complete OHLCV data\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 8. df_clean must have fewer or equal rows (never more)\n",
    "    # ---------------------------------------------------------\n",
    "    assert len(df_clean) <= len(df_raw), \\\n",
    "        \"‚ùå clean_raw_ohlcv added rows ‚Äî this should never happen\"\n",
    "\n",
    "    print(\"‚úì Row count is valid (no unexpected new rows)\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 9. Shape and summary\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\nüéâ STEP A.5 PASSED ‚Äî Raw OHLCV cleaned safely.\\n\")\n",
    "    print(f\"Final shape after Step A.5: {df_clean.shape}\")\n",
    "\n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba6d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL\n",
    "feature_registry = {}\n",
    "\n",
    "def register_feature(name, shift):\n",
    "    global feature_registry\n",
    "    feature_registry[name] = shift\n",
    "    print(f\"Registered feature: {name} with shift: {shift}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP B ‚Äî FEATURE REGISTRY (Exact Implementation Requested)\n",
    "# ============================================================\n",
    "\n",
    "def initialize_feature_registry(df, target, support_tickers):\n",
    "\n",
    "    global feature_registry\n",
    "\n",
    "    # (1) RESET\n",
    "    feature_registry.clear()\n",
    "\n",
    "    # (2) Add canonical target_close\n",
    "    raw_target_close = f\"{target}_Close\"\n",
    "    df[\"target_close\"] = df[raw_target_close].copy()\n",
    "\n",
    "    # (3) Register canonical target\n",
    "    register_feature(\"target_close\", \"no_shift\")\n",
    "\n",
    "    # (4) Loop through all columns\n",
    "    for col in df.columns:\n",
    "\n",
    "        if col == \"target_close\":\n",
    "            continue  # already handled\n",
    "\n",
    "        if col.endswith(\"_Open\"):\n",
    "            register_feature(col, \"no_shift\")\n",
    "        else:\n",
    "            register_feature(col, \"shift_1\")\n",
    "\n",
    "    print(\"FINAL REGISTRY:\", feature_registry)\n",
    "    return feature_registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ed655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP C ‚Äî PER-TICKER TECHNICAL INDICATORS (FINAL VERSION)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OHLCV Helper\n",
    "# ------------------------------------------------------------\n",
    "def get_ohlcv(df, ticker):\n",
    "    \"\"\"\n",
    "    Returns (Open, High, Low, Close, Volume) column names for a ticker.\n",
    "    Ensures all are present.\n",
    "    \"\"\"\n",
    "    cols = [f\"{ticker}_{x}\" for x in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "    return cols if all(c in df.columns for c in cols) else None\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MOMENTUM FEATURES\n",
    "# ------------------------------------------------------------\n",
    "def feat_returns(df, ticker, close):\n",
    "    feats = {}\n",
    "    for w in [1, 5, 10, 20]:\n",
    "        name = f\"{ticker}_Return_{w}d\"\n",
    "        feats[name] = df[close].pct_change(w)\n",
    "        register_feature(name, \"shift_1\")\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TREND FEATURES (SMA/EMA/MA Cross)\n",
    "# ------------------------------------------------------------\n",
    "def feat_sma_ema(df, ticker, close):\n",
    "    feats = {}\n",
    "\n",
    "    name = f\"{ticker}_SMA_10\"\n",
    "    feats[name] = df[close].rolling(10).mean()\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    # name = f\"{ticker}_SMA_50\"\n",
    "    # feats[name] = df[close].rolling(50).mean()\n",
    "    # register_feature(name, \"shift_1\")\n",
    "\n",
    "    name = f\"{ticker}_EMA_20\"\n",
    "    feats[name] = df[close].ewm(span=20).mean()\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MACD FEATURES\n",
    "# ------------------------------------------------------------\n",
    "def feat_macd(df, ticker, close):\n",
    "    feats = {}\n",
    "\n",
    "    ema12 = df[close].ewm(span=12).mean()\n",
    "    ema26 = df[close].ewm(span=26).mean()\n",
    "    macd = ema12 - ema26\n",
    "\n",
    "    name = f\"{ticker}_MACD\"\n",
    "    feats[name] = macd\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    name = f\"{ticker}_MACD_sig\"\n",
    "    feats[name] = macd.ewm(span=9).mean()\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    name = f\"{ticker}_MACD_hist\"\n",
    "    feats[name] = feats[f\"{ticker}_MACD\"] - feats[f\"{ticker}_MACD_sig\"]\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RSI FEATURE\n",
    "# ------------------------------------------------------------\n",
    "def feat_rsi(df, ticker, close):\n",
    "    feats = {}\n",
    "\n",
    "    delta = df[close].diff()\n",
    "    up = delta.clip(lower=0).rolling(14).mean()\n",
    "    down = (-delta.clip(upper=0)).rolling(14).mean()\n",
    "    rs = up / down\n",
    "\n",
    "    name = f\"{ticker}_RSI_14\"\n",
    "    feats[name] = 100 - (100 / (1 + rs))\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STOCHASTICS FEATURES\n",
    "# ------------------------------------------------------------\n",
    "def feat_stochastics(df, ticker, close):\n",
    "    feats = {}\n",
    "\n",
    "    low14 = df[close].rolling(14).min()\n",
    "    high14 = df[close].rolling(14).max()\n",
    "\n",
    "    name = f\"{ticker}_StochK\"\n",
    "    feats[name] = 100 * (df[close] - low14) / (high14 - low14)\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    name = f\"{ticker}_StochD\"\n",
    "    feats[name] = feats[f\"{ticker}_StochK\"].rolling(3).mean()\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# VOLATILITY FEATURES\n",
    "# ------------------------------------------------------------\n",
    "def feat_volatility(df, ticker, o, h, l, c, returns_dict):\n",
    "    feats = {}\n",
    "    ret1 = returns_dict[f\"{ticker}_Return_1d\"]\n",
    "\n",
    "    name = f\"{ticker}_Vol_20\"\n",
    "    feats[name] = ret1.rolling(20).std()\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    name = f\"{ticker}_Parkinson_20\"\n",
    "    feats[name] = ((np.log(df[h]/df[l])**2).rolling(20).mean() * (1/(4*np.log(2))))\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    name = f\"{ticker}_GK_20\"\n",
    "    feats[name] = (\n",
    "        0.5*(np.log(df[h]/df[l])**2)\n",
    "        - (2*np.log(np.e)-1)*(np.log(df[c]/df[o])**2)\n",
    "    ).rolling(20).mean()\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    sma20 = df[c].rolling(20).mean()\n",
    "    std20 = df[c].rolling(20).std()\n",
    "\n",
    "    name = f\"{ticker}_BB_width\"\n",
    "    feats[name] = (2 * std20) / sma20\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# VOLUME FEATURES\n",
    "# ------------------------------------------------------------\n",
    "def feat_volume(df, ticker, v, c):\n",
    "    feats = {}\n",
    "\n",
    "    name = f\"{ticker}_Volume_ROC\"\n",
    "    feats[name] = df[v].pct_change(5)\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    name = f\"{ticker}_Volume_Z\"\n",
    "    feats[name] = (df[v] - df[v].rolling(20).mean()) / df[v].rolling(20).std()\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    name = f\"{ticker}_OBV\"\n",
    "    feats[name] = (np.sign(df[c].diff()) * df[v]).fillna(0).cumsum()\n",
    "    register_feature(name, \"shift_1\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ENTROPY FEATURE\n",
    "# ------------------------------------------------------------\n",
    "def feat_entropy(df, ticker):\n",
    "    feats = {}\n",
    "\n",
    "    col = f\"{ticker}_Return_1d\"\n",
    "    if col in df.columns:\n",
    "        name = f\"{ticker}_Entropy_20\"\n",
    "        feats[name] = (df[col]**2).rolling(20).sum()\n",
    "        register_feature(name, \"shift_1\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# HMM BLOCK (optional)\n",
    "# ------------------------------------------------------------\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "def feat_hmm(df, ticker, returns_dict, n_states=3):\n",
    "    \"\"\"\n",
    "    Fit a simple Gaussian HMM on 1-day returns and output hidden state sequence.\n",
    "    Output MUST be same length as df, with NaN padding at the top.\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    ret = returns_dict.get(f\"{ticker}_Return_1d\")\n",
    "\n",
    "    if ret is None:\n",
    "        return feats\n",
    "\n",
    "    # Convert to numpy, drop NaNs for fitting\n",
    "    clean = ret.dropna().values.reshape(-1, 1)\n",
    "\n",
    "    if len(clean) < 50:\n",
    "        # Not enough data for HMM\n",
    "        feats[f\"{ticker}_HMM\"] = pd.Series(np.nan, index=df.index)\n",
    "        register_feature(f\"{ticker}_HMM\", \"shift_1\")\n",
    "        return feats\n",
    "\n",
    "    # Fit HMM\n",
    "    model = GaussianHMM(n_components=n_states, covariance_type=\"full\", n_iter=200)\n",
    "    model.fit(clean)\n",
    "\n",
    "    # Predict states\n",
    "    hidden_states = model.predict(clean)\n",
    "\n",
    "    # Pad output back to full length\n",
    "    pad_length = len(df) - len(hidden_states)\n",
    "    padded = np.concatenate([np.full(pad_length, np.nan), hidden_states])\n",
    "\n",
    "    col = f\"{ticker}_HMM\"\n",
    "    feats[col] = pd.Series(padded, index=df.index)\n",
    "\n",
    "    register_feature(col, \"shift_1\")\n",
    "    return feats\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MASTER WRAPPER (FIXED)\n",
    "# ------------------------------------------------------------\n",
    "def generate_per_ticker_features(df, tickers, use_hmm=True):\n",
    "    \"\"\"\n",
    "    Computes all technical indicators for each ticker.\n",
    "    Ensures correct sequencing:\n",
    "\n",
    "        returns ‚Üí trend ‚Üí MACD ‚Üí RSI ‚Üí Stochastics ‚Üí volatility ‚Üí volume ‚Üí entropy ‚Üí HMM\n",
    "    \"\"\"\n",
    "    all_feats = {}\n",
    "\n",
    "    # We make a working copy so df is never modified outside Step C\n",
    "    df_local = df.copy()\n",
    "\n",
    "    for ticker in tqdm(tickers, desc=\"Per-ticker technicals\"):\n",
    "        ohlcv = get_ohlcv(df_local, ticker)\n",
    "        if not ohlcv:\n",
    "            continue\n",
    "\n",
    "        o, h, l, c, v = ohlcv\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # 1. RETURNS (must run first)\n",
    "        # --------------------------------------------\n",
    "        returns_dict = feat_returns(df_local, ticker, c)\n",
    "        all_feats.update(returns_dict)\n",
    "\n",
    "        # **CRITICAL FIX: Insert returns into df_local so entropy & others can see them**\n",
    "        for k, s in returns_dict.items():\n",
    "            df_local[k] = s\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # 2. TREND\n",
    "        # --------------------------------------------\n",
    "        feats = feat_sma_ema(df_local, ticker, c)\n",
    "        all_feats.update(feats)\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # 3. MACD\n",
    "        # --------------------------------------------\n",
    "        feats = feat_macd(df_local, ticker, c)\n",
    "        all_feats.update(feats)\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # 4. MOMENTUM\n",
    "        # --------------------------------------------\n",
    "        feats = feat_rsi(df_local, ticker, c)\n",
    "        all_feats.update(feats)\n",
    "\n",
    "        feats = feat_stochastics(df_local, ticker, c)\n",
    "        all_feats.update(feats)\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # 5. VOLATILITY (depends on Return_1d)\n",
    "        # --------------------------------------------\n",
    "        feats = feat_volatility(df_local, ticker, o, h, l, c, returns_dict)\n",
    "        all_feats.update(feats)\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # 6. VOLUME\n",
    "        # --------------------------------------------\n",
    "        feats = feat_volume(df_local, ticker, v, c)\n",
    "        all_feats.update(feats)\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # 7. ENTROPY  (now works, because Return_1d is in df_local)\n",
    "        # --------------------------------------------\n",
    "        feats = feat_entropy(df_local, ticker)\n",
    "        all_feats.update(feats)\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # 8. OPTIONAL HMM\n",
    "        # --------------------------------------------\n",
    "        if use_hmm:\n",
    "            feats = feat_hmm(df_local, ticker, returns_dict)\n",
    "            all_feats.update(feats)\n",
    "\n",
    "    return all_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eec94830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# STEP C ‚Äî AUTOMATED TEST BLOCK FOR generate_per_ticker_features\n",
    "# =====================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def test_step_C(df_cleanA, target, support_tickers, use_hmm=True):\n",
    "\n",
    "    print(\"\\n==============================================\")\n",
    "    print(\"üîç TESTING STEP C ‚Äî PER-TICKER TECHNICALS\")\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    tickers = [target] + support_tickers\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Run Step C\n",
    "    # ---------------------------------------------------------\n",
    "    tech_feats = generate_per_ticker_features(\n",
    "        df_cleanA,\n",
    "        tickers=tickers,\n",
    "        use_hmm=use_hmm\n",
    "    )\n",
    "\n",
    "    assert isinstance(tech_feats, dict), \\\n",
    "        \"‚ùå Step C must return a dict of feature_name ‚Üí Series\"\n",
    "\n",
    "    print(f\"‚úì Returned a dict with {len(tech_feats)} features\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Check all expected features exist for each ticker\n",
    "    # ---------------------------------------------------------\n",
    "    expected_suffixes = [\n",
    "        \"Return_1d\", \"Return_5d\", \"Return_10d\", \"Return_20d\",\n",
    "        \"SMA_10\", \"SMA_50\",\n",
    "        \"EMA_20\", \"EMA_50\",\n",
    "        \"MA_Cross\",\n",
    "        \"MACD\", \"MACD_sig\", \"MACD_hist\",\n",
    "        \"RSI_14\",\n",
    "        \"StochK\", \"StochD\",\n",
    "        \"Vol_20\",\n",
    "        \"Parkinson_20\",\n",
    "        \"GK_20\",\n",
    "        \"BB_width\",\n",
    "        \"Volume_ROC\",\n",
    "        \"Volume_Z\",\n",
    "        \"OBV\",\n",
    "        \"Entropy_20\",\n",
    "    ]\n",
    "\n",
    "    if use_hmm:\n",
    "        expected_suffixes.append(\"HMM\")\n",
    "\n",
    "    for t in tickers:\n",
    "        for suf in expected_suffixes:\n",
    "            feat = f\"{t}_{suf}\"\n",
    "            assert feat in tech_feats, f\"‚ùå Missing feature: {feat}\"\n",
    "\n",
    "    print(f\"‚úì All expected per-ticker features present for {len(tickers)} tickers\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. Verify each feature is a Pandas Series aligned to df_cleanA\n",
    "    # ---------------------------------------------------------\n",
    "    for name, series in tech_feats.items():\n",
    "        assert isinstance(series, pd.Series), \\\n",
    "            f\"‚ùå Feature {name} is not a Pandas Series\"\n",
    "\n",
    "        assert len(series) == len(df_cleanA), \\\n",
    "            f\"‚ùå Feature {name} length mismatch\"\n",
    "\n",
    "    print(\"‚úì All features are Series and correctly aligned\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Check numeric dtype for all non-HMM features\n",
    "    # ---------------------------------------------------------\n",
    "    for name, series in tech_feats.items():\n",
    "        if name.endswith(\"_HMM\"):\n",
    "            # HMM is integer states\n",
    "            assert pd.api.types.is_numeric_dtype(series), \\\n",
    "                f\"‚ùå HMM feature {name} must be numeric\"\n",
    "        else:\n",
    "            assert pd.api.types.is_numeric_dtype(series), \\\n",
    "                f\"‚ùå Feature {name} must have numeric dtype\"\n",
    "\n",
    "    print(\"‚úì All features have numeric dtype\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 5. Quick sanity check for NaN explosion (expected early NaNs OK)\n",
    "    # ---------------------------------------------------------\n",
    "    # Rolling features = NaNs at top (OK)\n",
    "    # But the entire column cannot be NaN.\n",
    "    for name, series in tech_feats.items():\n",
    "        assert series.notna().sum() > 10, \\\n",
    "            f\"‚ùå Feature {name} seems to be all-NaN (bad rolling window?)\"\n",
    "\n",
    "    print(\"‚úì Each feature has valid non-NaN data\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 6. Check registry integration (all shift_1)\n",
    "    # ---------------------------------------------------------\n",
    "    from pprint import pprint\n",
    "    all_rules = get_feature_registry()\n",
    "\n",
    "    # Every feature in tech_feats must exist in registry\n",
    "    missing_in_registry = [\n",
    "        f for f in tech_feats.keys() if f not in all_rules\n",
    "    ]\n",
    "    assert len(missing_in_registry) == 0, \\\n",
    "        f\"‚ùå Missing registry entries:\\n{missing_in_registry}\"\n",
    "\n",
    "    # All rules must be shift_1 for technicals\n",
    "    bad_rules = {\n",
    "        f: r for f, r in all_rules.items()\n",
    "        if f in tech_feats and r != \"shift_1\"\n",
    "    }\n",
    "    assert len(bad_rules) == 0, \\\n",
    "        f\"‚ùå Some technical features have incorrect shift rules:\\n{bad_rules}\"\n",
    "\n",
    "    print(\"‚úì Registry entries valid and all marked shift_1\")\n",
    "\n",
    "    print(\"\\nüéâ STEP C PASSED ‚Äî Technical Indicators Valid\\n\")\n",
    "\n",
    "    return tech_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db5b15a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP D ‚Äî CROSS-TICKER FEATURES\n",
    "# ============================================================\n",
    "\n",
    "def compute_cross_ticker_features(df, target, support_tickers):\n",
    "    \"\"\"\n",
    "    Builds cross-ticker features comparing each support ticker to the target.\n",
    "    Returns a dict of {feature_name: Series}.\n",
    "    \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    target_close = f\"{target}_Close\"\n",
    "    if target_close not in df.columns:\n",
    "        raise ValueError(f\"Target {target_close} not found in dataframe.\")\n",
    "\n",
    "    target_ret_20 = f\"{target}_Return_20d\"\n",
    "    if target_ret_20 not in df.columns:\n",
    "        # Should never happen because Step C adds it\n",
    "        raise ValueError(f\"{target_ret_20} missing ‚Äî Step C must run first.\")\n",
    "\n",
    "    for ticker in support_tickers:\n",
    "        close_col = f\"{ticker}_Close\"\n",
    "        ret20_col = f\"{ticker}_Return_20d\"\n",
    "\n",
    "        if close_col not in df.columns:\n",
    "            continue  # skip incomplete tickers\n",
    "\n",
    "        # ------------------------------\n",
    "        # Price Ratio\n",
    "        # ------------------------------\n",
    "        name = f\"{ticker}_Ratio_{target}\"\n",
    "        feats[name] = df[close_col] / df[target_close]\n",
    "        register_feature(name, \"shift_1\")\n",
    "\n",
    "        # ------------------------------\n",
    "        # RS-20 (Relative Strength)\n",
    "        # ------------------------------\n",
    "        if ret20_col in df.columns:\n",
    "            name = f\"{ticker}_RS_20\"\n",
    "            feats[name] = df[ret20_col] - df[target_ret_20]\n",
    "            register_feature(name, \"shift_1\")\n",
    "\n",
    "        # ------------------------------\n",
    "        # 60-Day Rolling Correlation\n",
    "        # ------------------------------\n",
    "        # name = f\"{ticker}_Corr_{target}_60\"\n",
    "        # feats[name] = df[close_col].rolling(60).corr(df[target_close])\n",
    "        # register_feature(name, \"shift_1\")\n",
    "\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44816af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP E ‚Äî PCA LATENT FACTORS\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def compute_pca_features(df, tickers):\n",
    "    \"\"\"\n",
    "    Computes PCA on the 1-day returns of all tickers.\n",
    "    Returns a dict of {feature_name: Series}.\n",
    "    \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    # Collect all 1-day return columns\n",
    "    ret_cols = [f\"{t}_Return_1d\" for t in tickers if f\"{t}_Return_1d\" in df.columns]\n",
    "\n",
    "    if len(ret_cols) == 0:\n",
    "        # Should never happen (Step C must generate them)\n",
    "        return feats\n",
    "\n",
    "    # Extract data for PCA\n",
    "    ret_df = df[ret_cols].dropna()\n",
    "\n",
    "    # Need enough rows for PCA stability\n",
    "    if len(ret_df) < 200:\n",
    "        return feats\n",
    "\n",
    "    try:\n",
    "        pca = PCA(n_components=3)\n",
    "        pca_values = pca.fit_transform(ret_df)\n",
    "\n",
    "        # PCA_1\n",
    "        name = \"PCA_1\"\n",
    "        feats[name] = pd.Series(pca_values[:, 0], index=ret_df.index)\n",
    "        register_feature(name, \"shift_1\")\n",
    "\n",
    "        # PCA_2\n",
    "        name = \"PCA_2\"\n",
    "        feats[name] = pd.Series(pca_values[:, 1], index=ret_df.index)\n",
    "        register_feature(name, \"shift_1\")\n",
    "\n",
    "        # PCA_3\n",
    "        name = \"PCA_3\"\n",
    "        feats[name] = pd.Series(pca_values[:, 2], index=ret_df.index)\n",
    "        register_feature(name, \"shift_1\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"PCA failed: {e}\")\n",
    "\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP F ‚Äî CALENDAR + MACRO FEATURES (COMPRESSED, NO 'date')\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import holidays\n",
    "\n",
    "# -----------------------------\n",
    "# FOMC Calendar Fetcher\n",
    "# -----------------------------\n",
    "def fetch_fomc_dates():\n",
    "    url = \"https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\"\n",
    "    try:\n",
    "        tables = pd.read_html(url)\n",
    "        dates = pd.to_datetime(tables[0].iloc[:, 0], errors=\"coerce\").dropna()\n",
    "        return dates.sort_values()\n",
    "    except:\n",
    "        fallback = pd.to_datetime([\n",
    "            \"2024-01-31\",\"2024-03-20\",\"2024-05-01\",\n",
    "            \"2024-06-12\",\"2024-07-31\",\"2024-09-18\",\n",
    "            \"2024-11-07\",\"2024-12-18\"\n",
    "        ])\n",
    "        return fallback.sort_values()\n",
    "\n",
    "# -----------------------------\n",
    "# Macro Calendar\n",
    "# -----------------------------\n",
    "def macro_calendar():\n",
    "    events = {\n",
    "        \"cpi\": [\"2024-01-11\",\"2024-02-13\",\"2024-03-12\",\"2024-04-10\"],\n",
    "        \"nfp\": [\"2024-01-05\",\"2024-02-02\",\"2024-03-08\",\"2024-04-05\"],\n",
    "        \"ppi\": [\"2024-01-12\",\"2024-02-16\",\"2024-03-14\",\"2024-04-11\"],\n",
    "        \"gdp\": [\"2024-01-25\",\"2024-02-28\",\"2024-03-28\",\"2024-04-25\"],\n",
    "    }\n",
    "    return {k: pd.to_datetime(v).sort_values() for k, v in events.items()}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# HELPER ‚Äî Compute distances to events\n",
    "# ============================================================\n",
    "\n",
    "def _days_to_next(idx, event_dates):\n",
    "    \"\"\"Return Series: days until the next event.\"\"\"\n",
    "    out = []\n",
    "    j = 0\n",
    "\n",
    "    for d in idx:\n",
    "        while j < len(event_dates) and event_dates[j] < d:\n",
    "            j += 1\n",
    "        if j == len(event_dates):\n",
    "            out.append(None)  # No next event\n",
    "        else:\n",
    "            out.append((event_dates[j] - d).days)\n",
    "\n",
    "    return pd.Series(out, index=idx)\n",
    "\n",
    "\n",
    "def _days_since_prev(idx, event_dates):\n",
    "    \"\"\"Return Series: days since previous event.\"\"\"\n",
    "    out = []\n",
    "    j = 0\n",
    "\n",
    "    for d in idx:\n",
    "        # Find last event <= d\n",
    "        while j < len(event_dates) and event_dates[j] <= d:\n",
    "            j += 1\n",
    "        if j == 0:\n",
    "            out.append(None)\n",
    "        else:\n",
    "            out.append((d - event_dates[j-1]).days)\n",
    "\n",
    "    return pd.Series(out, index=idx)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CALENDAR BASICS (NO SHIFT)\n",
    "# ============================================================\n",
    "\n",
    "def add_calendar_basics(df):\n",
    "    idx = df.index\n",
    "\n",
    "    features = {\n",
    "        \"day_of_week\":    idx.dayofweek,\n",
    "        \"day_of_month\":   idx.day,\n",
    "        \"month\":          idx.month,\n",
    "        \"quarter\":        idx.quarter,\n",
    "        \"is_month_end\":   idx.is_month_end.astype(int),\n",
    "        \"is_month_start\": idx.is_month_start.astype(int)\n",
    "    }\n",
    "\n",
    "    for name, series in features.items():\n",
    "        df[name] = series\n",
    "        register_feature(name, \"no_shift\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# HOLIDAYS (NO SHIFT)\n",
    "# ============================================================\n",
    "\n",
    "def add_holiday_features(df):\n",
    "    us_holidays = holidays.US()\n",
    "    idx = df.index\n",
    "\n",
    "    df[\"is_holiday_adjacent\"] = [\n",
    "        int((d + pd.Timedelta(days=1) in us_holidays) or\n",
    "            (d - pd.Timedelta(days=1) in us_holidays))\n",
    "        for d in idx\n",
    "    ]\n",
    "    register_feature(\"is_holiday_adjacent\", \"no_shift\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# OPEX WEEK (NO SHIFT)\n",
    "# ============================================================\n",
    "\n",
    "def add_opex_features(df):\n",
    "    idx = df.index\n",
    "    df[\"is_opex_week\"] = [\n",
    "        int((d.weekday() == 4) and (15 <= d.day <= 21))\n",
    "        for d in idx\n",
    "    ]\n",
    "    register_feature(\"is_opex_week\", \"no_shift\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FOMC (COMPRESSED)\n",
    "# ============================================================\n",
    "\n",
    "def add_fomc_features(df):\n",
    "    idx = df.index\n",
    "    dates = fetch_fomc_dates()\n",
    "\n",
    "    df[\"is_fomc_day\"]        = idx.isin(dates).astype(int)\n",
    "    df[\"days_to_fomc\"]       = _days_to_next(idx, dates)\n",
    "    df[\"days_since_fomc\"]    = _days_since_prev(idx, dates)\n",
    "\n",
    "    for col in [\"is_fomc_day\", \"days_to_fomc\", \"days_since_fomc\"]:\n",
    "        register_feature(col, \"no_shift\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MACRO EVENTS (COMPRESSED)\n",
    "# ============================================================\n",
    "\n",
    "def add_macro_features(df):\n",
    "    idx = df.index\n",
    "    macros = macro_calendar()\n",
    "\n",
    "    for name, dates in macros.items():\n",
    "\n",
    "        df[f\"is_{name}_day\"]          = idx.isin(dates).astype(int)\n",
    "        df[f\"days_to_{name}\"]        = _days_to_next(idx, dates)\n",
    "        df[f\"days_since_{name}\"]     = _days_since_prev(idx, dates)\n",
    "\n",
    "        register_feature(f\"is_{name}_day\",      \"no_shift\")\n",
    "        register_feature(f\"days_to_{name}\",     \"no_shift\")\n",
    "        register_feature(f\"days_since_{name}\",  \"no_shift\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MASTER WRAPPER\n",
    "# ============================================================\n",
    "\n",
    "def generate_calendar_and_macro_features(df):\n",
    "    df = df.copy()\n",
    "    df = add_calendar_basics(df)\n",
    "    df = add_holiday_features(df)\n",
    "    df = add_opex_features(df)\n",
    "    df = add_fomc_features(df)\n",
    "    df = add_macro_features(df)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "    # FIX: Macro distance features produce unavoidable NaNs\n",
    "    # ------------------------------------------------------------\n",
    "    # Any column like days_since_* or days_to_* will have NaNs:\n",
    "    #   - days_since_* ‚Üí NaN before the FIRST macro event\n",
    "    #   - days_to_*    ‚Üí NaN after the LAST macro event\n",
    "    # These NaNs should NOT cause row deletion in the cleaning step.\n",
    "    #\n",
    "    # Strategy:\n",
    "    #   Impute with a large sentinel value (999) so the model\n",
    "    #   interprets ‚Äúfar from event‚Äù properly.\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    dist_cols = [c for c in df.columns\n",
    "                 if c.startswith(\"days_since_\") or c.startswith(\"days_to_\")]\n",
    "\n",
    "    if dist_cols:\n",
    "        df[dist_cols] = df[dist_cols].fillna(999)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9750e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP G ‚Äî SHIFT ENGINE (Column-Replacing, Registry-Driven)\n",
    "# ============================================================\n",
    "\n",
    "def apply_shift_engine(df, horizon):\n",
    "    \"\"\"\n",
    "    Applies registry-driven temporal alignment AND updates the registry.\n",
    "\n",
    "    For each column:\n",
    "        - no_shift:       keep col(t)\n",
    "        - shift_1:        replace col   ‚Üí col_t-1\n",
    "        - shift_plus_k:   replace col   ‚Üí col_t+H\n",
    "\n",
    "    Additional improvements:\n",
    "        ‚úî Registry is rewritten to match final column names\n",
    "        ‚úî Markdown output will now be accurate\n",
    "        ‚úî No duplicate columns\n",
    "        ‚úî No original (pre-shift) columns remain\n",
    "    \"\"\"\n",
    "\n",
    "    global feature_registry\n",
    "\n",
    "    df = df.copy()\n",
    "    out = {}\n",
    "    new_registry = {}     # fully rebuild registry from scratch\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 1. Validate registry vs dataframe columns\n",
    "    # ----------------------------------------------------------\n",
    "    df_cols = set(df.columns)\n",
    "    reg_cols = set(feature_registry.keys())\n",
    "\n",
    "    missing = df_cols - reg_cols\n",
    "    extra   = reg_cols - df_cols\n",
    "\n",
    "    if missing:\n",
    "        raise ValueError(f\"Registry missing {len(missing)} columns: {sorted(missing)}\")\n",
    "\n",
    "    if extra:\n",
    "        raise ValueError(f\"Registry contains columns not in df: {sorted(extra)}\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 2. Apply transformations + rebuild registry\n",
    "    # ----------------------------------------------------------\n",
    "    for col in df.columns:\n",
    "        rule = feature_registry[col]\n",
    "        s = df[col]\n",
    "\n",
    "        # ----- no_shift ‚Üí keep original name -----\n",
    "        if rule == \"no_shift\":\n",
    "            new_name = col\n",
    "            out[new_name] = s\n",
    "            new_registry[new_name] = \"no_shift\"\n",
    "\n",
    "        # ----- shift_1 ‚Üí output col_t-1 -----\n",
    "        elif rule == \"shift_1\":\n",
    "            new_name = f\"{col}_t-1\"\n",
    "            out[new_name] = s.shift(1)\n",
    "            new_registry[new_name] = \"shift_1\"    # final shift label\n",
    "\n",
    "        # ----- shift_plus_k ‚Üí output col_t+H -----\n",
    "        elif rule == \"shift_plus_k\":\n",
    "            new_name = f\"{col}_t+{horizon}\"\n",
    "            out[new_name] = s.shift(-horizon)\n",
    "            new_registry[new_name] = f\"shift_plus_{horizon}\"\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown shift rule: {rule}\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 3. Replace registry with the new post-shift registry\n",
    "    # ----------------------------------------------------------\n",
    "    feature_registry = new_registry\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 4. Build final DataFrame\n",
    "    # ----------------------------------------------------------\n",
    "    aligned_df = pd.DataFrame(out, index=df.index)\n",
    "\n",
    "    return aligned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c1ad8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP H ‚Äî MARKDOWN FEATURE DOCUMENTATION GENERATOR\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "def generate_markdown_feature_doc(path, horizon):\n",
    "    \"\"\"\n",
    "    Creates a Markdown file documenting:\n",
    "    - every feature registered\n",
    "    - the shift rule\n",
    "    - the resulting final columns after Step G\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Output path to write markdown file (e.g. \"docs/feature_table.md\")\n",
    "    horizon : int\n",
    "        Forecast horizon, used for shift_plus_k expansion\n",
    "    \"\"\"\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"# Feature Transformation Table\\n\")\n",
    "    lines.append(\"This table is auto-generated from the feature pipeline.\\n\")\n",
    "    lines.append(\"\\n\")\n",
    "    lines.append(\"| Feature | Rule | Output Columns |\\n\")\n",
    "    lines.append(\"|---------|------|----------------|\\n\")\n",
    "\n",
    "    for feature, rule in feature_registry.items():\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # shift_1\n",
    "        # ----------------------------------------------\n",
    "        if rule == \"shift_1\":\n",
    "            output_cols = f\"{feature}_t-1\"\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # no_shift\n",
    "        # ----------------------------------------------\n",
    "        elif rule == \"no_shift\":\n",
    "            output_cols = feature\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # shift_plus_k\n",
    "        # ----------------------------------------------\n",
    "        elif rule == \"shift_plus_k\":\n",
    "            shifted = [f\"{feature}_t+{k}\" for k in range(1, horizon + 1)]\n",
    "            output_cols = \", \".join(shifted)\n",
    "\n",
    "        else:\n",
    "            output_cols = \"ERROR_UNKNOWN_RULE\"\n",
    "\n",
    "        # add row\n",
    "        lines.append(f\"| `{feature}` | `{rule}` | `{output_cols}` |\\n\")\n",
    "\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    # Write file\n",
    "    with open(path, \"w\") as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "    print(f\"Markdown feature documentation written to: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3103991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL PRODUCTION CLEANER FOR THE FEATURE DATASET\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def clean_final_dataset(df, target, min_history=20):\n",
    "    \"\"\"\n",
    "    Cleans the aligned dataset after the shift engine.\n",
    "\n",
    "    Actions performed:\n",
    "        1. Drop rows where absolutely no features exist\n",
    "        2. Drop first rows where rolling windows leave insufficient history\n",
    "        3. Drop last rows with NaNs caused by shift_plus_k\n",
    "        4. Remove columns that are all-NaN or constant\n",
    "           AND update the registry accordingly\n",
    "        5. Ensure index is clean, sorted, unique\n",
    "    \"\"\"\n",
    "\n",
    "    global feature_registry\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Drop rows that are literally all NaN\n",
    "    # ---------------------------------------------------------\n",
    "    df = df.dropna(how=\"all\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Drop early rows lacking enough usable feature history\n",
    "    # ---------------------------------------------------------\n",
    "    non_target_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in {target, \"target_close\"}\n",
    "    ]\n",
    "\n",
    "    df[\"valid_feature_count\"] = df[non_target_cols].notna().sum(axis=1)\n",
    "    df = df[df[\"valid_feature_count\"] >= min_history]\n",
    "    df = df.drop(columns=[\"valid_feature_count\"], errors=\"ignore\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. Drop trailing NaN rows (shift_plus_k consequences)\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Remove all-NaN columns and constant columns\n",
    "    #    AND update the registry BEFORE dropping\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Drop all-NaN, constant, and long-window columns\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "    cols_all_nan = df.columns[df.isna().all()].tolist()\n",
    "\n",
    "    # Constant columns (e.g., is_year_end when dataset has no Dec 31)\n",
    "    cols_constant = [c for c in df.columns if df[c].nunique() <= 1]\n",
    "\n",
    "    # Combine everything to remove\n",
    "    cols_to_remove = set(cols_all_nan + cols_constant)\n",
    "\n",
    "    # ----- 1) Remove from registry -----\n",
    "    for col in cols_to_remove:\n",
    "        feature_registry.pop(col, None)\n",
    "\n",
    "    # ----- 2) Remove from dataframe safely -----\n",
    "    df = df.drop(columns=list(cols_to_remove), errors=\"ignore\")\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 5. Final index validations\n",
    "    # ---------------------------------------------------------\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Final dataset index must be DatetimeIndex.\")\n",
    "\n",
    "    df = df.sort_index()\n",
    "\n",
    "    if df.index.duplicated().any():\n",
    "        raise ValueError(\"Duplicate timestamps detected.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4980ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP J - MASTER PIPELINE ‚Äî COMPLETE DATASET BUILDER\n",
    "# ============================================================\n",
    "\n",
    "def build_feature_dataset(\n",
    "    target,\n",
    "    support_tickers,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    horizon,\n",
    "    markdown_output_path=None,\n",
    "    USE_HMM=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Full leakage-safe feature engineering pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. DOWNLOAD RAW PRICE DATA\n",
    "    # ---------------------------------------------------------\n",
    "    df = download_and_prepare_data(\n",
    "        target=target,\n",
    "        support_tickers=support_tickers,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date\n",
    "    )\n",
    "    print(df.shape)\n",
    "    print(df.shape)\n",
    "    tickers = [target] + support_tickers\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. EARLY RAW DATA CLEANUP\n",
    "    # ---------------------------------------------------------\n",
    "    def clean_raw_ohlcv(df, tickers):\n",
    "        \"\"\"\n",
    "        Ensures raw OHLCV downloaded from yfinance is clean, sorted, and valid.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        # Ensure index is sorted and unique\n",
    "        df = df[~df.index.duplicated()].sort_index()\n",
    "\n",
    "        # Replace inf\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # Drop any row where ALL tickers have missing OHLCV\n",
    "        required_cols = []\n",
    "        for t in tickers:\n",
    "            for f in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
    "                required_cols.append(f\"{t}_{f}\")\n",
    "\n",
    "        df = df.dropna(subset=required_cols, how=\"all\")\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. RESET FEATURE REGISTRY\n",
    "    # ---------------------------------------------------------\n",
    "    initialize_feature_registry(df, target, support_tickers)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. PER-TICKER TECHNICAL FEATURES  (Step C)\n",
    "    # ---------------------------------------------------------\n",
    "    tech_feats = generate_per_ticker_features(df, tickers, use_hmm=USE_HMM)\n",
    "\n",
    "    # Merge tech features so that Step D & Step E can see them\n",
    "    df_with_tech = pd.concat(\n",
    "        [df, pd.DataFrame(tech_feats, index=df.index)], axis=1\n",
    "    )\n",
    "\n",
    "    print(df_with_tech.shape)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 5. CROSS-TICKER FEATURES (Step D)\n",
    "    # ---------------------------------------------------------\n",
    "    cross_feats = compute_cross_ticker_features(df_with_tech, target, support_tickers)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 6. PCA LATENT FACTORS (Step E)\n",
    "    # ---------------------------------------------------------\n",
    "    pca_feats = compute_pca_features(df_with_tech, tickers)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 7. CALENDAR + MACRO FEATURES\n",
    "    # ---------------------------------------------------------\n",
    "    df_calendar = generate_calendar_and_macro_features(df)\n",
    "\n",
    "    print(df_calendar.shape)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 8. MERGE ALL FEATURE BLOCKS\n",
    "    # ---------------------------------------------------------\n",
    "    all_feats = {}\n",
    "    all_feats.update(tech_feats)\n",
    "    all_feats.update(cross_feats)\n",
    "    all_feats.update(pca_feats)\n",
    "\n",
    "    df_all = pd.concat(\n",
    "        [df_calendar, pd.DataFrame(all_feats, index=df.index)],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    print(df_all.shape)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 9. SHIFT ENGINE (APPLY TEMPORAL ALIGNMENT)\n",
    "    # ---------------------------------------------------------\n",
    "    df_aligned = apply_shift_engine(\n",
    "        df_all,\n",
    "        horizon=horizon       # <-- FIXED\n",
    "    )\n",
    "    print(df_aligned.shape)\n",
    "    # ---------------------------------------------------------\n",
    "    # 9. SHIFT ENGINE (APPLY TEMPORAL ALIGNMENT)\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    df_cleaned = clean_final_dataset(df_aligned, target, min_history=20)\n",
    "    print(df_cleaned.shape)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 11. OPTIONAL: FEATURE DOCUMENTATION\n",
    "    # ---------------------------------------------------------\n",
    "    if markdown_output_path is not None:\n",
    "        generate_markdown_feature_doc(\n",
    "            path=markdown_output_path,\n",
    "            horizon=horizon\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 12. TRIM AND RETURN MODEL-READY DATAFRAME\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    df = df_cleaned.loc[start_date : end_date]\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3b325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be17b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e151baa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data for: ['PPH', 'XPH', 'IHE', 'IBB', 'XBI', 'XLV', 'VHT', 'SPY', 'VIXY']\n",
      "Safe fetch window: 2013-12-16 ‚Üí 2025-09-12\n",
      "Data prepared. Shape after safe fetch: (2952, 45)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2952, 45)\n",
      "(2952, 45)\n",
      "Registered feature: target_close with shift: no_shift\n",
      "Registered feature: VIXY_Open with shift: no_shift\n",
      "Registered feature: VIXY_High with shift: shift_1\n",
      "Registered feature: VIXY_Low with shift: shift_1\n",
      "Registered feature: VIXY_Close with shift: shift_1\n",
      "Registered feature: VIXY_Volume with shift: shift_1\n",
      "Registered feature: IHE_Open with shift: no_shift\n",
      "Registered feature: IHE_High with shift: shift_1\n",
      "Registered feature: IHE_Low with shift: shift_1\n",
      "Registered feature: IHE_Close with shift: shift_1\n",
      "Registered feature: IHE_Volume with shift: shift_1\n",
      "Registered feature: IBB_Open with shift: no_shift\n",
      "Registered feature: IBB_High with shift: shift_1\n",
      "Registered feature: IBB_Low with shift: shift_1\n",
      "Registered feature: IBB_Close with shift: shift_1\n",
      "Registered feature: IBB_Volume with shift: shift_1\n",
      "Registered feature: XBI_Open with shift: no_shift\n",
      "Registered feature: XBI_High with shift: shift_1\n",
      "Registered feature: XBI_Low with shift: shift_1\n",
      "Registered feature: XBI_Close with shift: shift_1\n",
      "Registered feature: XBI_Volume with shift: shift_1\n",
      "Registered feature: PPH_Open with shift: no_shift\n",
      "Registered feature: PPH_High with shift: shift_1\n",
      "Registered feature: PPH_Low with shift: shift_1\n",
      "Registered feature: PPH_Close with shift: shift_1\n",
      "Registered feature: PPH_Volume with shift: shift_1\n",
      "Registered feature: XPH_Open with shift: no_shift\n",
      "Registered feature: XPH_High with shift: shift_1\n",
      "Registered feature: XPH_Low with shift: shift_1\n",
      "Registered feature: XPH_Close with shift: shift_1\n",
      "Registered feature: XPH_Volume with shift: shift_1\n",
      "Registered feature: XLV_Open with shift: no_shift\n",
      "Registered feature: XLV_High with shift: shift_1\n",
      "Registered feature: XLV_Low with shift: shift_1\n",
      "Registered feature: XLV_Close with shift: shift_1\n",
      "Registered feature: XLV_Volume with shift: shift_1\n",
      "Registered feature: SPY_Open with shift: no_shift\n",
      "Registered feature: SPY_High with shift: shift_1\n",
      "Registered feature: SPY_Low with shift: shift_1\n",
      "Registered feature: SPY_Close with shift: shift_1\n",
      "Registered feature: SPY_Volume with shift: shift_1\n",
      "Registered feature: VHT_Open with shift: no_shift\n",
      "Registered feature: VHT_High with shift: shift_1\n",
      "Registered feature: VHT_Low with shift: shift_1\n",
      "Registered feature: VHT_Close with shift: shift_1\n",
      "Registered feature: VHT_Volume with shift: shift_1\n",
      "FINAL REGISTRY: {'target_close': 'no_shift', 'VIXY_Open': 'no_shift', 'VIXY_High': 'shift_1', 'VIXY_Low': 'shift_1', 'VIXY_Close': 'shift_1', 'VIXY_Volume': 'shift_1', 'IHE_Open': 'no_shift', 'IHE_High': 'shift_1', 'IHE_Low': 'shift_1', 'IHE_Close': 'shift_1', 'IHE_Volume': 'shift_1', 'IBB_Open': 'no_shift', 'IBB_High': 'shift_1', 'IBB_Low': 'shift_1', 'IBB_Close': 'shift_1', 'IBB_Volume': 'shift_1', 'XBI_Open': 'no_shift', 'XBI_High': 'shift_1', 'XBI_Low': 'shift_1', 'XBI_Close': 'shift_1', 'XBI_Volume': 'shift_1', 'PPH_Open': 'no_shift', 'PPH_High': 'shift_1', 'PPH_Low': 'shift_1', 'PPH_Close': 'shift_1', 'PPH_Volume': 'shift_1', 'XPH_Open': 'no_shift', 'XPH_High': 'shift_1', 'XPH_Low': 'shift_1', 'XPH_Close': 'shift_1', 'XPH_Volume': 'shift_1', 'XLV_Open': 'no_shift', 'XLV_High': 'shift_1', 'XLV_Low': 'shift_1', 'XLV_Close': 'shift_1', 'XLV_Volume': 'shift_1', 'SPY_Open': 'no_shift', 'SPY_High': 'shift_1', 'SPY_Low': 'shift_1', 'SPY_Close': 'shift_1', 'SPY_Volume': 'shift_1', 'VHT_Open': 'no_shift', 'VHT_High': 'shift_1', 'VHT_Low': 'shift_1', 'VHT_Close': 'shift_1', 'VHT_Volume': 'shift_1'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d863290132e64be4b3084c5a83640af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Per-ticker technicals:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered feature: PPH_Return_1d with shift: shift_1\n",
      "Registered feature: PPH_Return_5d with shift: shift_1\n",
      "Registered feature: PPH_Return_10d with shift: shift_1\n",
      "Registered feature: PPH_Return_20d with shift: shift_1\n",
      "Registered feature: PPH_SMA_10 with shift: shift_1\n",
      "Registered feature: PPH_EMA_20 with shift: shift_1\n",
      "Registered feature: PPH_MACD with shift: shift_1\n",
      "Registered feature: PPH_MACD_sig with shift: shift_1\n",
      "Registered feature: PPH_MACD_hist with shift: shift_1\n",
      "Registered feature: PPH_RSI_14 with shift: shift_1\n",
      "Registered feature: PPH_StochK with shift: shift_1\n",
      "Registered feature: PPH_StochD with shift: shift_1\n",
      "Registered feature: PPH_Vol_20 with shift: shift_1\n",
      "Registered feature: PPH_Parkinson_20 with shift: shift_1\n",
      "Registered feature: PPH_GK_20 with shift: shift_1\n",
      "Registered feature: PPH_BB_width with shift: shift_1\n",
      "Registered feature: PPH_Volume_ROC with shift: shift_1\n",
      "Registered feature: PPH_Volume_Z with shift: shift_1\n",
      "Registered feature: PPH_OBV with shift: shift_1\n",
      "Registered feature: PPH_Entropy_20 with shift: shift_1\n",
      "Registered feature: XPH_Return_1d with shift: shift_1\n",
      "Registered feature: XPH_Return_5d with shift: shift_1\n",
      "Registered feature: XPH_Return_10d with shift: shift_1\n",
      "Registered feature: XPH_Return_20d with shift: shift_1\n",
      "Registered feature: XPH_SMA_10 with shift: shift_1\n",
      "Registered feature: XPH_EMA_20 with shift: shift_1\n",
      "Registered feature: XPH_MACD with shift: shift_1\n",
      "Registered feature: XPH_MACD_sig with shift: shift_1\n",
      "Registered feature: XPH_MACD_hist with shift: shift_1\n",
      "Registered feature: XPH_RSI_14 with shift: shift_1\n",
      "Registered feature: XPH_StochK with shift: shift_1\n",
      "Registered feature: XPH_StochD with shift: shift_1\n",
      "Registered feature: XPH_Vol_20 with shift: shift_1\n",
      "Registered feature: XPH_Parkinson_20 with shift: shift_1\n",
      "Registered feature: XPH_GK_20 with shift: shift_1\n",
      "Registered feature: XPH_BB_width with shift: shift_1\n",
      "Registered feature: XPH_Volume_ROC with shift: shift_1\n",
      "Registered feature: XPH_Volume_Z with shift: shift_1\n",
      "Registered feature: XPH_OBV with shift: shift_1\n",
      "Registered feature: XPH_Entropy_20 with shift: shift_1\n",
      "Registered feature: IHE_Return_1d with shift: shift_1\n",
      "Registered feature: IHE_Return_5d with shift: shift_1\n",
      "Registered feature: IHE_Return_10d with shift: shift_1\n",
      "Registered feature: IHE_Return_20d with shift: shift_1\n",
      "Registered feature: IHE_SMA_10 with shift: shift_1\n",
      "Registered feature: IHE_EMA_20 with shift: shift_1\n",
      "Registered feature: IHE_MACD with shift: shift_1\n",
      "Registered feature: IHE_MACD_sig with shift: shift_1\n",
      "Registered feature: IHE_MACD_hist with shift: shift_1\n",
      "Registered feature: IHE_RSI_14 with shift: shift_1\n",
      "Registered feature: IHE_StochK with shift: shift_1\n",
      "Registered feature: IHE_StochD with shift: shift_1\n",
      "Registered feature: IHE_Vol_20 with shift: shift_1\n",
      "Registered feature: IHE_Parkinson_20 with shift: shift_1\n",
      "Registered feature: IHE_GK_20 with shift: shift_1\n",
      "Registered feature: IHE_BB_width with shift: shift_1\n",
      "Registered feature: IHE_Volume_ROC with shift: shift_1\n",
      "Registered feature: IHE_Volume_Z with shift: shift_1\n",
      "Registered feature: IHE_OBV with shift: shift_1\n",
      "Registered feature: IHE_Entropy_20 with shift: shift_1\n",
      "Registered feature: IBB_Return_1d with shift: shift_1\n",
      "Registered feature: IBB_Return_5d with shift: shift_1\n",
      "Registered feature: IBB_Return_10d with shift: shift_1\n",
      "Registered feature: IBB_Return_20d with shift: shift_1\n",
      "Registered feature: IBB_SMA_10 with shift: shift_1\n",
      "Registered feature: IBB_EMA_20 with shift: shift_1\n",
      "Registered feature: IBB_MACD with shift: shift_1\n",
      "Registered feature: IBB_MACD_sig with shift: shift_1\n",
      "Registered feature: IBB_MACD_hist with shift: shift_1\n",
      "Registered feature: IBB_RSI_14 with shift: shift_1\n",
      "Registered feature: IBB_StochK with shift: shift_1\n",
      "Registered feature: IBB_StochD with shift: shift_1\n",
      "Registered feature: IBB_Vol_20 with shift: shift_1\n",
      "Registered feature: IBB_Parkinson_20 with shift: shift_1\n",
      "Registered feature: IBB_GK_20 with shift: shift_1\n",
      "Registered feature: IBB_BB_width with shift: shift_1\n",
      "Registered feature: IBB_Volume_ROC with shift: shift_1\n",
      "Registered feature: IBB_Volume_Z with shift: shift_1\n",
      "Registered feature: IBB_OBV with shift: shift_1\n",
      "Registered feature: IBB_Entropy_20 with shift: shift_1\n",
      "Registered feature: XBI_Return_1d with shift: shift_1\n",
      "Registered feature: XBI_Return_5d with shift: shift_1\n",
      "Registered feature: XBI_Return_10d with shift: shift_1\n",
      "Registered feature: XBI_Return_20d with shift: shift_1\n",
      "Registered feature: XBI_SMA_10 with shift: shift_1\n",
      "Registered feature: XBI_EMA_20 with shift: shift_1\n",
      "Registered feature: XBI_MACD with shift: shift_1\n",
      "Registered feature: XBI_MACD_sig with shift: shift_1\n",
      "Registered feature: XBI_MACD_hist with shift: shift_1\n",
      "Registered feature: XBI_RSI_14 with shift: shift_1\n",
      "Registered feature: XBI_StochK with shift: shift_1\n",
      "Registered feature: XBI_StochD with shift: shift_1\n",
      "Registered feature: XBI_Vol_20 with shift: shift_1\n",
      "Registered feature: XBI_Parkinson_20 with shift: shift_1\n",
      "Registered feature: XBI_GK_20 with shift: shift_1\n",
      "Registered feature: XBI_BB_width with shift: shift_1\n",
      "Registered feature: XBI_Volume_ROC with shift: shift_1\n",
      "Registered feature: XBI_Volume_Z with shift: shift_1\n",
      "Registered feature: XBI_OBV with shift: shift_1\n",
      "Registered feature: XBI_Entropy_20 with shift: shift_1\n",
      "Registered feature: XLV_Return_1d with shift: shift_1\n",
      "Registered feature: XLV_Return_5d with shift: shift_1\n",
      "Registered feature: XLV_Return_10d with shift: shift_1\n",
      "Registered feature: XLV_Return_20d with shift: shift_1\n",
      "Registered feature: XLV_SMA_10 with shift: shift_1\n",
      "Registered feature: XLV_EMA_20 with shift: shift_1\n",
      "Registered feature: XLV_MACD with shift: shift_1\n",
      "Registered feature: XLV_MACD_sig with shift: shift_1\n",
      "Registered feature: XLV_MACD_hist with shift: shift_1\n",
      "Registered feature: XLV_RSI_14 with shift: shift_1\n",
      "Registered feature: XLV_StochK with shift: shift_1\n",
      "Registered feature: XLV_StochD with shift: shift_1\n",
      "Registered feature: XLV_Vol_20 with shift: shift_1\n",
      "Registered feature: XLV_Parkinson_20 with shift: shift_1\n",
      "Registered feature: XLV_GK_20 with shift: shift_1\n",
      "Registered feature: XLV_BB_width with shift: shift_1\n",
      "Registered feature: XLV_Volume_ROC with shift: shift_1\n",
      "Registered feature: XLV_Volume_Z with shift: shift_1\n",
      "Registered feature: XLV_OBV with shift: shift_1\n",
      "Registered feature: XLV_Entropy_20 with shift: shift_1\n",
      "Registered feature: VHT_Return_1d with shift: shift_1\n",
      "Registered feature: VHT_Return_5d with shift: shift_1\n",
      "Registered feature: VHT_Return_10d with shift: shift_1\n",
      "Registered feature: VHT_Return_20d with shift: shift_1\n",
      "Registered feature: VHT_SMA_10 with shift: shift_1\n",
      "Registered feature: VHT_EMA_20 with shift: shift_1\n",
      "Registered feature: VHT_MACD with shift: shift_1\n",
      "Registered feature: VHT_MACD_sig with shift: shift_1\n",
      "Registered feature: VHT_MACD_hist with shift: shift_1\n",
      "Registered feature: VHT_RSI_14 with shift: shift_1\n",
      "Registered feature: VHT_StochK with shift: shift_1\n",
      "Registered feature: VHT_StochD with shift: shift_1\n",
      "Registered feature: VHT_Vol_20 with shift: shift_1\n",
      "Registered feature: VHT_Parkinson_20 with shift: shift_1\n",
      "Registered feature: VHT_GK_20 with shift: shift_1\n",
      "Registered feature: VHT_BB_width with shift: shift_1\n",
      "Registered feature: VHT_Volume_ROC with shift: shift_1\n",
      "Registered feature: VHT_Volume_Z with shift: shift_1\n",
      "Registered feature: VHT_OBV with shift: shift_1\n",
      "Registered feature: VHT_Entropy_20 with shift: shift_1\n",
      "Registered feature: SPY_Return_1d with shift: shift_1\n",
      "Registered feature: SPY_Return_5d with shift: shift_1\n",
      "Registered feature: SPY_Return_10d with shift: shift_1\n",
      "Registered feature: SPY_Return_20d with shift: shift_1\n",
      "Registered feature: SPY_SMA_10 with shift: shift_1\n",
      "Registered feature: SPY_EMA_20 with shift: shift_1\n",
      "Registered feature: SPY_MACD with shift: shift_1\n",
      "Registered feature: SPY_MACD_sig with shift: shift_1\n",
      "Registered feature: SPY_MACD_hist with shift: shift_1\n",
      "Registered feature: SPY_RSI_14 with shift: shift_1\n",
      "Registered feature: SPY_StochK with shift: shift_1\n",
      "Registered feature: SPY_StochD with shift: shift_1\n",
      "Registered feature: SPY_Vol_20 with shift: shift_1\n",
      "Registered feature: SPY_Parkinson_20 with shift: shift_1\n",
      "Registered feature: SPY_GK_20 with shift: shift_1\n",
      "Registered feature: SPY_BB_width with shift: shift_1\n",
      "Registered feature: SPY_Volume_ROC with shift: shift_1\n",
      "Registered feature: SPY_Volume_Z with shift: shift_1\n",
      "Registered feature: SPY_OBV with shift: shift_1\n",
      "Registered feature: SPY_Entropy_20 with shift: shift_1\n",
      "Registered feature: VIXY_Return_1d with shift: shift_1\n",
      "Registered feature: VIXY_Return_5d with shift: shift_1\n",
      "Registered feature: VIXY_Return_10d with shift: shift_1\n",
      "Registered feature: VIXY_Return_20d with shift: shift_1\n",
      "Registered feature: VIXY_SMA_10 with shift: shift_1\n",
      "Registered feature: VIXY_EMA_20 with shift: shift_1\n",
      "Registered feature: VIXY_MACD with shift: shift_1\n",
      "Registered feature: VIXY_MACD_sig with shift: shift_1\n",
      "Registered feature: VIXY_MACD_hist with shift: shift_1\n",
      "Registered feature: VIXY_RSI_14 with shift: shift_1\n",
      "Registered feature: VIXY_StochK with shift: shift_1\n",
      "Registered feature: VIXY_StochD with shift: shift_1\n",
      "Registered feature: VIXY_Vol_20 with shift: shift_1\n",
      "Registered feature: VIXY_Parkinson_20 with shift: shift_1\n",
      "Registered feature: VIXY_GK_20 with shift: shift_1\n",
      "Registered feature: VIXY_BB_width with shift: shift_1\n",
      "Registered feature: VIXY_Volume_ROC with shift: shift_1\n",
      "Registered feature: VIXY_Volume_Z with shift: shift_1\n",
      "Registered feature: VIXY_OBV with shift: shift_1\n",
      "Registered feature: VIXY_Entropy_20 with shift: shift_1\n",
      "(2952, 226)\n",
      "Registered feature: XPH_Ratio_PPH with shift: shift_1\n",
      "Registered feature: XPH_RS_20 with shift: shift_1\n",
      "Registered feature: IHE_Ratio_PPH with shift: shift_1\n",
      "Registered feature: IHE_RS_20 with shift: shift_1\n",
      "Registered feature: IBB_Ratio_PPH with shift: shift_1\n",
      "Registered feature: IBB_RS_20 with shift: shift_1\n",
      "Registered feature: XBI_Ratio_PPH with shift: shift_1\n",
      "Registered feature: XBI_RS_20 with shift: shift_1\n",
      "Registered feature: XLV_Ratio_PPH with shift: shift_1\n",
      "Registered feature: XLV_RS_20 with shift: shift_1\n",
      "Registered feature: VHT_Ratio_PPH with shift: shift_1\n",
      "Registered feature: VHT_RS_20 with shift: shift_1\n",
      "Registered feature: SPY_Ratio_PPH with shift: shift_1\n",
      "Registered feature: SPY_RS_20 with shift: shift_1\n",
      "Registered feature: VIXY_Ratio_PPH with shift: shift_1\n",
      "Registered feature: VIXY_RS_20 with shift: shift_1\n",
      "Registered feature: PCA_1 with shift: shift_1\n",
      "Registered feature: PCA_2 with shift: shift_1\n",
      "Registered feature: PCA_3 with shift: shift_1\n",
      "Registered feature: day_of_week with shift: no_shift\n",
      "Registered feature: day_of_month with shift: no_shift\n",
      "Registered feature: month with shift: no_shift\n",
      "Registered feature: quarter with shift: no_shift\n",
      "Registered feature: is_month_end with shift: no_shift\n",
      "Registered feature: is_month_start with shift: no_shift\n",
      "Registered feature: is_holiday_adjacent with shift: no_shift\n",
      "Registered feature: is_opex_week with shift: no_shift\n",
      "Registered feature: is_fomc_day with shift: no_shift\n",
      "Registered feature: days_to_fomc with shift: no_shift\n",
      "Registered feature: days_since_fomc with shift: no_shift\n",
      "Registered feature: is_cpi_day with shift: no_shift\n",
      "Registered feature: days_to_cpi with shift: no_shift\n",
      "Registered feature: days_since_cpi with shift: no_shift\n",
      "Registered feature: is_nfp_day with shift: no_shift\n",
      "Registered feature: days_to_nfp with shift: no_shift\n",
      "Registered feature: days_since_nfp with shift: no_shift\n",
      "Registered feature: is_ppi_day with shift: no_shift\n",
      "Registered feature: days_to_ppi with shift: no_shift\n",
      "Registered feature: days_since_ppi with shift: no_shift\n",
      "Registered feature: is_gdp_day with shift: no_shift\n",
      "Registered feature: days_to_gdp with shift: no_shift\n",
      "Registered feature: days_since_gdp with shift: no_shift\n",
      "(2952, 69)\n",
      "(2952, 268)\n",
      "(2952, 268)\n",
      "(2952, 268)\n",
      "Markdown feature documentation written to: docs/features_no_clean.md\n",
      "(2891, 268)\n"
     ]
    }
   ],
   "source": [
    "TARGET_TICKER = \"PPH\"\n",
    "SUPPORT_TICKERS = [\n",
    "    \"XPH\", \"IHE\", \"IBB\", \"XBI\", \"XLV\", \"VHT\", \"SPY\", \"VIXY\"\n",
    "]\n",
    "\n",
    "START_DATE = \"2014-02-04\"\n",
    "END_DATE = \"2025-08-03\"\n",
    "HORIZON = 30\n",
    "USE_HMM = True\n",
    "df_final = build_feature_dataset(\n",
    "    target=TARGET_TICKER,\n",
    "    support_tickers=SUPPORT_TICKERS,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    horizon=HORIZON,\n",
    "    markdown_output_path=\"docs/features_no_clean.md\"    # Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e571e2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIXY_Open</th>\n",
       "      <th>VIXY_High_t-1</th>\n",
       "      <th>VIXY_Low_t-1</th>\n",
       "      <th>VIXY_Close_t-1</th>\n",
       "      <th>VIXY_Volume_t-1</th>\n",
       "      <th>IHE_Open</th>\n",
       "      <th>IHE_High_t-1</th>\n",
       "      <th>IHE_Low_t-1</th>\n",
       "      <th>IHE_Close_t-1</th>\n",
       "      <th>IHE_Volume_t-1</th>\n",
       "      <th>...</th>\n",
       "      <th>XLV_RS_20_t-1</th>\n",
       "      <th>VHT_Ratio_PPH_t-1</th>\n",
       "      <th>VHT_RS_20_t-1</th>\n",
       "      <th>SPY_Ratio_PPH_t-1</th>\n",
       "      <th>SPY_RS_20_t-1</th>\n",
       "      <th>VIXY_Ratio_PPH_t-1</th>\n",
       "      <th>VIXY_RS_20_t-1</th>\n",
       "      <th>PCA_1_t-1</th>\n",
       "      <th>PCA_2_t-1</th>\n",
       "      <th>PCA_3_t-1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-02-04</th>\n",
       "      <td>55728.0</td>\n",
       "      <td>57232.0</td>\n",
       "      <td>52784.0</td>\n",
       "      <td>56624.0</td>\n",
       "      <td>3322.0</td>\n",
       "      <td>39.639999</td>\n",
       "      <td>40.543331</td>\n",
       "      <td>39.310001</td>\n",
       "      <td>39.356667</td>\n",
       "      <td>135900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012533</td>\n",
       "      <td>1.892965</td>\n",
       "      <td>-0.007081</td>\n",
       "      <td>3.276336</td>\n",
       "      <td>-0.052974</td>\n",
       "      <td>1065.161779</td>\n",
       "      <td>0.218426</td>\n",
       "      <td>0.105332</td>\n",
       "      <td>-0.025382</td>\n",
       "      <td>-0.001905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-05</th>\n",
       "      <td>56640.0</td>\n",
       "      <td>56688.0</td>\n",
       "      <td>54736.0</td>\n",
       "      <td>55504.0</td>\n",
       "      <td>4969.0</td>\n",
       "      <td>39.803333</td>\n",
       "      <td>39.869999</td>\n",
       "      <td>39.496666</td>\n",
       "      <td>39.813332</td>\n",
       "      <td>71100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011100</td>\n",
       "      <td>1.893256</td>\n",
       "      <td>-0.005572</td>\n",
       "      <td>3.267325</td>\n",
       "      <td>-0.056044</td>\n",
       "      <td>1033.979130</td>\n",
       "      <td>0.196813</td>\n",
       "      <td>-0.029754</td>\n",
       "      <td>0.010988</td>\n",
       "      <td>-0.003979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-06</th>\n",
       "      <td>56880.0</td>\n",
       "      <td>59056.0</td>\n",
       "      <td>56096.0</td>\n",
       "      <td>57536.0</td>\n",
       "      <td>862.0</td>\n",
       "      <td>39.820000</td>\n",
       "      <td>39.869999</td>\n",
       "      <td>39.246666</td>\n",
       "      <td>39.633331</td>\n",
       "      <td>138600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018914</td>\n",
       "      <td>1.876672</td>\n",
       "      <td>-0.014781</td>\n",
       "      <td>3.253529</td>\n",
       "      <td>-0.054854</td>\n",
       "      <td>1068.647842</td>\n",
       "      <td>0.277941</td>\n",
       "      <td>0.044851</td>\n",
       "      <td>-0.000998</td>\n",
       "      <td>-0.013096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-07</th>\n",
       "      <td>50448.0</td>\n",
       "      <td>56880.0</td>\n",
       "      <td>51792.0</td>\n",
       "      <td>51792.0</td>\n",
       "      <td>744.0</td>\n",
       "      <td>39.776669</td>\n",
       "      <td>39.886665</td>\n",
       "      <td>39.473331</td>\n",
       "      <td>39.613335</td>\n",
       "      <td>64800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014334</td>\n",
       "      <td>1.880096</td>\n",
       "      <td>-0.012680</td>\n",
       "      <td>3.289103</td>\n",
       "      <td>-0.034769</td>\n",
       "      <td>959.822107</td>\n",
       "      <td>0.155041</td>\n",
       "      <td>-0.082527</td>\n",
       "      <td>-0.055760</td>\n",
       "      <td>-0.001468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-10</th>\n",
       "      <td>48416.0</td>\n",
       "      <td>50688.0</td>\n",
       "      <td>47568.0</td>\n",
       "      <td>48720.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>40.450001</td>\n",
       "      <td>40.450001</td>\n",
       "      <td>39.776669</td>\n",
       "      <td>40.130001</td>\n",
       "      <td>52200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015535</td>\n",
       "      <td>1.880459</td>\n",
       "      <td>-0.013784</td>\n",
       "      <td>3.269287</td>\n",
       "      <td>-0.034092</td>\n",
       "      <td>886.462897</td>\n",
       "      <td>0.073808</td>\n",
       "      <td>-0.091241</td>\n",
       "      <td>0.035934</td>\n",
       "      <td>0.017027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            VIXY_Open  VIXY_High_t-1  VIXY_Low_t-1  VIXY_Close_t-1  \\\n",
       "Date                                                                 \n",
       "2014-02-04    55728.0        57232.0       52784.0         56624.0   \n",
       "2014-02-05    56640.0        56688.0       54736.0         55504.0   \n",
       "2014-02-06    56880.0        59056.0       56096.0         57536.0   \n",
       "2014-02-07    50448.0        56880.0       51792.0         51792.0   \n",
       "2014-02-10    48416.0        50688.0       47568.0         48720.0   \n",
       "\n",
       "            VIXY_Volume_t-1   IHE_Open  IHE_High_t-1  IHE_Low_t-1  \\\n",
       "Date                                                                \n",
       "2014-02-04           3322.0  39.639999     40.543331    39.310001   \n",
       "2014-02-05           4969.0  39.803333     39.869999    39.496666   \n",
       "2014-02-06            862.0  39.820000     39.869999    39.246666   \n",
       "2014-02-07            744.0  39.776669     39.886665    39.473331   \n",
       "2014-02-10            714.0  40.450001     40.450001    39.776669   \n",
       "\n",
       "            IHE_Close_t-1  IHE_Volume_t-1  ...  XLV_RS_20_t-1  \\\n",
       "Date                                       ...                  \n",
       "2014-02-04      39.356667        135900.0  ...      -0.012533   \n",
       "2014-02-05      39.813332         71100.0  ...      -0.011100   \n",
       "2014-02-06      39.633331        138600.0  ...      -0.018914   \n",
       "2014-02-07      39.613335         64800.0  ...      -0.014334   \n",
       "2014-02-10      40.130001         52200.0  ...      -0.015535   \n",
       "\n",
       "            VHT_Ratio_PPH_t-1  VHT_RS_20_t-1  SPY_Ratio_PPH_t-1  \\\n",
       "Date                                                              \n",
       "2014-02-04           1.892965      -0.007081           3.276336   \n",
       "2014-02-05           1.893256      -0.005572           3.267325   \n",
       "2014-02-06           1.876672      -0.014781           3.253529   \n",
       "2014-02-07           1.880096      -0.012680           3.289103   \n",
       "2014-02-10           1.880459      -0.013784           3.269287   \n",
       "\n",
       "            SPY_RS_20_t-1  VIXY_Ratio_PPH_t-1  VIXY_RS_20_t-1  PCA_1_t-1  \\\n",
       "Date                                                                       \n",
       "2014-02-04      -0.052974         1065.161779        0.218426   0.105332   \n",
       "2014-02-05      -0.056044         1033.979130        0.196813  -0.029754   \n",
       "2014-02-06      -0.054854         1068.647842        0.277941   0.044851   \n",
       "2014-02-07      -0.034769          959.822107        0.155041  -0.082527   \n",
       "2014-02-10      -0.034092          886.462897        0.073808  -0.091241   \n",
       "\n",
       "            PCA_2_t-1  PCA_3_t-1  \n",
       "Date                              \n",
       "2014-02-04  -0.025382  -0.001905  \n",
       "2014-02-05   0.010988  -0.003979  \n",
       "2014-02-06  -0.000998  -0.013096  \n",
       "2014-02-07  -0.055760  -0.001468  \n",
       "2014-02-10   0.035934   0.017027  \n",
       "\n",
       "[5 rows x 268 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2265a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def validate_final_dataframe(\n",
    "    df,\n",
    "    feature_registry,\n",
    "    horizon,\n",
    "    requested_start_date,\n",
    "    requested_end_date,\n",
    "    target_col=\"target_close\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Validates the completed post-shift feature matrix.\n",
    "\n",
    "    Checks:\n",
    "        - NaNs / Inf\n",
    "        - DatetimeIndex integrity\n",
    "        - Start & end dates correct (relative to user input)\n",
    "        - Expected number of columns based on registry + horizon\n",
    "        - No all-NaN or constant columns\n",
    "        - Sampled shift sanity checks\n",
    "        - Target column validity\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîç VALIDATING FINAL DATAFRAME\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    ok = True\n",
    "\n",
    "    def good(msg):\n",
    "        print(f\"  ‚úÖ {msg}\")\n",
    "\n",
    "    def bad(msg):\n",
    "        nonlocal ok\n",
    "        ok = False\n",
    "        print(f\"  ‚ùå {msg}\")\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 1. Basic Data Health\n",
    "    # -------------------------------------------------------------\n",
    "    if df.isna().any().any():\n",
    "        bad(\"DataFrame contains NaN values.\")\n",
    "    else:\n",
    "        good(\"No NaN values found.\")\n",
    "\n",
    "    if np.isinf(df.select_dtypes(include=[np.number])).any().any():\n",
    "        bad(\"DataFrame contains infinite values.\")\n",
    "    else:\n",
    "        good(\"No infinite values detected.\")\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 2. Index Integrity\n",
    "    # -------------------------------------------------------------\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        bad(\"Index is not a DatetimeIndex.\")\n",
    "    else:\n",
    "        good(\"Index type OK (DatetimeIndex).\")\n",
    "\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        bad(\"Index not sorted.\")\n",
    "    else:\n",
    "        good(\"Index sorted ascending.\")\n",
    "\n",
    "    if df.index.duplicated().any():\n",
    "        bad(\"Duplicate timestamps present in index.\")\n",
    "    else:\n",
    "        good(\"No duplicate timestamps.\")\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 3. EXACT START AND END DATE VALIDATION\n",
    "    # -------------------------------------------------------------\n",
    "    req_start = pd.Timestamp(requested_start_date)\n",
    "    req_end   = pd.Timestamp(requested_end_date)\n",
    "\n",
    "    first_df_date = df.index.min()\n",
    "    last_df_date  = df.index.max()\n",
    "\n",
    "    # ---- START DATE MUST MATCH EXACTLY ----\n",
    "    if first_df_date != req_start:\n",
    "        bad(\n",
    "            f\"Start date mismatch: DF begins at {first_df_date.date()}, \"\n",
    "            f\"but requested start was {req_start.date()}. \"\n",
    "            f\"This indicates Step A failed to expand the lookback window \"\n",
    "            f\"or the ticker lacks sufficient historical data.\"\n",
    "        )\n",
    "    else:\n",
    "        good(f\"Start date correct: {first_df_date.date()}\")\n",
    "\n",
    "    # ---- END DATE MUST MATCH EXACTLY ----\n",
    "    if last_df_date != req_end:\n",
    "        bad(\n",
    "            f\"End date mismatch: DF ends at {last_df_date.date()}, \"\n",
    "            f\"but requested end was {req_end.date()}.\"\n",
    "        )\n",
    "    else:\n",
    "        good(f\"End date correct: {last_df_date.date()}\")\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 4. EXPECTED COLUMN COUNT (Registry + Horizon)\n",
    "    # -------------------------------------------------------------\n",
    "    # Number of base columns the registry knows about\n",
    "    base_cols = len(feature_registry)\n",
    "\n",
    "    # How many of those are shift_plus_k?\n",
    "    shift_plus_k_cols = [\n",
    "        f for f, rule in feature_registry.items()\n",
    "        if rule == \"shift_plus_k\"\n",
    "    ]\n",
    "\n",
    "    expected_final_columns = (\n",
    "        # each base feature (no_shift + shift_1) becomes exactly 1 column\n",
    "        len(feature_registry)\n",
    "        +\n",
    "        # each shift_plus_k feature creates H additional columns\n",
    "        len(shift_plus_k_cols) * horizon\n",
    "    )\n",
    "\n",
    "    actual_columns = df.shape[1]\n",
    "\n",
    "    if expected_final_columns != actual_columns:\n",
    "        bad(\n",
    "            f\"Column count mismatch:\\n\"\n",
    "            f\"  Expected (registry+horizon): {expected_final_columns}\\n\"\n",
    "            f\"  Actual df columns:           {actual_columns}\"\n",
    "        )\n",
    "    else:\n",
    "        good(\n",
    "            f\"Column count correct: {actual_columns} \"\n",
    "            f\"(matches registry+horizon projection)\"\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 5. No all-NaN columns\n",
    "    # -------------------------------------------------------------\n",
    "    all_nan_cols = df.columns[df.isna().all()]\n",
    "    if len(all_nan_cols) > 0:\n",
    "        bad(f\"{len(all_nan_cols)} columns are all NaN.\")\n",
    "    else:\n",
    "        good(\"No empty (all-NaN) columns.\")\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 6. Constant columns\n",
    "    # -------------------------------------------------------------\n",
    "    const_cols = [c for c in df.columns if df[c].nunique() <= 1]\n",
    "    if len(const_cols) > 0:\n",
    "        bad(f\"{len(const_cols)} constant columns detected: {const_cols}\")\n",
    "    else:\n",
    "        good(\"No constant columns.\")\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 7. SHIFT SANITY CHECK (sample)\n",
    "    # -------------------------------------------------------------\n",
    "    sample_shifted = [c for c in df.columns if c.endswith(\"_t-1\")][:8]\n",
    "\n",
    "    for col in sample_shifted:\n",
    "        base = col[:-4]  # remove _t-1 suffix\n",
    "        if base in df.columns:\n",
    "            if not df[col].shift(-1).equals(df[base]):\n",
    "                bad(f\"Shift alignment error: {col} is not base shifted by 1.\")\n",
    "            else:\n",
    "                good(f\"{col} correctly aligned with base {base}.\")\n",
    "        else:\n",
    "            good(f\"{col}: no base column found (OK for calendar/macro).\")\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 8. Target sanity\n",
    "    # -------------------------------------------------------------\n",
    "    if target_col in df.columns:\n",
    "        tgt = df[target_col]\n",
    "        if tgt.isna().any() or (tgt <= 0).any():\n",
    "            bad(f\"Target column '{target_col}' contains invalid values.\")\n",
    "        else:\n",
    "            good(f\"Target column '{target_col}' looks valid.\")\n",
    "    else:\n",
    "        bad(f\"Target column '{target_col}' not found in final dataset.\")\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # SUMMARY\n",
    "    # -------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    if ok:\n",
    "        print(\"üéâ FINAL DATAFRAME VALID ‚Äî All checks passed.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è FINAL DATAFRAME **NOT** VALID ‚Äî See issues above.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    return ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba389117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç VALIDATING FINAL DATAFRAME\n",
      "================================================================================\n",
      "  ‚úÖ No NaN values found.\n",
      "  ‚úÖ No infinite values detected.\n",
      "  ‚úÖ Index type OK (DatetimeIndex).\n",
      "  ‚úÖ Index sorted ascending.\n",
      "  ‚úÖ No duplicate timestamps.\n",
      "  ‚úÖ Start date correct: 2014-02-04\n",
      "  ‚ùå End date mismatch: DF ends at 2025-08-01, but requested end was 2025-08-03.\n",
      "  ‚úÖ Column count correct: 268 (matches registry+horizon projection)\n",
      "  ‚úÖ No empty (all-NaN) columns.\n",
      "  ‚úÖ No constant columns.\n",
      "  ‚úÖ VIXY_High_t-1: no base column found (OK for calendar/macro).\n",
      "  ‚úÖ VIXY_Low_t-1: no base column found (OK for calendar/macro).\n",
      "  ‚úÖ VIXY_Close_t-1: no base column found (OK for calendar/macro).\n",
      "  ‚úÖ VIXY_Volume_t-1: no base column found (OK for calendar/macro).\n",
      "  ‚úÖ IHE_High_t-1: no base column found (OK for calendar/macro).\n",
      "  ‚úÖ IHE_Low_t-1: no base column found (OK for calendar/macro).\n",
      "  ‚úÖ IHE_Close_t-1: no base column found (OK for calendar/macro).\n",
      "  ‚úÖ IHE_Volume_t-1: no base column found (OK for calendar/macro).\n",
      "  ‚úÖ Target column 'target_close' looks valid.\n",
      "\n",
      "================================================================================\n",
      "‚ö†Ô∏è FINAL DATAFRAME **NOT** VALID ‚Äî See issues above.\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_final_dataframe(\n",
    "    df_final,\n",
    "    feature_registry,\n",
    "    horizon=HORIZON,\n",
    "    requested_start_date= START_DATE,\n",
    "    requested_end_date= END_DATE,\n",
    "    target_col=\"target_close\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f03f022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (22.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c9c765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_parquet(\"data/final_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc728b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2891, 268)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccdfca1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VIXY_Open                0\n",
       "XLV_BB_width_t-1         0\n",
       "XLV_Return_5d_t-1        0\n",
       "XLV_Return_10d_t-1       0\n",
       "XLV_Return_20d_t-1       0\n",
       "XLV_SMA_10_t-1           0\n",
       "XLV_EMA_20_t-1           0\n",
       "XLV_MACD_t-1             0\n",
       "XLV_MACD_sig_t-1         0\n",
       "XLV_MACD_hist_t-1        0\n",
       "XLV_RSI_14_t-1           0\n",
       "XLV_StochK_t-1           0\n",
       "XLV_StochD_t-1           0\n",
       "XLV_Vol_20_t-1           0\n",
       "XLV_Parkinson_20_t-1     0\n",
       "XLV_GK_20_t-1            0\n",
       "XLV_Volume_ROC_t-1       0\n",
       "XBI_Entropy_20_t-1       0\n",
       "XLV_Volume_Z_t-1         0\n",
       "XLV_OBV_t-1              0\n",
       "XLV_Entropy_20_t-1       0\n",
       "VHT_Return_1d_t-1        0\n",
       "VHT_Return_5d_t-1        0\n",
       "VHT_Return_10d_t-1       0\n",
       "VHT_Return_20d_t-1       0\n",
       "VHT_SMA_10_t-1           0\n",
       "VHT_EMA_20_t-1           0\n",
       "VHT_MACD_t-1             0\n",
       "VHT_MACD_sig_t-1         0\n",
       "VHT_MACD_hist_t-1        0\n",
       "VHT_RSI_14_t-1           0\n",
       "VHT_StochK_t-1           0\n",
       "XLV_Return_1d_t-1        0\n",
       "XBI_OBV_t-1              0\n",
       "VIXY_High_t-1            0\n",
       "XBI_Return_5d_t-1        0\n",
       "IBB_MACD_sig_t-1         0\n",
       "IBB_MACD_hist_t-1        0\n",
       "IBB_RSI_14_t-1           0\n",
       "IBB_StochK_t-1           0\n",
       "IBB_StochD_t-1           0\n",
       "IBB_Vol_20_t-1           0\n",
       "IBB_Parkinson_20_t-1     0\n",
       "IBB_GK_20_t-1            0\n",
       "IBB_BB_width_t-1         0\n",
       "IBB_Volume_ROC_t-1       0\n",
       "IBB_Volume_Z_t-1         0\n",
       "IBB_OBV_t-1              0\n",
       "IBB_Entropy_20_t-1       0\n",
       "XBI_Return_1d_t-1        0\n",
       "XBI_Return_10d_t-1       0\n",
       "XBI_Volume_Z_t-1         0\n",
       "XBI_Return_20d_t-1       0\n",
       "XBI_SMA_10_t-1           0\n",
       "XBI_EMA_20_t-1           0\n",
       "XBI_MACD_t-1             0\n",
       "XBI_MACD_sig_t-1         0\n",
       "XBI_MACD_hist_t-1        0\n",
       "XBI_RSI_14_t-1           0\n",
       "XBI_StochK_t-1           0\n",
       "XBI_StochD_t-1           0\n",
       "XBI_Vol_20_t-1           0\n",
       "XBI_Parkinson_20_t-1     0\n",
       "XBI_GK_20_t-1            0\n",
       "XBI_BB_width_t-1         0\n",
       "XBI_Volume_ROC_t-1       0\n",
       "VHT_StochD_t-1           0\n",
       "VHT_Vol_20_t-1           0\n",
       "VHT_Parkinson_20_t-1     0\n",
       "IHE_Ratio_PPH_t-1        0\n",
       "VIXY_MACD_hist_t-1       0\n",
       "VIXY_RSI_14_t-1          0\n",
       "VIXY_StochK_t-1          0\n",
       "VIXY_StochD_t-1          0\n",
       "VIXY_Vol_20_t-1          0\n",
       "VIXY_Parkinson_20_t-1    0\n",
       "VIXY_GK_20_t-1           0\n",
       "VIXY_BB_width_t-1        0\n",
       "VIXY_Volume_ROC_t-1      0\n",
       "VIXY_Volume_Z_t-1        0\n",
       "VIXY_OBV_t-1             0\n",
       "VIXY_Entropy_20_t-1      0\n",
       "XPH_Ratio_PPH_t-1        0\n",
       "XPH_RS_20_t-1            0\n",
       "IHE_RS_20_t-1            0\n",
       "VHT_GK_20_t-1            0\n",
       "IBB_Ratio_PPH_t-1        0\n",
       "IBB_RS_20_t-1            0\n",
       "XBI_Ratio_PPH_t-1        0\n",
       "XBI_RS_20_t-1            0\n",
       "XLV_Ratio_PPH_t-1        0\n",
       "XLV_RS_20_t-1            0\n",
       "VHT_Ratio_PPH_t-1        0\n",
       "VHT_RS_20_t-1            0\n",
       "SPY_Ratio_PPH_t-1        0\n",
       "SPY_RS_20_t-1            0\n",
       "VIXY_Ratio_PPH_t-1       0\n",
       "VIXY_RS_20_t-1           0\n",
       "PCA_1_t-1                0\n",
       "PCA_2_t-1                0\n",
       "VIXY_MACD_sig_t-1        0\n",
       "VIXY_MACD_t-1            0\n",
       "VIXY_EMA_20_t-1          0\n",
       "VIXY_SMA_10_t-1          0\n",
       "VHT_BB_width_t-1         0\n",
       "VHT_Volume_ROC_t-1       0\n",
       "VHT_Volume_Z_t-1         0\n",
       "VHT_OBV_t-1              0\n",
       "VHT_Entropy_20_t-1       0\n",
       "SPY_Return_1d_t-1        0\n",
       "SPY_Return_5d_t-1        0\n",
       "SPY_Return_10d_t-1       0\n",
       "SPY_Return_20d_t-1       0\n",
       "SPY_SMA_10_t-1           0\n",
       "SPY_EMA_20_t-1           0\n",
       "SPY_MACD_t-1             0\n",
       "SPY_MACD_sig_t-1         0\n",
       "SPY_MACD_hist_t-1        0\n",
       "SPY_RSI_14_t-1           0\n",
       "SPY_StochK_t-1           0\n",
       "SPY_StochD_t-1           0\n",
       "SPY_Vol_20_t-1           0\n",
       "SPY_Parkinson_20_t-1     0\n",
       "SPY_GK_20_t-1            0\n",
       "SPY_BB_width_t-1         0\n",
       "SPY_Volume_ROC_t-1       0\n",
       "SPY_Volume_Z_t-1         0\n",
       "SPY_OBV_t-1              0\n",
       "SPY_Entropy_20_t-1       0\n",
       "VIXY_Return_1d_t-1       0\n",
       "VIXY_Return_5d_t-1       0\n",
       "VIXY_Return_10d_t-1      0\n",
       "VIXY_Return_20d_t-1      0\n",
       "IBB_MACD_t-1             0\n",
       "IBB_EMA_20_t-1           0\n",
       "IBB_SMA_10_t-1           0\n",
       "quarter                  0\n",
       "SPY_Open                 0\n",
       "SPY_High_t-1             0\n",
       "SPY_Low_t-1              0\n",
       "SPY_Close_t-1            0\n",
       "SPY_Volume_t-1           0\n",
       "VHT_Open                 0\n",
       "VHT_High_t-1             0\n",
       "VHT_Low_t-1              0\n",
       "VHT_Close_t-1            0\n",
       "VHT_Volume_t-1           0\n",
       "target_close             0\n",
       "day_of_week              0\n",
       "day_of_month             0\n",
       "month                    0\n",
       "is_month_end             0\n",
       "is_gdp_day               0\n",
       "is_month_start           0\n",
       "is_holiday_adjacent      0\n",
       "is_opex_week             0\n",
       "is_fomc_day              0\n",
       "days_to_fomc             0\n",
       "days_since_fomc          0\n",
       "is_cpi_day               0\n",
       "days_to_cpi              0\n",
       "days_since_cpi           0\n",
       "is_nfp_day               0\n",
       "days_to_nfp              0\n",
       "days_since_nfp           0\n",
       "is_ppi_day               0\n",
       "days_to_ppi              0\n",
       "XLV_Volume_t-1           0\n",
       "XLV_Close_t-1            0\n",
       "XLV_Low_t-1              0\n",
       "XLV_High_t-1             0\n",
       "VIXY_Low_t-1             0\n",
       "VIXY_Close_t-1           0\n",
       "VIXY_Volume_t-1          0\n",
       "IHE_Open                 0\n",
       "IHE_High_t-1             0\n",
       "IHE_Low_t-1              0\n",
       "IHE_Close_t-1            0\n",
       "IHE_Volume_t-1           0\n",
       "IBB_Open                 0\n",
       "IBB_High_t-1             0\n",
       "IBB_Low_t-1              0\n",
       "IBB_Close_t-1            0\n",
       "IBB_Volume_t-1           0\n",
       "XBI_Open                 0\n",
       "XBI_High_t-1             0\n",
       "XBI_Low_t-1              0\n",
       "XBI_Close_t-1            0\n",
       "XBI_Volume_t-1           0\n",
       "PPH_Open                 0\n",
       "PPH_High_t-1             0\n",
       "PPH_Low_t-1              0\n",
       "PPH_Close_t-1            0\n",
       "PPH_Volume_t-1           0\n",
       "XPH_Open                 0\n",
       "XPH_High_t-1             0\n",
       "XPH_Low_t-1              0\n",
       "XPH_Close_t-1            0\n",
       "XPH_Volume_t-1           0\n",
       "XLV_Open                 0\n",
       "days_since_ppi           0\n",
       "days_to_gdp              0\n",
       "IBB_Return_20d_t-1       0\n",
       "IHE_MACD_sig_t-1         0\n",
       "XPH_Parkinson_20_t-1     0\n",
       "XPH_GK_20_t-1            0\n",
       "XPH_BB_width_t-1         0\n",
       "XPH_Volume_ROC_t-1       0\n",
       "XPH_Volume_Z_t-1         0\n",
       "XPH_OBV_t-1              0\n",
       "XPH_Entropy_20_t-1       0\n",
       "IHE_Return_1d_t-1        0\n",
       "IHE_Return_5d_t-1        0\n",
       "IHE_Return_10d_t-1       0\n",
       "IHE_Return_20d_t-1       0\n",
       "IHE_SMA_10_t-1           0\n",
       "IHE_EMA_20_t-1           0\n",
       "IHE_MACD_t-1             0\n",
       "IHE_MACD_hist_t-1        0\n",
       "days_since_gdp           0\n",
       "IHE_RSI_14_t-1           0\n",
       "IHE_StochK_t-1           0\n",
       "IHE_StochD_t-1           0\n",
       "IHE_Vol_20_t-1           0\n",
       "IHE_Parkinson_20_t-1     0\n",
       "IHE_GK_20_t-1            0\n",
       "IHE_BB_width_t-1         0\n",
       "IHE_Volume_ROC_t-1       0\n",
       "IHE_Volume_Z_t-1         0\n",
       "IHE_OBV_t-1              0\n",
       "IHE_Entropy_20_t-1       0\n",
       "IBB_Return_1d_t-1        0\n",
       "IBB_Return_5d_t-1        0\n",
       "IBB_Return_10d_t-1       0\n",
       "XPH_Vol_20_t-1           0\n",
       "XPH_StochD_t-1           0\n",
       "XPH_StochK_t-1           0\n",
       "XPH_RSI_14_t-1           0\n",
       "PPH_Return_1d_t-1        0\n",
       "PPH_Return_5d_t-1        0\n",
       "PPH_Return_10d_t-1       0\n",
       "PPH_Return_20d_t-1       0\n",
       "PPH_SMA_10_t-1           0\n",
       "PPH_EMA_20_t-1           0\n",
       "PPH_MACD_t-1             0\n",
       "PPH_MACD_sig_t-1         0\n",
       "PPH_MACD_hist_t-1        0\n",
       "PPH_RSI_14_t-1           0\n",
       "PPH_StochK_t-1           0\n",
       "PPH_StochD_t-1           0\n",
       "PPH_Vol_20_t-1           0\n",
       "PPH_Parkinson_20_t-1     0\n",
       "PPH_GK_20_t-1            0\n",
       "PPH_BB_width_t-1         0\n",
       "PPH_Volume_ROC_t-1       0\n",
       "PPH_Volume_Z_t-1         0\n",
       "PPH_OBV_t-1              0\n",
       "PPH_Entropy_20_t-1       0\n",
       "XPH_Return_1d_t-1        0\n",
       "XPH_Return_5d_t-1        0\n",
       "XPH_Return_10d_t-1       0\n",
       "XPH_Return_20d_t-1       0\n",
       "XPH_SMA_10_t-1           0\n",
       "XPH_EMA_20_t-1           0\n",
       "XPH_MACD_t-1             0\n",
       "XPH_MACD_sig_t-1         0\n",
       "XPH_MACD_hist_t-1        0\n",
       "PCA_3_t-1                0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.width', None):\n",
    "    display(df_final.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69545a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trend-Surgeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
