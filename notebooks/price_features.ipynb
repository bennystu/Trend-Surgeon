{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894f511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  9 of 9 completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "# --- 1. PARAMETERS ---\n",
    "TICKERS = [\n",
    "    \"PPH\",   # Pharma\n",
    "    \"XPH\",   # S&P Pharma\n",
    "    \"IHE\",   # iShares Pharma\n",
    "    \"IBB\",   # Biotech\n",
    "    \"XBI\",   # Biotech equal-weight\n",
    "    \"XLV\",   # Healthcare sector\n",
    "    \"VHT\",   # Vanguard Healthcare\n",
    "    \"SPY\",   # Market benchmark\n",
    "    \"VIXY\",  # VIX ETF proxy (VIX index isn't downloadable directly via yfinance)\n",
    "]\n",
    "\n",
    "START_DATE = \"2011-01-04\"\n",
    "END_DATE   = \"2025-12-01\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. DOWNLOAD MULTI-TICKER DATA\n",
    "# -----------------------------\n",
    "df = yf.download(\n",
    "    tickers=TICKERS,\n",
    "    start=START_DATE,\n",
    "    end=END_DATE,\n",
    "    auto_adjust=False,\n",
    "    group_by='ticker'\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. FLATTEN MULTIINDEX COLUMNS\n",
    "# -----------------------------\n",
    "# Result example: Open_PPH, Close_XBI, Volume_SPY, etc.\n",
    "df.columns = ['_'.join(col).strip() for col in df.columns.to_flat_index()]\n",
    "\n",
    "# -----------------------------\n",
    "# 4. DROP ALL ADJ CLOSE COLUMNS\n",
    "# -----------------------------\n",
    "df = df[[c for c in df.columns if \"Adj Close\" not in c]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb4b284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected tickers: ['IBB', 'IHE', 'PPH', 'SPY', 'VHT', 'VIXY', 'XBI', 'XLV', 'XPH']\n",
      "Base returns created. Registry initialized.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# STEP A — SANITIZE DATAFRAME, INFER TICKERS, BUILD BASE RETURNS\n",
    "# ================================================================\n",
    "\n",
    "# 1. Infer tickers from the flattened column names\n",
    "def infer_tickers_from_df(df):\n",
    "    \"\"\"\n",
    "    Extracts ticker symbols from columns like 'PPH_Close', 'XBI_Open'.\n",
    "    \"\"\"\n",
    "    tickers = sorted({col.split(\"_\")[0] for col in df.columns})\n",
    "    return tickers\n",
    "\n",
    "TICKERS = infer_tickers_from_df(df)\n",
    "print(\"Detected tickers:\", TICKERS)\n",
    "\n",
    "\n",
    "# 2. Create a clean helper that returns OHLCV column names for a ticker\n",
    "def ohlcv_cols(df, ticker):\n",
    "    cols = {\n",
    "        \"open\":   f\"{ticker}_Open\",\n",
    "        \"high\":   f\"{ticker}_High\",\n",
    "        \"low\":    f\"{ticker}_Low\",\n",
    "        \"close\":  f\"{ticker}_Close\",\n",
    "        \"volume\": f\"{ticker}_Volume\",\n",
    "    }\n",
    "    # Sanity check (if any missing, skip ticker later)\n",
    "    if not all(col in df.columns for col in cols.values()):\n",
    "        return None\n",
    "    return cols\n",
    "\n",
    "\n",
    "# 3. Compute **1-day returns for all tickers**\n",
    "#    (Most features depend on these. They are OHLCV-dependent, so shift(1) later.)\n",
    "for t in TICKERS:\n",
    "    cols = ohlcv_cols(df, t)\n",
    "    if cols is None:\n",
    "        continue\n",
    "\n",
    "    df[f\"{t}_Return_1d\"] = df[cols[\"close\"]].pct_change(1)\n",
    "\n",
    "\n",
    "# 4. Initialize the global feature registry\n",
    "#    THIS STAYS EMPTY FOR NOW—later steps will fill it automatically.\n",
    "feature_registry = {}\n",
    "\n",
    "print(\"Base returns created. Registry initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa054cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# STEP B — MINIMAL FEATURE REGISTRY (EXPLICIT SHIFT RULES)\n",
    "# ======================================================================\n",
    "\n",
    "# Registry mapping:\n",
    "#     feature_name → rule\n",
    "#\n",
    "# Where rule ∈ {\n",
    "#     \"shift_1\",         # Feature depends on OHLCV(t)\n",
    "#     \"no_shift\",        # Feature known before t or already lagged\n",
    "#     \"shift_plus_k\"     # Future scheduled events → expand to shift(+1…+horizon)\n",
    "# }\n",
    "#\n",
    "# This registry is filled *incrementally* during feature creation.\n",
    "# After all features are created, the shift engine interprets these rules\n",
    "# and applies the correct temporal alignment safely (no leakage).\n",
    "\n",
    "\n",
    "feature_registry = {}  # global within the pipeline\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# VALID SHIFT RULES\n",
    "# --------------------------\n",
    "VALID_RULES = {\"shift_1\", \"no_shift\", \"shift_plus_k\"}\n",
    "\n",
    "\n",
    "def register_feature(registry: dict, feature_name: str, rule: str):\n",
    "    \"\"\"\n",
    "    Register a feature with an explicit temporal shift rule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    registry : dict\n",
    "        The global registry storing feature_name → rule mappings.\n",
    "\n",
    "    feature_name : str\n",
    "        The new feature’s column name.\n",
    "\n",
    "    rule : str\n",
    "        One of:\n",
    "            - \"shift_1\"       → use t−1 values instead of t\n",
    "            - \"no_shift\"      → feature known before t, no alignment needed\n",
    "            - \"shift_plus_k\"  → shift forward by k ∈ [1…horizon] later\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function stores *metadata only*, not data.\n",
    "    - Actual shifting occurs in the shift engine (Step F).\n",
    "    - No rule inference is used — this prevents mistakes and leakage.\n",
    "    \"\"\"\n",
    "\n",
    "    if rule not in VALID_RULES:\n",
    "        raise ValueError(\n",
    "            f\"Invalid rule '{rule}' for feature '{feature_name}'. \"\n",
    "            f\"Allowed: {VALID_RULES}\"\n",
    "        )\n",
    "\n",
    "    registry[feature_name] = rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7e5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# STEP C — PER-TICKER TECHNICAL FEATURE GENERATORS\n",
    "# ======================================================================\n",
    "# All these features depend on today's OHLCV(t), therefore:\n",
    "#     → They ALWAYS get rule = \"shift_1\"\n",
    "# Registry tagging is included in every function.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Helper: Determine OHLCV column names for a given ticker\n",
    "# ---------------------------------------------------------\n",
    "def ohlcv_cols(df: pd.DataFrame, ticker: str):\n",
    "    \"\"\"\n",
    "    Returns (Open, High, Low, Close, Volume) column names for a ticker.\n",
    "    Returns None if any are missing.\n",
    "    \"\"\"\n",
    "    cols = [f\"{ticker}_{c}\" for c in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "    return cols if all(c in df.columns for c in cols) else None\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MOMENTUM FEATURES\n",
    "# ---------------------------------------------------------\n",
    "def add_momentum_features(df: pd.DataFrame, ticker: str, close: str, registry: dict) -> dict:\n",
    "    feats = {}\n",
    "\n",
    "    # Simple returns\n",
    "    for w in [1, 5, 10, 20]:\n",
    "        name = f\"{ticker}_Return_{w}d\"\n",
    "        feats[name] = df[close].pct_change(w)\n",
    "        register_feature(registry, name, \"shift_1\")\n",
    "\n",
    "    # SMAs / EMAs\n",
    "    sma10 = f\"{ticker}_SMA_10\"\n",
    "    sma50 = f\"{ticker}_SMA_50\"\n",
    "    ema20 = f\"{ticker}_EMA_20\"\n",
    "    ema50 = f\"{ticker}_EMA_50\"\n",
    "    macd_name = f\"{ticker}_MACD\"\n",
    "    macd_sig = f\"{ticker}_MACD_sig\"\n",
    "    macd_hist = f\"{ticker}_MACD_hist\"\n",
    "    rsi14 = f\"{ticker}_RSI_14\"\n",
    "    stochK = f\"{ticker}_StochK\"\n",
    "    stochD = f\"{ticker}_StochD\"\n",
    "\n",
    "    feats[sma10] = df[close].rolling(10).mean()\n",
    "    feats[sma50] = df[close].rolling(50).mean()\n",
    "    feats[ema20] = df[close].ewm(span=20).mean()\n",
    "    feats[ema50] = df[close].ewm(span=50).mean()\n",
    "\n",
    "    register_feature(registry, sma10, \"shift_1\")\n",
    "    register_feature(registry, sma50, \"shift_1\")\n",
    "    register_feature(registry, ema20, \"shift_1\")\n",
    "    register_feature(registry, ema50, \"shift_1\")\n",
    "\n",
    "    # MACD\n",
    "    ema12 = df[close].ewm(span=12).mean()\n",
    "    ema26 = df[close].ewm(span=26).mean()\n",
    "    macd = ema12 - ema26\n",
    "\n",
    "    feats[macd_name] = macd\n",
    "    feats[macd_sig] = macd.ewm(span=9).mean()\n",
    "    feats[macd_hist] = feats[macd_name] - feats[macd_sig]\n",
    "\n",
    "    register_feature(registry, macd_name, \"shift_1\")\n",
    "    register_feature(registry, macd_sig, \"shift_1\")\n",
    "    register_feature(registry, macd_hist, \"shift_1\")\n",
    "\n",
    "    # RSI (14)\n",
    "    delta = df[close].diff()\n",
    "    up = delta.clip(lower=0).rolling(14).mean()\n",
    "    down = (-delta.clip(lower=0)).rolling(14).mean()\n",
    "    rs = up / down\n",
    "    feats[rsi14] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    register_feature(registry, rsi14, \"shift_1\")\n",
    "\n",
    "    # Stochastics\n",
    "    low14 = df[close].rolling(14).min()\n",
    "    high14 = df[close].rolling(14).max()\n",
    "    feats[stochK] = 100 * (df[close] - low14) / (high14 - low14)\n",
    "    feats[stochD] = feats[stochK].rolling(3).mean()\n",
    "\n",
    "    register_feature(registry, stochK, \"shift_1\")\n",
    "    register_feature(registry, stochD, \"shift_1\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# VOLATILITY FEATURES\n",
    "# ---------------------------------------------------------\n",
    "def add_vol_features(df: pd.DataFrame, ticker: str, open_: str, high: str, low: str, close: str, registry: dict) -> dict:\n",
    "    feats = {}\n",
    "\n",
    "    # Return-based vol\n",
    "    name_vol20 = f\"{ticker}_Vol_20\"\n",
    "    feats[name_vol20] = df[f\"{ticker}_Return_1d\"].rolling(20).std()\n",
    "    register_feature(registry, name_vol20, \"shift_1\")\n",
    "\n",
    "    # Parkinson volatility\n",
    "    name_parkinson = f\"{ticker}_Parkinson_20\"\n",
    "    feats[name_parkinson] = (\n",
    "        (np.log(df[high] / df[low]) ** 2).rolling(20).mean()\n",
    "        * (1 / (4 * np.log(2)))\n",
    "    )\n",
    "    register_feature(registry, name_parkinson, \"shift_1\")\n",
    "\n",
    "    # Garman-Klass volatility\n",
    "    name_gk = f\"{ticker}_GK_20\"\n",
    "    feats[name_gk] = (\n",
    "        0.5 * (np.log(df[high] / df[low]) ** 2)\n",
    "        - (2 * np.log(np.e) - 1) * (np.log(df[close] / df[open_]) ** 2)\n",
    "    ).rolling(20).mean()\n",
    "    register_feature(registry, name_gk, \"shift_1\")\n",
    "\n",
    "    # Bollinger width\n",
    "    name_bb = f\"{ticker}_BB_width\"\n",
    "    sma20 = df[close].rolling(20).mean()\n",
    "    std20 = df[close].rolling(20).std()\n",
    "    feats[name_bb] = (2 * std20) / sma20\n",
    "    register_feature(registry, name_bb, \"shift_1\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# VOLUME FEATURES\n",
    "# ---------------------------------------------------------\n",
    "def add_volume_features(df: pd.DataFrame, ticker: str, volume: str, close: str, registry: dict) -> dict:\n",
    "    feats = {}\n",
    "\n",
    "    roc = f\"{ticker}_Volume_ROC\"\n",
    "    zname = f\"{ticker}_Volume_Z\"\n",
    "    obv = f\"{ticker}_OBV\"\n",
    "\n",
    "    feats[roc] = df[volume].pct_change(5)\n",
    "    register_feature(registry, roc, \"shift_1\")\n",
    "\n",
    "    feats[zname] = (df[volume] - df[volume].rolling(20).mean()) / df[volume].rolling(20).std()\n",
    "    register_feature(registry, zname, \"shift_1\")\n",
    "\n",
    "    feats[obv] = (np.sign(df[close].diff()) * df[volume]).fillna(0).cumsum()\n",
    "    register_feature(registry, obv, \"shift_1\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ENTROPY FEATURE\n",
    "# ---------------------------------------------------------\n",
    "def add_entropy(df: pd.DataFrame, ticker: str, registry: dict) -> dict:\n",
    "    feats = {}\n",
    "    ret = f\"{ticker}_Return_1d\"\n",
    "\n",
    "    if ret in df.columns:\n",
    "        name = f\"{ticker}_Entropy_20\"\n",
    "        feats[name] = (df[ret] ** 2).rolling(20).sum()\n",
    "        register_feature(registry, name, \"shift_1\")\n",
    "\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bca183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# STEP D — CROSS-TICKER FEATURES RELATIVE TO THE TARGET TICKER\n",
    "# ======================================================================\n",
    "# These features compare each ticker to the target ticker's Close.\n",
    "# Because they depend on *today’s* prices, all of them get shift_1.\n",
    "\n",
    "\n",
    "def add_cross_features(df: pd.DataFrame, target: str, ticker: str, registry: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Create cross-ticker features between TARGET and TICKER.\n",
    "\n",
    "    target : the ticker you are forecasting (e.g. \"PPH\")\n",
    "    ticker : the other ticker to compare against\n",
    "\n",
    "    Returns dict of new features.\n",
    "    All features depend on OHLCV(t) → rule = shift_1\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "\n",
    "    target_col = f\"{target}_Close\"\n",
    "    ticker_col = f\"{ticker}_Close\"\n",
    "\n",
    "    # if either side is missing → no features\n",
    "    if target_col not in df.columns or ticker_col not in df.columns:\n",
    "        return feats\n",
    "\n",
    "    # -------------------------\n",
    "    # 1. Price Ratio\n",
    "    # -------------------------\n",
    "    ratio = f\"{ticker}_Ratio_{target}\"\n",
    "    feats[ratio] = df[ticker_col] / df[target_col]\n",
    "    register_feature(registry, ratio, \"shift_1\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 2. Relative Strength (20d)\n",
    "    # RS20 = return_ticker(20d) - return_target(20d)\n",
    "    # -------------------------\n",
    "    rs20 = f\"{ticker}_RS_20\"\n",
    "    feats[rs20] = df[ticker_col].pct_change(20) - df[target_col].pct_change(20)\n",
    "    register_feature(registry, rs20, \"shift_1\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 3. Rolling Correlation (60d)\n",
    "    # -------------------------\n",
    "    corr = f\"{ticker}_Corr_{target}_60\"\n",
    "    feats[corr] = df[ticker_col].rolling(60).corr(df[target_col])\n",
    "    register_feature(registry, corr, \"shift_1\")\n",
    "\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e931e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# STEP E — PCA LATENT FACTORS (3 COMPONENTS)\n",
    "# ======================================================================\n",
    "# PCA is fit on returns(t), so PCA_1/2/3 must use shift_1.\n",
    "# These features capture common sector/market structure.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def add_pca_features(df: pd.DataFrame, tickers: list, registry: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Compute PCA latent factors from 1-day returns of all tickers.\n",
    "\n",
    "    Returns a dict of:\n",
    "        PCA_1, PCA_2, PCA_3\n",
    "\n",
    "    All receive rule = \"shift_1\" because they use OHLCV(t).\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "\n",
    "    # Collect available return columns\n",
    "    ret_cols = [\n",
    "        f\"{t}_Return_1d\"\n",
    "        for t in tickers\n",
    "        if f\"{t}_Return_1d\" in df.columns\n",
    "    ]\n",
    "\n",
    "    # Need enough data to fit PCA robustly\n",
    "    data = df[ret_cols].dropna()\n",
    "    if len(data) < 200:     # minimum stability requirement\n",
    "        return feats\n",
    "\n",
    "    try:\n",
    "        pca = PCA(n_components=3)\n",
    "        comps = pca.fit_transform(data)\n",
    "\n",
    "        # Create full-length series aligned to df index\n",
    "        feats[\"PCA_1\"] = pd.Series(comps[:, 0], index=data.index)\n",
    "        feats[\"PCA_2\"] = pd.Series(comps[:, 1], index=data.index)\n",
    "        feats[\"PCA_3\"] = pd.Series(comps[:, 2], index=data.index)\n",
    "\n",
    "        # Register shift rules\n",
    "        register_feature(registry, \"PCA_1\", \"shift_1\")\n",
    "        register_feature(registry, \"PCA_2\", \"shift_1\")\n",
    "        register_feature(registry, \"PCA_3\", \"shift_1\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"PCA failed: {e}\")\n",
    "\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9680f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# STEP F — CALENDAR & MACRO EVENT FEATURES (registry-aware)\n",
    "# ======================================================================\n",
    "# These features do NOT depend on OHLCV(t).\n",
    "# Safe rules:\n",
    "#   - \"no_shift\"        → calendar features known before market open\n",
    "#   - \"shift_plus_k\"    → future macro events (CPI, NFP, FOMC, PPI, GDP)\n",
    "#\n",
    "# These rules allow the shift engine to later build:\n",
    "#   is_cpi_day_t+1, is_cpi_day_t+2, ..., is_cpi_day_t+horizon\n",
    "# ======================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import holidays\n",
    "from pandas_market_calendars import get_calendar\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# FOMC Dates\n",
    "# -----------------------------------------------------------\n",
    "def fetch_fomc_dates():\n",
    "    \"\"\"\n",
    "    Fetch FOMC meeting dates from Federal Reserve website.\n",
    "    Returns a Pandas Series for .dt.isocalendar().\n",
    "    \"\"\"\n",
    "    url = \"https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\"\n",
    "    try:\n",
    "        tables = pd.read_html(url)\n",
    "        dates = pd.to_datetime(tables[0].iloc[:, 0], errors=\"coerce\").dropna()\n",
    "        return pd.Series(dates)\n",
    "    except:\n",
    "        fallback = pd.to_datetime([\n",
    "            \"2024-01-31\",\"2024-03-20\",\"2024-05-01\",\n",
    "            \"2024-06-12\",\"2024-07-31\",\"2024-09-18\",\n",
    "            \"2024-11-07\",\"2024-12-18\"\n",
    "        ])\n",
    "        return pd.Series(fallback)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Macro release dates (user-supplied)\n",
    "# -----------------------------------------------------------\n",
    "def macro_calendar():\n",
    "    \"\"\"Returns dict of event_name → Series of event dates.\"\"\"\n",
    "    events = {\n",
    "        \"cpi\": [\"2024-01-11\",\"2024-02-13\",\"2024-03-12\",\"2024-04-10\"],\n",
    "        \"nfp\": [\"2024-01-05\",\"2024-02-02\",\"2024-03-08\",\"2024-04-05\"],\n",
    "        \"ppi\": [\"2024-01-12\",\"2024-02-16\",\"2024-03-14\",\"2024-04-11\"],\n",
    "        \"gdp\": [\"2024-01-25\",\"2024-02-28\",\"2024-03-28\",\"2024-04-25\"],\n",
    "    }\n",
    "    return {k: pd.Series(pd.to_datetime(v)) for k, v in events.items()}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Calendar basics\n",
    "# -----------------------------------------------------------\n",
    "def add_calendar_basics(df, registry):\n",
    "    df[\"day_of_week\"]    = df[\"date\"].dt.dayofweek\n",
    "    df[\"day_of_month\"]   = df[\"date\"].dt.day\n",
    "    df[\"month\"]          = df[\"date\"].dt.month\n",
    "    df[\"quarter\"]        = df[\"date\"].dt.quarter\n",
    "    df[\"is_month_end\"]   = df[\"date\"].dt.is_month_end.astype(int)\n",
    "    df[\"is_month_start\"] = df[\"date\"].dt.is_month_start.astype(int)\n",
    "    df[\"is_year_end\"]    = ((df[\"month\"] == 12) & (df[\"day_of_month\"] == 31)).astype(int)\n",
    "\n",
    "    for col in [\n",
    "        \"day_of_week\", \"day_of_month\", \"month\", \"quarter\",\n",
    "        \"is_month_end\", \"is_month_start\", \"is_year_end\"\n",
    "    ]:\n",
    "        register_feature(registry, col, \"no_shift\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# US holiday adjacency\n",
    "# -----------------------------------------------------------\n",
    "def add_holiday_features(df, registry):\n",
    "    us_holidays = holidays.US()\n",
    "    df[\"is_holiday_adjacent\"] = df[\"date\"].apply(\n",
    "        lambda d: int((d + pd.Timedelta(days=1) in us_holidays) or\n",
    "                      (d - pd.Timedelta(days=1) in us_holidays))\n",
    "    )\n",
    "\n",
    "    register_feature(registry, \"is_holiday_adjacent\", \"no_shift\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# OPEX week\n",
    "# -----------------------------------------------------------\n",
    "def add_opex_features(df, registry):\n",
    "    df[\"is_opex_week\"] = df[\"date\"].apply(\n",
    "        lambda d: int((d.weekday() == 4) and (15 <= d.day <= 21))\n",
    "    )\n",
    "\n",
    "    register_feature(registry, \"is_opex_week\", \"no_shift\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# FOMC week (volatility clusters)\n",
    "# -----------------------------------------------------------\n",
    "def add_fomc_features(df, registry):\n",
    "    fomc_dates = fetch_fomc_dates()\n",
    "    fomc_weeks = fomc_dates.dt.isocalendar().week.unique()\n",
    "\n",
    "    df[\"is_fomc_week\"] = df[\"date\"].dt.isocalendar().week.isin(fomc_weeks).astype(int)\n",
    "    register_feature(registry, \"is_fomc_week\", \"no_shift\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# High-impact macroeconomic event flags\n",
    "# -----------------------------------------------------------\n",
    "def add_macro_features(df, registry):\n",
    "    macros = macro_calendar()\n",
    "\n",
    "    for name, dates in macros.items():\n",
    "\n",
    "        # Day-level event\n",
    "        day_col = f\"is_{name}_day\"\n",
    "        df[day_col] = df[\"date\"].isin(dates).astype(int)\n",
    "        register_feature(registry, day_col, \"shift_plus_k\")\n",
    "\n",
    "        # Event-week indicator – future relevance ambiguous\n",
    "        week_col = f\"is_{name}_week\"\n",
    "        df[week_col] = df[\"date\"].dt.isocalendar().week.isin(\n",
    "            dates.dt.isocalendar().week.unique()\n",
    "        ).astype(int)\n",
    "        register_feature(registry, week_col, \"shift_plus_k\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Master wrapper — assumes df.index is datetime index\n",
    "# -----------------------------------------------------------\n",
    "def add_all_calendar_features(df, registry):\n",
    "    \"\"\"\n",
    "    Adds calendar, holiday, OPEX, FOMC, and macro-event features.\n",
    "    Applies only registry tagging; no shifts are applied here.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert index → date column\n",
    "    df = df.reset_index().rename(columns={df.index.name or \"index\": \"date\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    df = add_calendar_basics(df, registry)\n",
    "    df = add_holiday_features(df, registry)\n",
    "    df = add_opex_features(df, registry)\n",
    "    df = add_fomc_features(df, registry)\n",
    "    df = add_macro_features(df, registry)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45412837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# STEP G — SHIFT ENGINE (APPLIES ALL SHIFT RULES)\n",
    "# ======================================================================\n",
    "# Interprets the registry and applies:\n",
    "#   - shift_1        → df[col] = df[col].shift(1)\n",
    "#   - no_shift       → leave unchanged\n",
    "#   - shift_plus_k   → create k=1…horizon shifted columns\n",
    "#\n",
    "# Ensures:\n",
    "#   - ZERO data leakage\n",
    "#   - multi-step recursive forecasting support\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "def apply_shift_rules(df: pd.DataFrame, registry: dict, horizon: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies temporal alignment to all features according to the registry.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe containing all raw features.\n",
    "    registry : dict\n",
    "        feature_name → rule\n",
    "        where rule ∈ {\"shift_1\", \"no_shift\", \"shift_plus_k\"}\n",
    "    horizon : int\n",
    "        Forecast horizon (e.g., 30, 60, etc.)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A new dataframe with aligned features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    new_cols = {}\n",
    "\n",
    "    for col, rule in registry.items():\n",
    "\n",
    "        # ---------------------------\n",
    "        # RULE 1 — shift_1\n",
    "        # ---------------------------\n",
    "        if rule == \"shift_1\":\n",
    "            new_cols[col] = df[col].shift(1)\n",
    "\n",
    "        # ---------------------------\n",
    "        # RULE 2 — no_shift\n",
    "        # ---------------------------\n",
    "        elif rule == \"no_shift\":\n",
    "            new_cols[col] = df[col]   # unchanged\n",
    "\n",
    "        # ---------------------------\n",
    "        # RULE 3 — shift_plus_k\n",
    "        # ---------------------------\n",
    "        elif rule == \"shift_plus_k\":\n",
    "            # Create ladder of k = 1…horizon\n",
    "            for k in range(1, horizon + 1):\n",
    "                new_name = f\"{col}_t+{k}\"\n",
    "                new_cols[new_name] = df[col].shift(k)\n",
    "\n",
    "            # We do NOT keep the original (unshifted) event flag\n",
    "            # because it's meaningless in recursive forecasting.\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown shift rule '{rule}' for column '{col}'\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Merge all new features\n",
    "    # ---------------------------\n",
    "    out = pd.concat([df, pd.DataFrame(new_cols, index=df.index)], axis=1)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f6a399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# STEP H — GENERATE MARKDOWN DOCUMENTATION\n",
    "# ======================================================================\n",
    "# Creates a markdown file listing:\n",
    "#   feature_name\n",
    "#   transformation_rule\n",
    "#   resulting feature names (including expanded future-event ladders)\n",
    "#\n",
    "# This does NOT affect the data pipeline — it is documentation only.\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "def generate_markdown_from_registry(registry: dict, horizon: int, path: str):\n",
    "    \"\"\"\n",
    "    Generate a markdown documentation table from the feature registry.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    registry : dict\n",
    "        feature_name → rule\n",
    "    horizon : int\n",
    "        Forecast horizon (determines how many shift_plus_k features to list)\n",
    "    path : str\n",
    "        Output markdown file path\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str : markdown text (also saved to 'path')\n",
    "    \"\"\"\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"# Feature Transformation Table\\n\")\n",
    "    lines.append(\"This table documents how each feature is temporally aligned.\\n\")\n",
    "    lines.append(\"\\n\")\n",
    "    lines.append(\"| Feature Name | Rule | Resulting Columns |\\n\")\n",
    "    lines.append(\"|--------------|------|------------------|\\n\")\n",
    "\n",
    "    for col, rule in registry.items():\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 1. shift_1  → one resulting column\n",
    "        # ---------------------------------------\n",
    "        if rule == \"shift_1\":\n",
    "            resulting = f\"{col} (t-1)\"\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 2. no_shift → one resulting column\n",
    "        # ---------------------------------------\n",
    "        elif rule == \"no_shift\":\n",
    "            resulting = col\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 3. shift_plus_k → many resulting columns\n",
    "        # ---------------------------------------\n",
    "        elif rule == \"shift_plus_k\":\n",
    "            resulting = \", \".join(\n",
    "                [f\"{col}_t+{k}\" for k in range(1, horizon + 1)]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            resulting = \"UNKNOWN RULE\"\n",
    "\n",
    "        # add markdown row\n",
    "        lines.append(f\"| {col} | {rule} | {resulting} |\\n\")\n",
    "\n",
    "    # Write markdown file\n",
    "    md = \"\".join(lines)\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(md)\n",
    "\n",
    "    return md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08069c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1edc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb707e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf8707a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1a205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91839f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VIXY_Close': Timestamp('2011-01-04 00:00:00'), 'XPH_Close': Timestamp('2011-01-04 00:00:00'), 'VHT_Close': Timestamp('2011-01-04 00:00:00'), 'IHE_Close': Timestamp('2011-01-04 00:00:00'), 'XBI_Close': Timestamp('2011-01-04 00:00:00'), 'XLV_Close': Timestamp('2011-01-04 00:00:00'), 'SPY_Close': Timestamp('2011-01-04 00:00:00'), 'PPH_Close': Timestamp('2011-01-04 00:00:00'), 'IBB_Close': Timestamp('2011-01-04 00:00:00')}\n"
     ]
    }
   ],
   "source": [
    "# Find start dates of each ETF\n",
    "start_dates = {col: df[col].first_valid_index() for col in df.columns if col.endswith(\"_Close\")}\n",
    "print(start_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0c9306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find start date to use when all ETFS have data\n",
    "global_start = max(start_dates.values())\n",
    "df = df.loc[df.index >= global_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcdd929e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIXY_Open</th>\n",
       "      <th>VIXY_High</th>\n",
       "      <th>VIXY_Low</th>\n",
       "      <th>VIXY_Close</th>\n",
       "      <th>VIXY_Volume</th>\n",
       "      <th>XPH_Open</th>\n",
       "      <th>XPH_High</th>\n",
       "      <th>XPH_Low</th>\n",
       "      <th>XPH_Close</th>\n",
       "      <th>XPH_Volume</th>\n",
       "      <th>...</th>\n",
       "      <th>PPH_Open</th>\n",
       "      <th>PPH_High</th>\n",
       "      <th>PPH_Low</th>\n",
       "      <th>PPH_Close</th>\n",
       "      <th>PPH_Volume</th>\n",
       "      <th>IBB_Open</th>\n",
       "      <th>IBB_High</th>\n",
       "      <th>IBB_Low</th>\n",
       "      <th>IBB_Close</th>\n",
       "      <th>IBB_Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>633120.0</td>\n",
       "      <td>650800.0</td>\n",
       "      <td>633120.0</td>\n",
       "      <td>633840.0</td>\n",
       "      <td>14</td>\n",
       "      <td>23.285000</td>\n",
       "      <td>23.285000</td>\n",
       "      <td>22.965000</td>\n",
       "      <td>23.059999</td>\n",
       "      <td>42000</td>\n",
       "      <td>...</td>\n",
       "      <td>32.794998</td>\n",
       "      <td>32.965000</td>\n",
       "      <td>32.695000</td>\n",
       "      <td>32.965000</td>\n",
       "      <td>529200</td>\n",
       "      <td>31.620001</td>\n",
       "      <td>31.620001</td>\n",
       "      <td>31.139999</td>\n",
       "      <td>31.299999</td>\n",
       "      <td>1088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>640400.0</td>\n",
       "      <td>640800.0</td>\n",
       "      <td>617440.0</td>\n",
       "      <td>620400.0</td>\n",
       "      <td>9</td>\n",
       "      <td>22.945000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>22.945000</td>\n",
       "      <td>23.219999</td>\n",
       "      <td>54400</td>\n",
       "      <td>...</td>\n",
       "      <td>32.840000</td>\n",
       "      <td>33.060001</td>\n",
       "      <td>32.810001</td>\n",
       "      <td>33.005001</td>\n",
       "      <td>1349600</td>\n",
       "      <td>31.299999</td>\n",
       "      <td>31.623333</td>\n",
       "      <td>31.240000</td>\n",
       "      <td>31.549999</td>\n",
       "      <td>1756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-06</th>\n",
       "      <td>619520.0</td>\n",
       "      <td>629440.0</td>\n",
       "      <td>614960.0</td>\n",
       "      <td>623040.0</td>\n",
       "      <td>10</td>\n",
       "      <td>23.360001</td>\n",
       "      <td>23.395000</td>\n",
       "      <td>23.209999</td>\n",
       "      <td>23.350000</td>\n",
       "      <td>34600</td>\n",
       "      <td>...</td>\n",
       "      <td>33.014999</td>\n",
       "      <td>33.264999</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.259998</td>\n",
       "      <td>363000</td>\n",
       "      <td>31.450001</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>31.450001</td>\n",
       "      <td>31.696667</td>\n",
       "      <td>714300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-07</th>\n",
       "      <td>617920.0</td>\n",
       "      <td>640080.0</td>\n",
       "      <td>609440.0</td>\n",
       "      <td>624320.0</td>\n",
       "      <td>5</td>\n",
       "      <td>23.290001</td>\n",
       "      <td>23.400000</td>\n",
       "      <td>23.115000</td>\n",
       "      <td>23.375000</td>\n",
       "      <td>96200</td>\n",
       "      <td>...</td>\n",
       "      <td>33.220001</td>\n",
       "      <td>33.320000</td>\n",
       "      <td>33.084999</td>\n",
       "      <td>33.275002</td>\n",
       "      <td>391000</td>\n",
       "      <td>31.673332</td>\n",
       "      <td>31.796667</td>\n",
       "      <td>31.466667</td>\n",
       "      <td>31.683332</td>\n",
       "      <td>1017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-10</th>\n",
       "      <td>637120.0</td>\n",
       "      <td>646960.0</td>\n",
       "      <td>622080.0</td>\n",
       "      <td>623040.0</td>\n",
       "      <td>9</td>\n",
       "      <td>23.370001</td>\n",
       "      <td>23.504999</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>23.480000</td>\n",
       "      <td>59600</td>\n",
       "      <td>...</td>\n",
       "      <td>33.154999</td>\n",
       "      <td>33.220001</td>\n",
       "      <td>33.055000</td>\n",
       "      <td>33.115002</td>\n",
       "      <td>302800</td>\n",
       "      <td>31.660000</td>\n",
       "      <td>31.693333</td>\n",
       "      <td>31.440001</td>\n",
       "      <td>31.646667</td>\n",
       "      <td>938400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            VIXY_Open  VIXY_High  VIXY_Low  VIXY_Close  VIXY_Volume  \\\n",
       "Date                                                                  \n",
       "2011-01-04   633120.0   650800.0  633120.0    633840.0           14   \n",
       "2011-01-05   640400.0   640800.0  617440.0    620400.0            9   \n",
       "2011-01-06   619520.0   629440.0  614960.0    623040.0           10   \n",
       "2011-01-07   617920.0   640080.0  609440.0    624320.0            5   \n",
       "2011-01-10   637120.0   646960.0  622080.0    623040.0            9   \n",
       "\n",
       "             XPH_Open   XPH_High    XPH_Low  XPH_Close  XPH_Volume  ...  \\\n",
       "Date                                                                ...   \n",
       "2011-01-04  23.285000  23.285000  22.965000  23.059999       42000  ...   \n",
       "2011-01-05  22.945000  23.250000  22.945000  23.219999       54400  ...   \n",
       "2011-01-06  23.360001  23.395000  23.209999  23.350000       34600  ...   \n",
       "2011-01-07  23.290001  23.400000  23.115000  23.375000       96200  ...   \n",
       "2011-01-10  23.370001  23.504999  23.250000  23.480000       59600  ...   \n",
       "\n",
       "             PPH_Open   PPH_High    PPH_Low  PPH_Close  PPH_Volume   IBB_Open  \\\n",
       "Date                                                                            \n",
       "2011-01-04  32.794998  32.965000  32.695000  32.965000      529200  31.620001   \n",
       "2011-01-05  32.840000  33.060001  32.810001  33.005001     1349600  31.299999   \n",
       "2011-01-06  33.014999  33.264999  33.000000  33.259998      363000  31.450001   \n",
       "2011-01-07  33.220001  33.320000  33.084999  33.275002      391000  31.673332   \n",
       "2011-01-10  33.154999  33.220001  33.055000  33.115002      302800  31.660000   \n",
       "\n",
       "             IBB_High    IBB_Low  IBB_Close  IBB_Volume  \n",
       "Date                                                     \n",
       "2011-01-04  31.620001  31.139999  31.299999     1088100  \n",
       "2011-01-05  31.623333  31.240000  31.549999     1756800  \n",
       "2011-01-06  31.750000  31.450001  31.696667      714300  \n",
       "2011-01-07  31.796667  31.466667  31.683332     1017300  \n",
       "2011-01-10  31.693333  31.440001  31.646667      938400  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbdca451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07',\n",
       "               '2011-01-10', '2011-01-11', '2011-01-12', '2011-01-13',\n",
       "               '2011-01-14', '2011-01-18',\n",
       "               ...\n",
       "               '2025-11-14', '2025-11-17', '2025-11-18', '2025-11-19',\n",
       "               '2025-11-20', '2025-11-21', '2025-11-24', '2025-11-25',\n",
       "               '2025-11-26', '2025-11-28'],\n",
       "              dtype='datetime64[ns]', name='Date', length=3749, freq=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0212cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Logging\n",
    "# ----------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#  UTILITY HELPERS\n",
    "# ==========================================================\n",
    "\n",
    "def infer_tickers(df: pd.DataFrame):\n",
    "    \"\"\"Infer tickers from *_Close columns.\"\"\"\n",
    "    return sorted({c.split(\"_\")[0] for c in df.columns if c.endswith(\"_Close\")})\n",
    "\n",
    "\n",
    "def ohlcv_cols(df: pd.DataFrame, ticker: str):\n",
    "    \"\"\"Return Open/High/Low/Close/Volume column names if present.\"\"\"\n",
    "    cols = [f\"{ticker}_{c}\" for c in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "    return cols if all(c in df.columns for c in cols) else None\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Rolling R² for trend quality\n",
    "# ----------------------------------------------------------\n",
    "def rolling_r2(series: pd.Series, window: int) -> pd.Series:\n",
    "    r2 = np.full(len(series), np.nan)\n",
    "    t = np.arange(window)\n",
    "\n",
    "    for i in range(window, len(series)):\n",
    "        y = series.iloc[i-window:i]\n",
    "        if y.isna().any():\n",
    "            continue\n",
    "\n",
    "        coef = np.polyfit(t, y, 1)\n",
    "        y_pred = coef[0] * t + coef[1]\n",
    "\n",
    "        ss_res = ((y - y_pred) ** 2).sum()\n",
    "        ss_tot = ((y - y.mean()) ** 2).sum()\n",
    "\n",
    "        if ss_tot != 0:\n",
    "            r2[i] = 1 - ss_res / ss_tot\n",
    "\n",
    "    return pd.Series(r2, index=series.index)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Rolling Beta\n",
    "# ----------------------------------------------------------\n",
    "def rolling_beta(ret_a: pd.Series, ret_b: pd.Series, window: int):\n",
    "    cov = ret_a.rolling(window).cov(ret_b)\n",
    "    var = ret_b.rolling(window).var()\n",
    "    return cov / var\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ADX wrapper\n",
    "# ----------------------------------------------------------\n",
    "def ADX(high, low, close, window=14):\n",
    "    import ta\n",
    "    return ta.trend.adx(high, low, close, window)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# PER-TICKER BASE TECHNICAL FEATURES\n",
    "# ==========================================================\n",
    "\n",
    "def add_momentum_features(df, ticker, close):\n",
    "    feats = {}\n",
    "\n",
    "    # Returns\n",
    "    for w in [1, 5, 10, 20]:\n",
    "        feats[f\"{ticker}_Return_{w}d\"] = df[close].pct_change(w)\n",
    "\n",
    "    # SMAs / EMAs\n",
    "    feats[f\"{ticker}_SMA_10\"] = df[close].rolling(10).mean()\n",
    "    feats[f\"{ticker}_SMA_50\"] = df[close].rolling(50).mean()\n",
    "    feats[f\"{ticker}_EMA_20\"] = df[close].ewm(span=20).mean()\n",
    "    feats[f\"{ticker}_EMA_50\"] = df[close].ewm(span=50).mean()\n",
    "    feats[f\"{ticker}_MA_Cross\"] = feats[f\"{ticker}_EMA_20\"] / feats[f\"{ticker}_EMA_50\"]\n",
    "\n",
    "    # MACD\n",
    "    ema12 = df[close].ewm(span=12).mean()\n",
    "    ema26 = df[close].ewm(span=26).mean()\n",
    "    macd = ema12 - ema26\n",
    "    feats[f\"{ticker}_MACD\"] = macd\n",
    "    feats[f\"{ticker}_MACD_sig\"] = macd.ewm(span=9).mean()\n",
    "    feats[f\"{ticker}_MACD_hist\"] = feats[f\"{ticker}_MACD\"] - feats[f\"{ticker}_MACD_sig\"]\n",
    "\n",
    "    # RSI\n",
    "    delta = df[close].diff()\n",
    "    up = delta.clip(lower=0).rolling(14).mean()\n",
    "    down = (-delta.clip(lower=0)).rolling(14).mean()\n",
    "    rs = up / down\n",
    "    feats[f\"{ticker}_RSI_14\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Stochastics\n",
    "    low14 = df[close].rolling(14).min()\n",
    "    high14 = df[close].rolling(14).max()\n",
    "    feats[f\"{ticker}_StochK\"] = 100 * (df[close] - low14) / (high14 - low14)\n",
    "    feats[f\"{ticker}_StochD\"] = feats[f\"{ticker}_StochK\"].rolling(3).mean()\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def add_vol_features(df, ticker, open_, high, low, close):\n",
    "    feats = {}\n",
    "\n",
    "    feats[f\"{ticker}_Vol_20\"] = df[f\"{ticker}_Return_1d\"].rolling(20).std()\n",
    "\n",
    "    feats[f\"{ticker}_Parkinson_20\"] = (\n",
    "        (np.log(df[high] / df[low]) ** 2).rolling(20).mean()\n",
    "        * (1 / (4 * np.log(2)))\n",
    "    )\n",
    "\n",
    "    feats[f\"{ticker}_GK_20\"] = (\n",
    "        0.5 * (np.log(df[high] / df[low]) ** 2)\n",
    "        - (2 * np.log(np.e) - 1) * (np.log(df[close] / df[open_]) ** 2)\n",
    "    ).rolling(20).mean()\n",
    "\n",
    "    sma20 = df[close].rolling(20).mean()\n",
    "    std20 = df[close].rolling(20).std()\n",
    "    feats[f\"{ticker}_BB_width\"] = (2 * std20) / sma20\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def add_volume_features(df, ticker, volume, close):\n",
    "    feats = {}\n",
    "\n",
    "    feats[f\"{ticker}_Volume_ROC\"] = df[volume].pct_change(5)\n",
    "    feats[f\"{ticker}_Volume_Z\"]   = (df[volume] - df[volume].rolling(20).mean()) / df[volume].rolling(20).std()\n",
    "    feats[f\"{ticker}_OBV\"]        = (np.sign(df[close].diff()) * df[volume]).fillna(0).cumsum()\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def add_entropy(df, ticker):\n",
    "    feats = {}\n",
    "    ret = f\"{ticker}_Return_1d\"\n",
    "    if ret in df.columns:\n",
    "        feats[f\"{ticker}_Entropy_20\"] = (df[ret] ** 2).rolling(20).sum()\n",
    "    return feats\n",
    "\n",
    "\n",
    "def add_hmm_states(df, ticker):\n",
    "    feats = {}\n",
    "    ret = f\"{ticker}_Return_1d\"\n",
    "    if ret not in df.columns:\n",
    "        return feats\n",
    "\n",
    "    series = df[ret].dropna()\n",
    "    if len(series) < 200:\n",
    "        return feats\n",
    "\n",
    "    hmm = GaussianHMM(\n",
    "        n_components=3,\n",
    "        covariance_type=\"full\",\n",
    "        n_iter=800,\n",
    "        tol=1e-3,\n",
    "        init_params=\"mct\",\n",
    "        params=\"mct\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        hmm.fit(series.values.reshape(-1, 1))\n",
    "        feats[f\"{ticker}_HMM\"] = pd.Series(hmm.predict(series.values.reshape(1, -1)[0]), index=series.index)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"HMM failed for {ticker}: {e}\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# ADVANCED PER-TICKER FEATURES (beta, skew, vov, ADX, trend R²)\n",
    "# ==========================================================\n",
    "\n",
    "def add_advanced_ticker_features(df, ticker, tickers):\n",
    "    feats = {}\n",
    "\n",
    "    close = df[f\"{ticker}_Close\"]\n",
    "    high  = df[f\"{ticker}_High\"]\n",
    "    low   = df[f\"{ticker}_Low\"]\n",
    "    vol   = df[f\"{ticker}_Volume\"]\n",
    "    ret1  = df[f\"{ticker}_Return_1d\"]\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 1. LAGGED CLOSE\n",
    "    # ---------------------------------------\n",
    "    feats[f\"{ticker}_Close_lag1\"] = close.shift(1)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 2. Betas + residuals vs all other tickers\n",
    "    # ---------------------------------------\n",
    "    for bench in tickers:\n",
    "        if bench == ticker:\n",
    "            continue\n",
    "        if f\"{bench}_Return_1d\" not in df.columns:\n",
    "            continue\n",
    "\n",
    "        bench_ret = df[f\"{bench}_Return_1d\"]\n",
    "\n",
    "        for w in [20, 60]:\n",
    "            beta = rolling_beta(ret1, bench_ret, w)\n",
    "            feats[f\"{ticker}_beta_{bench}_{w}\"] = beta\n",
    "            feats[f\"{ticker}_residual_{bench}_{w}\"] = ret1 - beta * bench_ret\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 3. Skew & Kurtosis\n",
    "    # ---------------------------------------\n",
    "    for w in [20, 60]:\n",
    "        feats[f\"{ticker}_skew_{w}\"] = ret1.rolling(w).apply(\n",
    "            lambda x: skew(x, bias=False) if not np.isnan(x).any() else np.nan,\n",
    "            raw=False\n",
    "        )\n",
    "        feats[f\"{ticker}_kurt_{w}\"] = ret1.rolling(w).apply(\n",
    "            lambda x: kurtosis(x, bias=False) if not np.isnan(x).any() else np.nan,\n",
    "            raw=False\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 4. Vol-of-Vol\n",
    "    # ---------------------------------------\n",
    "    vol20 = ret1.rolling(20).std()\n",
    "    for w in [20, 60]:\n",
    "        feats[f\"{ticker}_vov_{w}\"] = vol20.diff().rolling(w).std()\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 5. Trend R²\n",
    "    # ---------------------------------------\n",
    "    feats[f\"{ticker}_trend_r2_20\"] = rolling_r2(close, 20)\n",
    "    feats[f\"{ticker}_trend_r2_60\"] = rolling_r2(close, 60)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 6. ADX\n",
    "    # ---------------------------------------\n",
    "    feats[f\"{ticker}_adx_14\"] = ADX(high, low, close, 14)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 7. Volume Microstructure\n",
    "    # ---------------------------------------\n",
    "    feats[f\"{ticker}_volume_percentile_252\"] = (\n",
    "        vol.rolling(252).apply(\n",
    "            lambda x: (pd.Series(x).rank().iloc[-1]) / 252 if len(x) == 252 else np.nan,\n",
    "            raw=False\n",
    "        )\n",
    "    )\n",
    "    feats[f\"{ticker}_volume_trend_20\"] = rolling_r2(vol, 20)\n",
    "    feats[f\"{ticker}_volume_dryup_flag\"] = vol < vol.rolling(60).mean() * 0.5\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# CROSS-TICKER FEATURES\n",
    "# ==========================================================\n",
    "\n",
    "def add_cross_features(df, tickers):\n",
    "    feats = {}\n",
    "    for a in tickers:\n",
    "        for b in tickers:\n",
    "            if a == b:\n",
    "                continue\n",
    "            A = f\"{a}_Close\"\n",
    "            B = f\"{b}_Close\"\n",
    "\n",
    "            if A not in df.columns or B not in df.columns:\n",
    "                continue\n",
    "\n",
    "            feats[f\"{a}_Ratio_{b}\"] = df[A] / df[B]\n",
    "            feats[f\"{a}_RS_20_{b}\"] = df[A].pct_change(20) - df[B].pct_change(20)\n",
    "            feats[f\"{a}_Corr60_{b}\"] = df[A].rolling(60).corr(df[B])\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# PCA\n",
    "# ==========================================================\n",
    "\n",
    "def add_pca(df, tickers):\n",
    "    feats = {}\n",
    "    ret_cols = [f\"{t}_Return_1d\" for t in tickers if f\"{t}_Return_1d\" in df.columns]\n",
    "\n",
    "    X = df[ret_cols].dropna()\n",
    "    if len(X) < 200 or len(ret_cols) < 3:\n",
    "        return feats\n",
    "\n",
    "    try:\n",
    "        pca = PCA(n_components=3)\n",
    "        comps = pca.fit_transform(X)\n",
    "\n",
    "        feats[\"PCA_1\"] = pd.Series(comps[:, 0], index=X.index)\n",
    "        feats[\"PCA_2\"] = pd.Series(comps[:, 1], index=X.index)\n",
    "        feats[\"PCA_3\"] = pd.Series(comps[:, 2], index=X.index)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"PCA failed: {e}\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#  MASTER PIPELINE (TICKER-NEUTRAL)\n",
    "# ==========================================================\n",
    "\n",
    "def create_all_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fully unified ticker-neutral feature engine.\n",
    "\n",
    "    Order:\n",
    "    1. Technical per-ticker features\n",
    "    2. Volatility features\n",
    "    3. Volume features\n",
    "    4. Entropy\n",
    "    5. HMM states\n",
    "    6. Advanced per-ticker features (beta, skew, ADX, trend R², vov)\n",
    "    7. Cross-ticker features\n",
    "    8. PCA latent factors\n",
    "    9. Single concat at end\n",
    "\n",
    "    Returns:\n",
    "        df with all new features added.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    tickers = infer_tickers(df)\n",
    "\n",
    "    logger.info(f\"Building features for {len(tickers)} tickers...\")\n",
    "\n",
    "    new_feats = {}\n",
    "\n",
    "    # ------------------ Per-ticker blocks ------------------\n",
    "    for t in tqdm(tickers, desc=\"Per-ticker base features\"):\n",
    "        cols = ohlcv_cols(df, t)\n",
    "        if not cols:\n",
    "            continue\n",
    "\n",
    "        open_, high, low, close, volume = cols\n",
    "\n",
    "        new_feats.update(add_momentum_features(df, t, close))\n",
    "        new_feats.update(add_vol_features(df, t, open_, high, low, close))\n",
    "        new_feats.update(add_volume_features(df, t, volume, close))\n",
    "        new_feats.update(add_entropy(df, t))\n",
    "        new_feats.update(add_hmm_states(df, t))\n",
    "\n",
    "    # ------------------ Advanced per-ticker blocks ------------------\n",
    "    for t in tqdm(tickers, desc=\"Per-ticker advanced features\"):\n",
    "        new_feats.update(add_advanced_ticker_features(df, t, tickers))\n",
    "\n",
    "    # ------------------ Cross-ticker ------------------\n",
    "    logger.info(\"Building cross-ticker features...\")\n",
    "    new_feats.update(add_cross_features(df, tickers))\n",
    "\n",
    "    # ------------------ PCA ------------------\n",
    "    logger.info(\"Computing PCA...\")\n",
    "    new_feats.update(add_pca(df, tickers))\n",
    "\n",
    "    # ------------------ Final Merge ------------------\n",
    "    logger.info(\"Merging feature matrix...\")\n",
    "    df = pd.concat([df, pd.DataFrame(new_feats, index=df.index)], axis=1)\n",
    "\n",
    "    logger.info(\"Feature engineering complete.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logging setup (clean output, no spam)\n",
    "# ---------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#  HELPER FUNCTIONS (all return dicts to avoid fragmentation)\n",
    "# =====================================================================\n",
    "\n",
    "def ohlcv_cols(df: pd.DataFrame, ticker: str):\n",
    "    \"\"\"\n",
    "    Returns (Open, High, Low, Close, Volume) column names for a ticker.\n",
    "    If any are missing → return None.\n",
    "    \"\"\"\n",
    "    cols = [f\"{ticker}_{c}\" for c in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "    return cols if all(c in df.columns for c in cols) else None\n",
    "\n",
    "\n",
    "# ----------------------------- MOMENTUM ------------------------------\n",
    "def add_momentum_features(df: pd.DataFrame, ticker: str, close: str) -> dict:\n",
    "    feats = {}\n",
    "\n",
    "    # Simple returns\n",
    "    for w in [1, 5, 10, 20]:\n",
    "        feats[f\"{ticker}_Return_{w}d\"] = df[close].pct_change(w)\n",
    "\n",
    "    # SMAs / EMAs\n",
    "    feats[f\"{ticker}_SMA_10\"] = df[close].rolling(10).mean()\n",
    "    feats[f\"{ticker}_SMA_50\"] = df[close].rolling(50).mean()\n",
    "    feats[f\"{ticker}_EMA_20\"] = df[close].ewm(span=20).mean()\n",
    "    feats[f\"{ticker}_EMA_50\"] = df[close].ewm(span=50).mean()\n",
    "    feats[f\"{ticker}_MA_Cross\"] = feats[f\"{ticker}_EMA_20\"] / feats[f\"{ticker}_EMA_50\"]\n",
    "\n",
    "    # MACD\n",
    "    ema12 = df[close].ewm(span=12).mean()\n",
    "    ema26 = df[close].ewm(span=26).mean()\n",
    "    macd = ema12 - ema26\n",
    "\n",
    "    feats[f\"{ticker}_MACD\"] = macd\n",
    "    feats[f\"{ticker}_MACD_sig\"] = macd.ewm(span=9).mean()\n",
    "    feats[f\"{ticker}_MACD_hist\"] = feats[f\"{ticker}_MACD\"] - feats[f\"{ticker}_MACD_sig\"]\n",
    "\n",
    "    # RSI\n",
    "    delta = df[close].diff()\n",
    "    up = delta.clip(lower=0).rolling(14).mean()\n",
    "    down = (-delta.clip(lower=0)).rolling(14).mean()\n",
    "    rs = up / down\n",
    "    feats[f\"{ticker}_RSI_14\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Stochastics\n",
    "    low14 = df[close].rolling(14).min()\n",
    "    high14 = df[close].rolling(14).max()\n",
    "    feats[f\"{ticker}_StochK\"] = 100 * (df[close] - low14) / (high14 - low14)\n",
    "    feats[f\"{ticker}_StochD\"] = feats[f\"{ticker}_StochK\"].rolling(3).mean()\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ----------------------------- VOLATILITY ----------------------------\n",
    "def add_vol_features(\n",
    "    df: pd.DataFrame,\n",
    "    ticker: str,\n",
    "    open_: str,\n",
    "    high: str,\n",
    "    low: str,\n",
    "    close: str\n",
    ") -> dict:\n",
    "    feats = {}\n",
    "\n",
    "    # Standard deviation of returns\n",
    "    feats[f\"{ticker}_Vol_20\"] = df[f\"{ticker}_Return_1d\"].rolling(20).std()\n",
    "\n",
    "    # Parkinson volatility\n",
    "    feats[f\"{ticker}_Parkinson_20\"] = (\n",
    "        (np.log(df[high] / df[low]) ** 2).rolling(20).mean()\n",
    "        * (1 / (4 * np.log(2)))\n",
    "    )\n",
    "\n",
    "    # Garman-Klass volatility\n",
    "    feats[f\"{ticker}_GK_20\"] = (\n",
    "        0.5 * (np.log(df[high] / df[low]) ** 2)\n",
    "        - (2 * np.log(np.e) - 1) * (np.log(df[close] / df[open_]) ** 2)\n",
    "    ).rolling(20).mean()\n",
    "\n",
    "    # Bollinger width\n",
    "    sma20 = df[close].rolling(20).mean()\n",
    "    std20 = df[close].rolling(20).std()\n",
    "    feats[f\"{ticker}_BB_width\"] = (2 * std20) / sma20\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ----------------------------- VOLUME -------------------------------\n",
    "def add_volume_features(df: pd.DataFrame, ticker: str, volume: str, close: str) -> dict:\n",
    "    feats = {}\n",
    "\n",
    "    feats[f\"{ticker}_Volume_ROC\"] = df[volume].pct_change(5)\n",
    "\n",
    "    feats[f\"{ticker}_Volume_Z\"] = (\n",
    "        df[volume] - df[volume].rolling(20).mean()\n",
    "    ) / df[volume].rolling(20).std()\n",
    "\n",
    "    feats[f\"{ticker}_OBV\"] = (np.sign(df[close].diff()) * df[volume]).fillna(0).cumsum()\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ----------------------------- ENTROPY ------------------------------\n",
    "def add_entropy(df: pd.DataFrame, ticker: str) -> dict:\n",
    "    feats = {}\n",
    "    ret = f\"{ticker}_Return_1d\"\n",
    "    if ret in df.columns:\n",
    "        feats[f\"{ticker}_Entropy_20\"] = (df[ret] ** 2).rolling(20).sum()\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ------------------------------ HMM --------------------------------\n",
    "def add_hmm_states(df: pd.DataFrame, ticker: str) -> dict:\n",
    "    feats = {}\n",
    "    ret = f\"{ticker}_Return_1d\"\n",
    "    if ret not in df.columns:\n",
    "        return feats\n",
    "\n",
    "    series = df[ret].dropna()\n",
    "    if len(series) < 200:\n",
    "        return feats\n",
    "\n",
    "    # More stable HMM configuration\n",
    "    hmm = GaussianHMM(\n",
    "        n_components=3,\n",
    "        covariance_type=\"full\",\n",
    "        n_iter=800,        # higher iteration cap\n",
    "        tol=1e-3,          # easier to satisfy convergence\n",
    "        init_params=\"mct\", # more stable initialization\n",
    "        params=\"mct\"       # no need to estimate weights\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Fit & predict — warnings are NOT suppressed\n",
    "        hmm.fit(series.values.reshape(-1, 1))\n",
    "        states = hmm.predict(series.values.reshape(-1, 1))\n",
    "\n",
    "        feats[f\"{ticker}_HMM\"] = pd.Series(states, index=series.index)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"HMM failed for {ticker}: {e}\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ----------------------------- CROSS-TICKER -------------------------\n",
    "def add_cross_features(df: pd.DataFrame, base_ticker: str, target_ticker: str) -> dict:\n",
    "    feats = {}\n",
    "    base = f\"{base_ticker}_Close\"\n",
    "    tgt  = f\"{target_ticker}_Close\"\n",
    "\n",
    "    if base not in df.columns or tgt not in df.columns:\n",
    "        return feats\n",
    "\n",
    "    feats[f\"{target_ticker}_Ratio_{base_ticker}\"] = df[tgt] / df[base]\n",
    "    feats[f\"{target_ticker}_RS_20\"] = df[tgt].pct_change(20) - df[base].pct_change(20)\n",
    "    feats[f\"{target_ticker}_Corr_{base_ticker}_60\"] = df[tgt].rolling(60).corr(df[base])\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ----------------------------- PCA --------------------------------\n",
    "def add_pca(df: pd.DataFrame, tickers: list) -> dict:\n",
    "    feats = {}\n",
    "\n",
    "    ret_cols = [f\"{t}_Return_1d\" for t in tickers if f\"{t}_Return_1d\" in df.columns]\n",
    "    data = df[ret_cols].dropna()\n",
    "\n",
    "    if len(data) < 200:\n",
    "        return feats\n",
    "\n",
    "    try:\n",
    "        pca = PCA(n_components=3)\n",
    "        comps = pca.fit_transform(data)\n",
    "\n",
    "        feats[\"PCA_1\"] = pd.Series(comps[:, 0], index=data.index)\n",
    "        feats[\"PCA_2\"] = pd.Series(comps[:, 1], index=data.index)\n",
    "        feats[\"PCA_3\"] = pd.Series(comps[:, 2], index=data.index)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"PCA failed: {e}\")\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#  MAIN PIPELINE — CLEAN, SAFE, FAST\n",
    "# =====================================================================\n",
    "\n",
    "def create_all_features(df: pd.DataFrame, base: str = \"PPH\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Master feature engineering pipeline.\n",
    "\n",
    "    - Fully leakage-safe\n",
    "    - Fragmentation-free (only one concat)\n",
    "    - Includes technicals, volatility, volume, entropy, HMM, PCA, cross-ticker.\n",
    "    - Logs progress\n",
    "    - Uses tqdm for progress bars\n",
    "    \"\"\"\n",
    "\n",
    "    new_features = {}\n",
    "    tickers = sorted({c.split('_')[0] for c in df.columns})\n",
    "\n",
    "    logger.info(f\"Building features for {len(tickers)} tickers...\")\n",
    "    pbar = tqdm(tickers, desc=\"Per-ticker features\")\n",
    "\n",
    "    # -------------------- Per-ticker features --------------------\n",
    "    for ticker in pbar:\n",
    "        cols = ohlcv_cols(df, ticker)\n",
    "        if not cols:\n",
    "            continue\n",
    "\n",
    "        open_, high, low, close, volume = cols\n",
    "\n",
    "        new_features.update(add_momentum_features(df, ticker, close))\n",
    "        new_features.update(add_vol_features(df, ticker, open_, high, low, close))\n",
    "        new_features.update(add_volume_features(df, ticker, volume, close))\n",
    "        new_features.update(add_entropy(df, ticker))\n",
    "        new_features.update(add_hmm_states(df, ticker))\n",
    "\n",
    "    # -------------------- Cross-ticker features -------------------\n",
    "    logger.info(\"Building cross-ticker features...\")\n",
    "    for t in tqdm(tickers, desc=\"Cross features\"):\n",
    "        new_features.update(add_cross_features(df, base, t))\n",
    "\n",
    "    # -------------------- PCA latent factors ----------------------\n",
    "    logger.info(\"Fitting PCA on returns...\")\n",
    "    new_features.update(add_pca(df, tickers))\n",
    "\n",
    "    # -------------------- CONCAT ONCE (NO FRAGMENTATION) ---------\n",
    "    logger.info(\"Merging feature matrix...\")\n",
    "    df = pd.concat([df, pd.DataFrame(new_features, index=df.index)], axis=1)\n",
    "\n",
    "    logger.info(\"Feature engineering complete.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = create_all_features(data)\n",
    "df_full.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91ec6e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63eaec27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hmmlearn\n",
      "  Downloading hmmlearn-0.3.3-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.10 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from hmmlearn) (2.3.5)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from hmmlearn) (1.7.2)\n",
      "Requirement already satisfied: scipy>=0.19 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from hmmlearn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.6.0)\n",
      "Downloading hmmlearn-0.3.3-cp312-cp312-macosx_10_9_universal2.whl (196 kB)\n",
      "Installing collected packages: hmmlearn\n",
      "Successfully installed hmmlearn-0.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd6122af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIXY_Open</th>\n",
       "      <th>VIXY_High</th>\n",
       "      <th>VIXY_Low</th>\n",
       "      <th>VIXY_Close</th>\n",
       "      <th>VIXY_Volume</th>\n",
       "      <th>XPH_Open</th>\n",
       "      <th>XPH_High</th>\n",
       "      <th>XPH_Low</th>\n",
       "      <th>XPH_Close</th>\n",
       "      <th>XPH_Volume</th>\n",
       "      <th>...</th>\n",
       "      <th>IBB_Open</th>\n",
       "      <th>IBB_High</th>\n",
       "      <th>IBB_Low</th>\n",
       "      <th>IBB_Close</th>\n",
       "      <th>IBB_Volume</th>\n",
       "      <th>IHE_Open</th>\n",
       "      <th>IHE_High</th>\n",
       "      <th>IHE_Low</th>\n",
       "      <th>IHE_Close</th>\n",
       "      <th>IHE_Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>633120.0</td>\n",
       "      <td>650800.0</td>\n",
       "      <td>633120.0</td>\n",
       "      <td>633840.0</td>\n",
       "      <td>14</td>\n",
       "      <td>23.285000</td>\n",
       "      <td>23.285000</td>\n",
       "      <td>22.965000</td>\n",
       "      <td>23.059999</td>\n",
       "      <td>42000</td>\n",
       "      <td>...</td>\n",
       "      <td>31.620001</td>\n",
       "      <td>31.620001</td>\n",
       "      <td>31.139999</td>\n",
       "      <td>31.299999</td>\n",
       "      <td>1088100</td>\n",
       "      <td>21.650000</td>\n",
       "      <td>21.650000</td>\n",
       "      <td>21.370001</td>\n",
       "      <td>21.469999</td>\n",
       "      <td>54300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>640400.0</td>\n",
       "      <td>640800.0</td>\n",
       "      <td>617440.0</td>\n",
       "      <td>620400.0</td>\n",
       "      <td>9</td>\n",
       "      <td>22.945000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>22.945000</td>\n",
       "      <td>23.219999</td>\n",
       "      <td>54400</td>\n",
       "      <td>...</td>\n",
       "      <td>31.299999</td>\n",
       "      <td>31.623333</td>\n",
       "      <td>31.240000</td>\n",
       "      <td>31.549999</td>\n",
       "      <td>1756800</td>\n",
       "      <td>21.353333</td>\n",
       "      <td>21.576668</td>\n",
       "      <td>21.353333</td>\n",
       "      <td>21.563334</td>\n",
       "      <td>34200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-06</th>\n",
       "      <td>619520.0</td>\n",
       "      <td>629440.0</td>\n",
       "      <td>614960.0</td>\n",
       "      <td>623040.0</td>\n",
       "      <td>10</td>\n",
       "      <td>23.360001</td>\n",
       "      <td>23.395000</td>\n",
       "      <td>23.209999</td>\n",
       "      <td>23.350000</td>\n",
       "      <td>34600</td>\n",
       "      <td>...</td>\n",
       "      <td>31.450001</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>31.450001</td>\n",
       "      <td>31.696667</td>\n",
       "      <td>714300</td>\n",
       "      <td>21.663334</td>\n",
       "      <td>21.663334</td>\n",
       "      <td>21.543333</td>\n",
       "      <td>21.626667</td>\n",
       "      <td>195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-07</th>\n",
       "      <td>617920.0</td>\n",
       "      <td>640080.0</td>\n",
       "      <td>609440.0</td>\n",
       "      <td>624320.0</td>\n",
       "      <td>5</td>\n",
       "      <td>23.290001</td>\n",
       "      <td>23.400000</td>\n",
       "      <td>23.115000</td>\n",
       "      <td>23.375000</td>\n",
       "      <td>96200</td>\n",
       "      <td>...</td>\n",
       "      <td>31.673332</td>\n",
       "      <td>31.796667</td>\n",
       "      <td>31.466667</td>\n",
       "      <td>31.683332</td>\n",
       "      <td>1017300</td>\n",
       "      <td>21.656668</td>\n",
       "      <td>21.666668</td>\n",
       "      <td>21.466667</td>\n",
       "      <td>21.626667</td>\n",
       "      <td>21300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-10</th>\n",
       "      <td>637120.0</td>\n",
       "      <td>646960.0</td>\n",
       "      <td>622080.0</td>\n",
       "      <td>623040.0</td>\n",
       "      <td>9</td>\n",
       "      <td>23.370001</td>\n",
       "      <td>23.504999</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>23.480000</td>\n",
       "      <td>59600</td>\n",
       "      <td>...</td>\n",
       "      <td>31.660000</td>\n",
       "      <td>31.693333</td>\n",
       "      <td>31.440001</td>\n",
       "      <td>31.646667</td>\n",
       "      <td>938400</td>\n",
       "      <td>21.530001</td>\n",
       "      <td>21.616667</td>\n",
       "      <td>21.480000</td>\n",
       "      <td>21.616667</td>\n",
       "      <td>46200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            VIXY_Open  VIXY_High  VIXY_Low  VIXY_Close  VIXY_Volume  \\\n",
       "Date                                                                  \n",
       "2011-01-04   633120.0   650800.0  633120.0    633840.0           14   \n",
       "2011-01-05   640400.0   640800.0  617440.0    620400.0            9   \n",
       "2011-01-06   619520.0   629440.0  614960.0    623040.0           10   \n",
       "2011-01-07   617920.0   640080.0  609440.0    624320.0            5   \n",
       "2011-01-10   637120.0   646960.0  622080.0    623040.0            9   \n",
       "\n",
       "             XPH_Open   XPH_High    XPH_Low  XPH_Close  XPH_Volume  ...  \\\n",
       "Date                                                                ...   \n",
       "2011-01-04  23.285000  23.285000  22.965000  23.059999       42000  ...   \n",
       "2011-01-05  22.945000  23.250000  22.945000  23.219999       54400  ...   \n",
       "2011-01-06  23.360001  23.395000  23.209999  23.350000       34600  ...   \n",
       "2011-01-07  23.290001  23.400000  23.115000  23.375000       96200  ...   \n",
       "2011-01-10  23.370001  23.504999  23.250000  23.480000       59600  ...   \n",
       "\n",
       "             IBB_Open   IBB_High    IBB_Low  IBB_Close  IBB_Volume   IHE_Open  \\\n",
       "Date                                                                            \n",
       "2011-01-04  31.620001  31.620001  31.139999  31.299999     1088100  21.650000   \n",
       "2011-01-05  31.299999  31.623333  31.240000  31.549999     1756800  21.353333   \n",
       "2011-01-06  31.450001  31.750000  31.450001  31.696667      714300  21.663334   \n",
       "2011-01-07  31.673332  31.796667  31.466667  31.683332     1017300  21.656668   \n",
       "2011-01-10  31.660000  31.693333  31.440001  31.646667      938400  21.530001   \n",
       "\n",
       "             IHE_High    IHE_Low  IHE_Close  IHE_Volume  \n",
       "Date                                                     \n",
       "2011-01-04  21.650000  21.370001  21.469999       54300  \n",
       "2011-01-05  21.576668  21.353333  21.563334       34200  \n",
       "2011-01-06  21.663334  21.543333  21.626667      195300  \n",
       "2011-01-07  21.666668  21.466667  21.626667       21300  \n",
       "2011-01-10  21.616667  21.480000  21.616667       46200  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "466487be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building features for 10 tickers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068ebf4f61614ad0bbebc34b20389566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Per-ticker features:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not converging.  Current: 10698.493008110583 is not greater than 10698.52922953993. Delta is -0.0362214293472789\n",
      "Model is not converging.  Current: 12243.895646050472 is not greater than 12243.901178246677. Delta is -0.005532196204512729\n",
      "Model is not converging.  Current: 12385.812674391645 is not greater than 12385.818225453213. Delta is -0.005551061567530269\n",
      "Model is not converging.  Current: 12228.998505180101 is not greater than 12229.012395682737. Delta is -0.013890502636058955\n",
      "Model is not converging.  Current: 12329.586963596239 is not greater than 12329.5913701717. Delta is -0.0044065754609619034\n",
      "Building cross-ticker features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da65654db6ba4bb6890b9ebc66d493b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cross features:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting PCA on returns...\n",
      "Merging feature matrix...\n",
      "Feature engineering complete.\n"
     ]
    }
   ],
   "source": [
    "data = create_all_features(df, base=\"PPH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70e9a3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3749, 537)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cad560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "068dff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting holidays\n",
      "  Downloading holidays-0.86-py3-none-any.whl.metadata (50 kB)\n",
      "Collecting pandas-market-calendars\n",
      "  Downloading pandas_market_calendars-5.1.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: requests in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: python-dateutil in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from holidays) (2.9.0.post0)\n",
      "Collecting exchange-calendars>=3.3 (from pandas-market-calendars)\n",
      "  Downloading exchange_calendars-4.11.3-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: pandas>=1.1 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from pandas-market-calendars) (2.3.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: numpy>=1.26.4 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas-market-calendars) (2.3.5)\n",
      "Collecting pyluach>=2.3.0 (from exchange-calendars>=3.3->pandas-market-calendars)\n",
      "  Downloading pyluach-2.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting toolz>=1.0.0 (from exchange-calendars>=3.3->pandas-market-calendars)\n",
      "  Downloading toolz-1.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: tzdata>=2025.2 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas-market-calendars) (2025.2)\n",
      "Collecting korean_lunar_calendar>=0.3.1 (from exchange-calendars>=3.3->pandas-market-calendars)\n",
      "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from pandas>=1.1->pandas-market-calendars) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pnl1f276/.pyenv/versions/Trend-Surgeon/lib/python3.12/site-packages (from python-dateutil->holidays) (1.17.0)\n",
      "Downloading holidays-0.86-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0ms\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas_market_calendars-5.1.3-py3-none-any.whl (127 kB)\n",
      "Downloading exchange_calendars-4.11.3-py3-none-any.whl (209 kB)\n",
      "Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
      "Downloading pyluach-2.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading toolz-1.1.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: korean_lunar_calendar, toolz, pyluach, holidays, exchange-calendars, pandas-market-calendars\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [pandas-market-calendars][32m4/6\u001b[0m [exchange-calendars]\n",
      "\u001b[1A\u001b[2KSuccessfully installed exchange-calendars-4.11.3 holidays-0.86 korean_lunar_calendar-0.3.1 pandas-market-calendars-5.1.3 pyluach-2.3.0 toolz-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install holidays pandas-market-calendars requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64ee09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import holidays\n",
    "from pandas_market_calendars import get_calendar\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Fetch Federal Reserve meeting dates (auto-updating if possible)\n",
    "# -----------------------------------------------------------\n",
    "def fetch_fomc_dates():\n",
    "    \"\"\"\n",
    "    Fetch FOMC meeting dates directly from the Federal Reserve.\n",
    "    Falls back to known dates if parsing fails.\n",
    "    Returns a Pandas Series (not Index) for .dt access.\n",
    "    \"\"\"\n",
    "    url = \"https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\"\n",
    "    try:\n",
    "        tables = pd.read_html(url)\n",
    "        dates = pd.to_datetime(tables[0].iloc[:, 0], errors=\"coerce\").dropna()\n",
    "        return pd.Series(dates)     # <- FIX: return Series, not DatetimeIndex\n",
    "    except:\n",
    "        fallback = pd.to_datetime([\n",
    "            \"2024-01-31\",\"2024-03-20\",\"2024-05-01\",\n",
    "            \"2024-06-12\",\"2024-07-31\",\"2024-09-18\",\n",
    "            \"2024-11-07\",\"2024-12-18\"\n",
    "        ])\n",
    "        return pd.Series(fallback)  # <- FIX: return Series\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Macro release calendar (CPI, NFP, PPI, GDP)\n",
    "# -----------------------------------------------------------\n",
    "def macro_calendar():\n",
    "    \"\"\"\n",
    "    High-impact macroeconomic dates (extendable).\n",
    "    Returns dict of name -> Series of dates.\n",
    "    \"\"\"\n",
    "    events = {\n",
    "        \"cpi\": [\"2024-01-11\",\"2024-02-13\",\"2024-03-12\",\"2024-04-10\"],\n",
    "        \"nfp\": [\"2024-01-05\",\"2024-02-02\",\"2024-03-08\",\"2024-04-05\"],\n",
    "        \"ppi\": [\"2024-01-12\",\"2024-02-16\",\"2024-03-14\",\"2024-04-11\"],\n",
    "        \"gdp\": [\"2024-01-25\",\"2024-02-28\",\"2024-03-28\",\"2024-04-25\"],\n",
    "    }\n",
    "    return {k: pd.Series(pd.to_datetime(v)) for k, v in events.items()}\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Basic calendar seasonality (strong signals)\n",
    "# -----------------------------------------------------------\n",
    "def add_calendar_basics(df):\n",
    "    df[\"day_of_week\"]    = df[\"date\"].dt.dayofweek\n",
    "    df[\"day_of_month\"]   = df[\"date\"].dt.day\n",
    "    df[\"month\"]          = df[\"date\"].dt.month\n",
    "    df[\"quarter\"]        = df[\"date\"].dt.quarter\n",
    "    df[\"is_month_end\"]   = df[\"date\"].dt.is_month_end.astype(int)\n",
    "    df[\"is_month_start\"] = df[\"date\"].dt.is_month_start.astype(int)\n",
    "    df[\"is_year_end\"]    = ((df[\"month\"] == 12) & (df[\"day_of_month\"] == 31)).astype(int)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# US holiday adjacency (pre/post holiday drift)\n",
    "# -----------------------------------------------------------\n",
    "def add_holiday_features(df):\n",
    "    us_holidays = holidays.US()\n",
    "    df[\"is_holiday_adjacent\"] = df[\"date\"].apply(\n",
    "        lambda d: int((d + pd.Timedelta(days=1) in us_holidays) or\n",
    "                      (d - pd.Timedelta(days=1) in us_holidays))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Option expiration week (3rd Friday of each month)\n",
    "# -----------------------------------------------------------\n",
    "def add_opex_features(df):\n",
    "    df[\"is_opex_week\"] = df[\"date\"].apply(\n",
    "        lambda d: int((d.weekday() == 4) and (15 <= d.day <= 21))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# FOMC week (fed policy drift + volatility cluster)\n",
    "# -----------------------------------------------------------\n",
    "def add_fomc_features(df):\n",
    "    fomc_dates = fetch_fomc_dates()   # <-- now a Series, safe for .dt\n",
    "    fomc_weeks = fomc_dates.dt.isocalendar().week.unique()\n",
    "    df[\"is_fomc_week\"] = df[\"date\"].dt.isocalendar().week.isin(fomc_weeks).astype(int)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# High-impact macroeconomic event flags (day + week)\n",
    "# -----------------------------------------------------------\n",
    "def add_macro_features(df):\n",
    "    macros = macro_calendar()\n",
    "\n",
    "    for name, dates in macros.items():\n",
    "        # exact date match\n",
    "        df[f\"is_{name}_day\"] = df[\"date\"].isin(dates).astype(int)\n",
    "        # week match\n",
    "        df[f\"is_{name}_week\"] = (\n",
    "            df[\"date\"].dt.isocalendar().week.isin(\n",
    "                dates.dt.isocalendar().week.unique()\n",
    "            )\n",
    "        ).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# MASTER WRAPPER — assumes date is the index\n",
    "# -----------------------------------------------------------\n",
    "def add_all_calendar_features(df):\n",
    "    \"\"\"\n",
    "    Adds all high-value seasonality and event-driven features.\n",
    "    Assumes df.index is the date (yfinance default).\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert index → \"date\" column\n",
    "    df = df.reset_index().rename(columns={df.index.name or \"index\": \"date\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    # Add feature blocks\n",
    "    df = add_calendar_basics(df)\n",
    "    df = add_holiday_features(df)\n",
    "    df = add_opex_features(df)\n",
    "    df = add_fomc_features(df)\n",
    "    df = add_macro_features(df)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01d52c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_all_calendar_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07903e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>VIXY_Open</th>\n",
       "      <th>VIXY_High</th>\n",
       "      <th>VIXY_Low</th>\n",
       "      <th>VIXY_Close</th>\n",
       "      <th>VIXY_Volume</th>\n",
       "      <th>XPH_Open</th>\n",
       "      <th>XPH_High</th>\n",
       "      <th>XPH_Low</th>\n",
       "      <th>XPH_Close</th>\n",
       "      <th>...</th>\n",
       "      <th>is_opex_week</th>\n",
       "      <th>is_fomc_week</th>\n",
       "      <th>is_cpi_day</th>\n",
       "      <th>is_cpi_week</th>\n",
       "      <th>is_nfp_day</th>\n",
       "      <th>is_nfp_week</th>\n",
       "      <th>is_ppi_day</th>\n",
       "      <th>is_ppi_week</th>\n",
       "      <th>is_gdp_day</th>\n",
       "      <th>is_gdp_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>633120.0</td>\n",
       "      <td>650800.0</td>\n",
       "      <td>633120.0</td>\n",
       "      <td>633840.0</td>\n",
       "      <td>14</td>\n",
       "      <td>23.285000</td>\n",
       "      <td>23.285000</td>\n",
       "      <td>22.965000</td>\n",
       "      <td>23.059999</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>640400.0</td>\n",
       "      <td>640800.0</td>\n",
       "      <td>617440.0</td>\n",
       "      <td>620400.0</td>\n",
       "      <td>9</td>\n",
       "      <td>22.945000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>22.945000</td>\n",
       "      <td>23.219999</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-06</td>\n",
       "      <td>619520.0</td>\n",
       "      <td>629440.0</td>\n",
       "      <td>614960.0</td>\n",
       "      <td>623040.0</td>\n",
       "      <td>10</td>\n",
       "      <td>23.360001</td>\n",
       "      <td>23.395000</td>\n",
       "      <td>23.209999</td>\n",
       "      <td>23.350000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-07</td>\n",
       "      <td>617920.0</td>\n",
       "      <td>640080.0</td>\n",
       "      <td>609440.0</td>\n",
       "      <td>624320.0</td>\n",
       "      <td>5</td>\n",
       "      <td>23.290001</td>\n",
       "      <td>23.400000</td>\n",
       "      <td>23.115000</td>\n",
       "      <td>23.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-10</td>\n",
       "      <td>637120.0</td>\n",
       "      <td>646960.0</td>\n",
       "      <td>622080.0</td>\n",
       "      <td>623040.0</td>\n",
       "      <td>9</td>\n",
       "      <td>23.370001</td>\n",
       "      <td>23.504999</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>23.480000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 556 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  VIXY_Open  VIXY_High  VIXY_Low  VIXY_Close  VIXY_Volume  \\\n",
       "0 2011-01-04   633120.0   650800.0  633120.0    633840.0           14   \n",
       "1 2011-01-05   640400.0   640800.0  617440.0    620400.0            9   \n",
       "2 2011-01-06   619520.0   629440.0  614960.0    623040.0           10   \n",
       "3 2011-01-07   617920.0   640080.0  609440.0    624320.0            5   \n",
       "4 2011-01-10   637120.0   646960.0  622080.0    623040.0            9   \n",
       "\n",
       "    XPH_Open   XPH_High    XPH_Low  XPH_Close  ...  is_opex_week  \\\n",
       "0  23.285000  23.285000  22.965000  23.059999  ...             0   \n",
       "1  22.945000  23.250000  22.945000  23.219999  ...             0   \n",
       "2  23.360001  23.395000  23.209999  23.350000  ...             0   \n",
       "3  23.290001  23.400000  23.115000  23.375000  ...             0   \n",
       "4  23.370001  23.504999  23.250000  23.480000  ...             0   \n",
       "\n",
       "   is_fomc_week  is_cpi_day  is_cpi_week  is_nfp_day  is_nfp_week  is_ppi_day  \\\n",
       "0             0           0            0           0            1           0   \n",
       "1             0           0            0           0            1           0   \n",
       "2             0           0            0           0            1           0   \n",
       "3             0           0            0           0            1           0   \n",
       "4             0           0            1           0            0           0   \n",
       "\n",
       "   is_ppi_week  is_gdp_day  is_gdp_week  \n",
       "0            0           0            0  \n",
       "1            0           0            0  \n",
       "2            0           0            0  \n",
       "3            0           0            0  \n",
       "4            1           0            0  \n",
       "\n",
       "[5 rows x 556 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa9a36e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "VIXY_Open\n",
      "VIXY_High\n",
      "VIXY_Low\n",
      "VIXY_Close\n",
      "VIXY_Volume\n",
      "XPH_Open\n",
      "XPH_High\n",
      "XPH_Low\n",
      "XPH_Close\n",
      "XPH_Volume\n",
      "VHT_Open\n",
      "VHT_High\n",
      "VHT_Low\n",
      "VHT_Close\n",
      "VHT_Volume\n",
      "XLV_Open\n",
      "XLV_High\n",
      "XLV_Low\n",
      "XLV_Close\n",
      "XLV_Volume\n",
      "PPH_Open\n",
      "PPH_High\n",
      "PPH_Low\n",
      "PPH_Close\n",
      "PPH_Volume\n",
      "SPY_Open\n",
      "SPY_High\n",
      "SPY_Low\n",
      "SPY_Close\n",
      "SPY_Volume\n",
      "XBI_Open\n",
      "XBI_High\n",
      "XBI_Low\n",
      "XBI_Close\n",
      "XBI_Volume\n",
      "IBB_Open\n",
      "IBB_High\n",
      "IBB_Low\n",
      "IBB_Close\n",
      "IBB_Volume\n",
      "IHE_Open\n",
      "IHE_High\n",
      "IHE_Low\n",
      "IHE_Close\n",
      "IHE_Volume\n",
      "IBB_Return_1d\n",
      "IBB_Return_5d\n",
      "IBB_Return_10d\n",
      "IBB_Return_20d\n",
      "IBB_SMA_10\n",
      "IBB_SMA_50\n",
      "IBB_EMA_20\n",
      "IBB_EMA_50\n",
      "IBB_MA_Cross\n",
      "IBB_MACD\n",
      "IBB_MACD_sig\n",
      "IBB_MACD_hist\n",
      "IBB_RSI_14\n",
      "IBB_StochK\n",
      "IBB_StochD\n",
      "IBB_Vol_20\n",
      "IBB_Parkinson_20\n",
      "IBB_GK_20\n",
      "IBB_BB_width\n",
      "IBB_Volume_ROC\n",
      "IBB_Volume_Z\n",
      "IBB_OBV\n",
      "IBB_Entropy_20\n",
      "IBB_HMM\n",
      "IHE_Return_1d\n",
      "IHE_Return_5d\n",
      "IHE_Return_10d\n",
      "IHE_Return_20d\n",
      "IHE_SMA_10\n",
      "IHE_SMA_50\n",
      "IHE_EMA_20\n",
      "IHE_EMA_50\n",
      "IHE_MA_Cross\n",
      "IHE_MACD\n",
      "IHE_MACD_sig\n",
      "IHE_MACD_hist\n",
      "IHE_RSI_14\n",
      "IHE_StochK\n",
      "IHE_StochD\n",
      "IHE_Vol_20\n",
      "IHE_Parkinson_20\n",
      "IHE_GK_20\n",
      "IHE_BB_width\n",
      "IHE_Volume_ROC\n",
      "IHE_Volume_Z\n",
      "IHE_OBV\n",
      "IHE_Entropy_20\n",
      "IHE_HMM\n",
      "PPH_Return_1d\n",
      "PPH_Return_5d\n",
      "PPH_Return_10d\n",
      "PPH_Return_20d\n",
      "PPH_SMA_10\n",
      "PPH_SMA_50\n",
      "PPH_EMA_20\n",
      "PPH_EMA_50\n",
      "PPH_MA_Cross\n",
      "PPH_MACD\n",
      "PPH_MACD_sig\n",
      "PPH_MACD_hist\n",
      "PPH_RSI_14\n",
      "PPH_StochK\n",
      "PPH_StochD\n",
      "PPH_Vol_20\n",
      "PPH_Parkinson_20\n",
      "PPH_GK_20\n",
      "PPH_BB_width\n",
      "PPH_Volume_ROC\n",
      "PPH_Volume_Z\n",
      "PPH_OBV\n",
      "PPH_Entropy_20\n",
      "PPH_HMM\n",
      "SPY_Return_1d\n",
      "SPY_Return_5d\n",
      "SPY_Return_10d\n",
      "SPY_Return_20d\n",
      "SPY_SMA_10\n",
      "SPY_SMA_50\n",
      "SPY_EMA_20\n",
      "SPY_EMA_50\n",
      "SPY_MA_Cross\n",
      "SPY_MACD\n",
      "SPY_MACD_sig\n",
      "SPY_MACD_hist\n",
      "SPY_RSI_14\n",
      "SPY_StochK\n",
      "SPY_StochD\n",
      "SPY_Vol_20\n",
      "SPY_Parkinson_20\n",
      "SPY_GK_20\n",
      "SPY_BB_width\n",
      "SPY_Volume_ROC\n",
      "SPY_Volume_Z\n",
      "SPY_OBV\n",
      "SPY_Entropy_20\n",
      "SPY_HMM\n",
      "VHT_Return_1d\n",
      "VHT_Return_5d\n",
      "VHT_Return_10d\n",
      "VHT_Return_20d\n",
      "VHT_SMA_10\n",
      "VHT_SMA_50\n",
      "VHT_EMA_20\n",
      "VHT_EMA_50\n",
      "VHT_MA_Cross\n",
      "VHT_MACD\n",
      "VHT_MACD_sig\n",
      "VHT_MACD_hist\n",
      "VHT_RSI_14\n",
      "VHT_StochK\n",
      "VHT_StochD\n",
      "VHT_Vol_20\n",
      "VHT_Parkinson_20\n",
      "VHT_GK_20\n",
      "VHT_BB_width\n",
      "VHT_Volume_ROC\n",
      "VHT_Volume_Z\n",
      "VHT_OBV\n",
      "VHT_Entropy_20\n",
      "VHT_HMM\n",
      "VIXY_Return_1d\n",
      "VIXY_Return_5d\n",
      "VIXY_Return_10d\n",
      "VIXY_Return_20d\n",
      "VIXY_SMA_10\n",
      "VIXY_SMA_50\n",
      "VIXY_EMA_20\n",
      "VIXY_EMA_50\n",
      "VIXY_MA_Cross\n",
      "VIXY_MACD\n",
      "VIXY_MACD_sig\n",
      "VIXY_MACD_hist\n",
      "VIXY_RSI_14\n",
      "VIXY_StochK\n",
      "VIXY_StochD\n",
      "VIXY_Vol_20\n",
      "VIXY_Parkinson_20\n",
      "VIXY_GK_20\n",
      "VIXY_BB_width\n",
      "VIXY_Volume_ROC\n",
      "VIXY_Volume_Z\n",
      "VIXY_OBV\n",
      "VIXY_Entropy_20\n",
      "VIXY_HMM\n",
      "XBI_Return_1d\n",
      "XBI_Return_5d\n",
      "XBI_Return_10d\n",
      "XBI_Return_20d\n",
      "XBI_SMA_10\n",
      "XBI_SMA_50\n",
      "XBI_EMA_20\n",
      "XBI_EMA_50\n",
      "XBI_MA_Cross\n",
      "XBI_MACD\n",
      "XBI_MACD_sig\n",
      "XBI_MACD_hist\n",
      "XBI_RSI_14\n",
      "XBI_StochK\n",
      "XBI_StochD\n",
      "XBI_Vol_20\n",
      "XBI_Parkinson_20\n",
      "XBI_GK_20\n",
      "XBI_BB_width\n",
      "XBI_Volume_ROC\n",
      "XBI_Volume_Z\n",
      "XBI_OBV\n",
      "XBI_Entropy_20\n",
      "XBI_HMM\n",
      "XLV_Return_1d\n",
      "XLV_Return_5d\n",
      "XLV_Return_10d\n",
      "XLV_Return_20d\n",
      "XLV_SMA_10\n",
      "XLV_SMA_50\n",
      "XLV_EMA_20\n",
      "XLV_EMA_50\n",
      "XLV_MA_Cross\n",
      "XLV_MACD\n",
      "XLV_MACD_sig\n",
      "XLV_MACD_hist\n",
      "XLV_RSI_14\n",
      "XLV_StochK\n",
      "XLV_StochD\n",
      "XLV_Vol_20\n",
      "XLV_Parkinson_20\n",
      "XLV_GK_20\n",
      "XLV_BB_width\n",
      "XLV_Volume_ROC\n",
      "XLV_Volume_Z\n",
      "XLV_OBV\n",
      "XLV_Entropy_20\n",
      "XLV_HMM\n",
      "XPH_Return_1d\n",
      "XPH_Return_5d\n",
      "XPH_Return_10d\n",
      "XPH_Return_20d\n",
      "XPH_SMA_10\n",
      "XPH_SMA_50\n",
      "XPH_EMA_20\n",
      "XPH_EMA_50\n",
      "XPH_MA_Cross\n",
      "XPH_MACD\n",
      "XPH_MACD_sig\n",
      "XPH_MACD_hist\n",
      "XPH_RSI_14\n",
      "XPH_StochK\n",
      "XPH_StochD\n",
      "XPH_Vol_20\n",
      "XPH_Parkinson_20\n",
      "XPH_GK_20\n",
      "XPH_BB_width\n",
      "XPH_Volume_ROC\n",
      "XPH_Volume_Z\n",
      "XPH_OBV\n",
      "XPH_Entropy_20\n",
      "XPH_HMM\n",
      "IBB_Ratio_PPH\n",
      "IBB_RS_20\n",
      "IBB_Corr_PPH_60\n",
      "IHE_Ratio_PPH\n",
      "IHE_RS_20\n",
      "IHE_Corr_PPH_60\n",
      "PPH_Ratio_PPH\n",
      "PPH_RS_20\n",
      "PPH_Corr_PPH_60\n",
      "SPY_Ratio_PPH\n",
      "SPY_RS_20\n",
      "SPY_Corr_PPH_60\n",
      "VHT_Ratio_PPH\n",
      "VHT_RS_20\n",
      "VHT_Corr_PPH_60\n",
      "VIXY_Ratio_PPH\n",
      "VIXY_RS_20\n",
      "VIXY_Corr_PPH_60\n",
      "XBI_Ratio_PPH\n",
      "XBI_RS_20\n",
      "XBI_Corr_PPH_60\n",
      "XLV_Ratio_PPH\n",
      "XLV_RS_20\n",
      "XLV_Corr_PPH_60\n",
      "XPH_Ratio_PPH\n",
      "XPH_RS_20\n",
      "XPH_Corr_PPH_60\n",
      "PCA_1\n",
      "PCA_2\n",
      "PCA_3\n",
      "IBB_Return_1d\n",
      "IBB_Return_5d\n",
      "IBB_Return_10d\n",
      "IBB_Return_20d\n",
      "IBB_SMA_10\n",
      "IBB_SMA_50\n",
      "IBB_EMA_20\n",
      "IBB_EMA_50\n",
      "IBB_MA_Cross\n",
      "IBB_MACD\n",
      "IBB_MACD_sig\n",
      "IBB_MACD_hist\n",
      "IBB_RSI_14\n",
      "IBB_StochK\n",
      "IBB_StochD\n",
      "IBB_Vol_20\n",
      "IBB_Parkinson_20\n",
      "IBB_GK_20\n",
      "IBB_BB_width\n",
      "IBB_Volume_ROC\n",
      "IBB_Volume_Z\n",
      "IBB_OBV\n",
      "IBB_Entropy_20\n",
      "IBB_HMM\n",
      "IHE_Return_1d\n",
      "IHE_Return_5d\n",
      "IHE_Return_10d\n",
      "IHE_Return_20d\n",
      "IHE_SMA_10\n",
      "IHE_SMA_50\n",
      "IHE_EMA_20\n",
      "IHE_EMA_50\n",
      "IHE_MA_Cross\n",
      "IHE_MACD\n",
      "IHE_MACD_sig\n",
      "IHE_MACD_hist\n",
      "IHE_RSI_14\n",
      "IHE_StochK\n",
      "IHE_StochD\n",
      "IHE_Vol_20\n",
      "IHE_Parkinson_20\n",
      "IHE_GK_20\n",
      "IHE_BB_width\n",
      "IHE_Volume_ROC\n",
      "IHE_Volume_Z\n",
      "IHE_OBV\n",
      "IHE_Entropy_20\n",
      "IHE_HMM\n",
      "PPH_Return_1d\n",
      "PPH_Return_5d\n",
      "PPH_Return_10d\n",
      "PPH_Return_20d\n",
      "PPH_SMA_10\n",
      "PPH_SMA_50\n",
      "PPH_EMA_20\n",
      "PPH_EMA_50\n",
      "PPH_MA_Cross\n",
      "PPH_MACD\n",
      "PPH_MACD_sig\n",
      "PPH_MACD_hist\n",
      "PPH_RSI_14\n",
      "PPH_StochK\n",
      "PPH_StochD\n",
      "PPH_Vol_20\n",
      "PPH_Parkinson_20\n",
      "PPH_GK_20\n",
      "PPH_BB_width\n",
      "PPH_Volume_ROC\n",
      "PPH_Volume_Z\n",
      "PPH_OBV\n",
      "PPH_Entropy_20\n",
      "PPH_HMM\n",
      "SPY_Return_1d\n",
      "SPY_Return_5d\n",
      "SPY_Return_10d\n",
      "SPY_Return_20d\n",
      "SPY_SMA_10\n",
      "SPY_SMA_50\n",
      "SPY_EMA_20\n",
      "SPY_EMA_50\n",
      "SPY_MA_Cross\n",
      "SPY_MACD\n",
      "SPY_MACD_sig\n",
      "SPY_MACD_hist\n",
      "SPY_RSI_14\n",
      "SPY_StochK\n",
      "SPY_StochD\n",
      "SPY_Vol_20\n",
      "SPY_Parkinson_20\n",
      "SPY_GK_20\n",
      "SPY_BB_width\n",
      "SPY_Volume_ROC\n",
      "SPY_Volume_Z\n",
      "SPY_OBV\n",
      "SPY_Entropy_20\n",
      "SPY_HMM\n",
      "VHT_Return_1d\n",
      "VHT_Return_5d\n",
      "VHT_Return_10d\n",
      "VHT_Return_20d\n",
      "VHT_SMA_10\n",
      "VHT_SMA_50\n",
      "VHT_EMA_20\n",
      "VHT_EMA_50\n",
      "VHT_MA_Cross\n",
      "VHT_MACD\n",
      "VHT_MACD_sig\n",
      "VHT_MACD_hist\n",
      "VHT_RSI_14\n",
      "VHT_StochK\n",
      "VHT_StochD\n",
      "VHT_Vol_20\n",
      "VHT_Parkinson_20\n",
      "VHT_GK_20\n",
      "VHT_BB_width\n",
      "VHT_Volume_ROC\n",
      "VHT_Volume_Z\n",
      "VHT_OBV\n",
      "VHT_Entropy_20\n",
      "VHT_HMM\n",
      "VIXY_Return_1d\n",
      "VIXY_Return_5d\n",
      "VIXY_Return_10d\n",
      "VIXY_Return_20d\n",
      "VIXY_SMA_10\n",
      "VIXY_SMA_50\n",
      "VIXY_EMA_20\n",
      "VIXY_EMA_50\n",
      "VIXY_MA_Cross\n",
      "VIXY_MACD\n",
      "VIXY_MACD_sig\n",
      "VIXY_MACD_hist\n",
      "VIXY_RSI_14\n",
      "VIXY_StochK\n",
      "VIXY_StochD\n",
      "VIXY_Vol_20\n",
      "VIXY_Parkinson_20\n",
      "VIXY_GK_20\n",
      "VIXY_BB_width\n",
      "VIXY_Volume_ROC\n",
      "VIXY_Volume_Z\n",
      "VIXY_OBV\n",
      "VIXY_Entropy_20\n",
      "VIXY_HMM\n",
      "XBI_Return_1d\n",
      "XBI_Return_5d\n",
      "XBI_Return_10d\n",
      "XBI_Return_20d\n",
      "XBI_SMA_10\n",
      "XBI_SMA_50\n",
      "XBI_EMA_20\n",
      "XBI_EMA_50\n",
      "XBI_MA_Cross\n",
      "XBI_MACD\n",
      "XBI_MACD_sig\n",
      "XBI_MACD_hist\n",
      "XBI_RSI_14\n",
      "XBI_StochK\n",
      "XBI_StochD\n",
      "XBI_Vol_20\n",
      "XBI_Parkinson_20\n",
      "XBI_GK_20\n",
      "XBI_BB_width\n",
      "XBI_Volume_ROC\n",
      "XBI_Volume_Z\n",
      "XBI_OBV\n",
      "XBI_Entropy_20\n",
      "XBI_HMM\n",
      "XLV_Return_1d\n",
      "XLV_Return_5d\n",
      "XLV_Return_10d\n",
      "XLV_Return_20d\n",
      "XLV_SMA_10\n",
      "XLV_SMA_50\n",
      "XLV_EMA_20\n",
      "XLV_EMA_50\n",
      "XLV_MA_Cross\n",
      "XLV_MACD\n",
      "XLV_MACD_sig\n",
      "XLV_MACD_hist\n",
      "XLV_RSI_14\n",
      "XLV_StochK\n",
      "XLV_StochD\n",
      "XLV_Vol_20\n",
      "XLV_Parkinson_20\n",
      "XLV_GK_20\n",
      "XLV_BB_width\n",
      "XLV_Volume_ROC\n",
      "XLV_Volume_Z\n",
      "XLV_OBV\n",
      "XLV_Entropy_20\n",
      "XLV_HMM\n",
      "XPH_Return_1d\n",
      "XPH_Return_5d\n",
      "XPH_Return_10d\n",
      "XPH_Return_20d\n",
      "XPH_SMA_10\n",
      "XPH_SMA_50\n",
      "XPH_EMA_20\n",
      "XPH_EMA_50\n",
      "XPH_MA_Cross\n",
      "XPH_MACD\n",
      "XPH_MACD_sig\n",
      "XPH_MACD_hist\n",
      "XPH_RSI_14\n",
      "XPH_StochK\n",
      "XPH_StochD\n",
      "XPH_Vol_20\n",
      "XPH_Parkinson_20\n",
      "XPH_GK_20\n",
      "XPH_BB_width\n",
      "XPH_Volume_ROC\n",
      "XPH_Volume_Z\n",
      "XPH_OBV\n",
      "XPH_Entropy_20\n",
      "XPH_HMM\n",
      "IBB_Ratio_PPH\n",
      "IBB_RS_20\n",
      "IBB_Corr_PPH_60\n",
      "IHE_Ratio_PPH\n",
      "IHE_RS_20\n",
      "IHE_Corr_PPH_60\n",
      "PPH_Ratio_PPH\n",
      "PPH_RS_20\n",
      "PPH_Corr_PPH_60\n",
      "SPY_Ratio_PPH\n",
      "SPY_RS_20\n",
      "SPY_Corr_PPH_60\n",
      "VHT_Ratio_PPH\n",
      "VHT_RS_20\n",
      "VHT_Corr_PPH_60\n",
      "VIXY_Ratio_PPH\n",
      "VIXY_RS_20\n",
      "VIXY_Corr_PPH_60\n",
      "XBI_Ratio_PPH\n",
      "XBI_RS_20\n",
      "XBI_Corr_PPH_60\n",
      "XLV_Ratio_PPH\n",
      "XLV_RS_20\n",
      "XLV_Corr_PPH_60\n",
      "XPH_Ratio_PPH\n",
      "XPH_RS_20\n",
      "XPH_Corr_PPH_60\n",
      "PCA_1\n",
      "PCA_2\n",
      "PCA_3\n",
      "day_of_week\n",
      "day_of_month\n",
      "month\n",
      "quarter\n",
      "is_month_end\n",
      "is_month_start\n",
      "is_year_end\n",
      "is_holiday_adjacent\n",
      "is_opex_week\n",
      "is_fomc_week\n",
      "is_cpi_day\n",
      "is_cpi_week\n",
      "is_nfp_day\n",
      "is_nfp_week\n",
      "is_ppi_day\n",
      "is_ppi_week\n",
      "is_gdp_day\n",
      "is_gdp_week\n"
     ]
    }
   ],
   "source": [
    "for c in data.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9d0e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_missing_features(df):\n",
    "    \"\"\"\n",
    "    Adds missing high-value, non-leaking features.\n",
    "    Automatically deduplicates the DataFrame before adding anything.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 0. Deduplicate columns FIRST to avoid \"multiple columns returned\" bugs\n",
    "    # ----------------------------------------------------------------------\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep=\"last\")]\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 1. Rolling volatility / trend / autocorrelation for PPH\n",
    "    # ----------------------------------------------------------------------\n",
    "    close = df[\"PPH_Close\"]\n",
    "    ret = close.pct_change()\n",
    "\n",
    "    df[\"PPH_vol_20\"] = ret.rolling(20).std()\n",
    "    df[\"PPH_vol_60\"] = ret.rolling(60).std()\n",
    "    df[\"PPH_vol_120\"] = ret.rolling(120).std()\n",
    "\n",
    "    df[\"PPH_trend_20\"]  = (close - close.shift(20))  / 20\n",
    "    df[\"PPH_trend_60\"]  = (close - close.shift(60))  / 60\n",
    "    df[\"PPH_trend_120\"] = (close - close.shift(120)) / 120\n",
    "\n",
    "    df[\"PPH_autocorr_20\"] = ret.rolling(20).apply(lambda x: pd.Series(x).autocorr(), raw=False)\n",
    "    df[\"PPH_autocorr_60\"] = ret.rolling(60).apply(lambda x: pd.Series(x).autocorr(), raw=False)\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2. Drawdowns (no leakage: uses only past rolling window)\n",
    "    # ----------------------------------------------------------------------\n",
    "    df[\"PPH_drawdown_60\"] = (close / close.rolling(60).max()) - 1\n",
    "    df[\"PPH_max_drawdown_120\"] = close.rolling(120).apply(\n",
    "        lambda x: (x / x.cummax() - 1).min(),\n",
    "        raw=False\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 3. Lagged cross-asset features (safe because we shift forward in time)\n",
    "    # ----------------------------------------------------------------------\n",
    "    assets = [\"SPY\", \"VIXY\", \"XLV\", \"XBI\", \"IBB\"]\n",
    "    lags = [1, 2, 3]\n",
    "\n",
    "    for a in assets:\n",
    "\n",
    "        # These lines will now ALWAYS return Series (never DataFrames)\n",
    "        ret_series = df[f\"{a}_Return_1d\"]\n",
    "        vol_series = df[f\"{a}_Vol_20\"]\n",
    "\n",
    "        for lag in lags:\n",
    "            df[f\"{a}_Return_1d_lag{lag}\"] = ret_series.shift(lag)\n",
    "            df[f\"{a}_Vol_20_lag{lag}\"] = vol_series.shift(lag)\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 4. Regime labels\n",
    "    # ----------------------------------------------------------------------\n",
    "    vol20 = df[\"PPH_vol_20\"]\n",
    "    df[\"PPH_vol_zscore_60\"] = (vol20 - vol20.rolling(60).mean()) / vol20.rolling(60).std()\n",
    "\n",
    "    ema20 = df[\"PPH_EMA_20\"]\n",
    "    ema50 = df[\"PPH_EMA_50\"]\n",
    "    diff  = ema20 - ema50\n",
    "\n",
    "    threshold = diff.rolling(60).std() * 0.25\n",
    "\n",
    "    df[\"PPH_trend_regime\"] = np.where(\n",
    "        diff > threshold, 1,\n",
    "        np.where(diff < -threshold, -1, 0)\n",
    "    )\n",
    "\n",
    "    vol = df[\"PPH_Volume\"]\n",
    "    df[\"PPH_volume_zscore_60\"] = (vol - vol.rolling(60).mean()) / vol.rolling(60).std()\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 5. Deduplicate again in case new names collided\n",
    "    # ----------------------------------------------------------------------\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep=\"first\")]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8f47bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_missing_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a6613069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'VIXY_Open',\n",
       " 'VIXY_High',\n",
       " 'VIXY_Low',\n",
       " 'VIXY_Close',\n",
       " 'VIXY_Volume',\n",
       " 'XPH_Open',\n",
       " 'XPH_High',\n",
       " 'XPH_Low',\n",
       " 'XPH_Close',\n",
       " 'XPH_Volume',\n",
       " 'VHT_Open',\n",
       " 'VHT_High',\n",
       " 'VHT_Low',\n",
       " 'VHT_Close',\n",
       " 'VHT_Volume',\n",
       " 'XLV_Open',\n",
       " 'XLV_High',\n",
       " 'XLV_Low',\n",
       " 'XLV_Close',\n",
       " 'XLV_Volume',\n",
       " 'PPH_Open',\n",
       " 'PPH_High',\n",
       " 'PPH_Low',\n",
       " 'PPH_Close',\n",
       " 'PPH_Volume',\n",
       " 'SPY_Open',\n",
       " 'SPY_High',\n",
       " 'SPY_Low',\n",
       " 'SPY_Close',\n",
       " 'SPY_Volume',\n",
       " 'XBI_Open',\n",
       " 'XBI_High',\n",
       " 'XBI_Low',\n",
       " 'XBI_Close',\n",
       " 'XBI_Volume',\n",
       " 'IBB_Open',\n",
       " 'IBB_High',\n",
       " 'IBB_Low',\n",
       " 'IBB_Close',\n",
       " 'IBB_Volume',\n",
       " 'IHE_Open',\n",
       " 'IHE_High',\n",
       " 'IHE_Low',\n",
       " 'IHE_Close',\n",
       " 'IHE_Volume',\n",
       " 'IBB_Return_1d',\n",
       " 'IBB_Return_5d',\n",
       " 'IBB_Return_10d',\n",
       " 'IBB_Return_20d',\n",
       " 'IBB_SMA_10',\n",
       " 'IBB_SMA_50',\n",
       " 'IBB_EMA_20',\n",
       " 'IBB_EMA_50',\n",
       " 'IBB_MA_Cross',\n",
       " 'IBB_MACD',\n",
       " 'IBB_MACD_sig',\n",
       " 'IBB_MACD_hist',\n",
       " 'IBB_RSI_14',\n",
       " 'IBB_StochK',\n",
       " 'IBB_StochD',\n",
       " 'IBB_Vol_20',\n",
       " 'IBB_Parkinson_20',\n",
       " 'IBB_GK_20',\n",
       " 'IBB_BB_width',\n",
       " 'IBB_Volume_ROC',\n",
       " 'IBB_Volume_Z',\n",
       " 'IBB_OBV',\n",
       " 'IBB_Entropy_20',\n",
       " 'IBB_HMM',\n",
       " 'IHE_Return_1d',\n",
       " 'IHE_Return_5d',\n",
       " 'IHE_Return_10d',\n",
       " 'IHE_Return_20d',\n",
       " 'IHE_SMA_10',\n",
       " 'IHE_SMA_50',\n",
       " 'IHE_EMA_20',\n",
       " 'IHE_EMA_50',\n",
       " 'IHE_MA_Cross',\n",
       " 'IHE_MACD',\n",
       " 'IHE_MACD_sig',\n",
       " 'IHE_MACD_hist',\n",
       " 'IHE_RSI_14',\n",
       " 'IHE_StochK',\n",
       " 'IHE_StochD',\n",
       " 'IHE_Vol_20',\n",
       " 'IHE_Parkinson_20',\n",
       " 'IHE_GK_20',\n",
       " 'IHE_BB_width',\n",
       " 'IHE_Volume_ROC',\n",
       " 'IHE_Volume_Z',\n",
       " 'IHE_OBV',\n",
       " 'IHE_Entropy_20',\n",
       " 'IHE_HMM',\n",
       " 'PPH_Return_1d',\n",
       " 'PPH_Return_5d',\n",
       " 'PPH_Return_10d',\n",
       " 'PPH_Return_20d',\n",
       " 'PPH_SMA_10',\n",
       " 'PPH_SMA_50',\n",
       " 'PPH_EMA_20',\n",
       " 'PPH_EMA_50',\n",
       " 'PPH_MA_Cross',\n",
       " 'PPH_MACD',\n",
       " 'PPH_MACD_sig',\n",
       " 'PPH_MACD_hist',\n",
       " 'PPH_RSI_14',\n",
       " 'PPH_StochK',\n",
       " 'PPH_StochD',\n",
       " 'PPH_Vol_20',\n",
       " 'PPH_Parkinson_20',\n",
       " 'PPH_GK_20',\n",
       " 'PPH_BB_width',\n",
       " 'PPH_Volume_ROC',\n",
       " 'PPH_Volume_Z',\n",
       " 'PPH_OBV',\n",
       " 'PPH_Entropy_20',\n",
       " 'PPH_HMM',\n",
       " 'SPY_Return_1d',\n",
       " 'SPY_Return_5d',\n",
       " 'SPY_Return_10d',\n",
       " 'SPY_Return_20d',\n",
       " 'SPY_SMA_10',\n",
       " 'SPY_SMA_50',\n",
       " 'SPY_EMA_20',\n",
       " 'SPY_EMA_50',\n",
       " 'SPY_MA_Cross',\n",
       " 'SPY_MACD',\n",
       " 'SPY_MACD_sig',\n",
       " 'SPY_MACD_hist',\n",
       " 'SPY_RSI_14',\n",
       " 'SPY_StochK',\n",
       " 'SPY_StochD',\n",
       " 'SPY_Vol_20',\n",
       " 'SPY_Parkinson_20',\n",
       " 'SPY_GK_20',\n",
       " 'SPY_BB_width',\n",
       " 'SPY_Volume_ROC',\n",
       " 'SPY_Volume_Z',\n",
       " 'SPY_OBV',\n",
       " 'SPY_Entropy_20',\n",
       " 'SPY_HMM',\n",
       " 'VHT_Return_1d',\n",
       " 'VHT_Return_5d',\n",
       " 'VHT_Return_10d',\n",
       " 'VHT_Return_20d',\n",
       " 'VHT_SMA_10',\n",
       " 'VHT_SMA_50',\n",
       " 'VHT_EMA_20',\n",
       " 'VHT_EMA_50',\n",
       " 'VHT_MA_Cross',\n",
       " 'VHT_MACD',\n",
       " 'VHT_MACD_sig',\n",
       " 'VHT_MACD_hist',\n",
       " 'VHT_RSI_14',\n",
       " 'VHT_StochK',\n",
       " 'VHT_StochD',\n",
       " 'VHT_Vol_20',\n",
       " 'VHT_Parkinson_20',\n",
       " 'VHT_GK_20',\n",
       " 'VHT_BB_width',\n",
       " 'VHT_Volume_ROC',\n",
       " 'VHT_Volume_Z',\n",
       " 'VHT_OBV',\n",
       " 'VHT_Entropy_20',\n",
       " 'VHT_HMM',\n",
       " 'VIXY_Return_1d',\n",
       " 'VIXY_Return_5d',\n",
       " 'VIXY_Return_10d',\n",
       " 'VIXY_Return_20d',\n",
       " 'VIXY_SMA_10',\n",
       " 'VIXY_SMA_50',\n",
       " 'VIXY_EMA_20',\n",
       " 'VIXY_EMA_50',\n",
       " 'VIXY_MA_Cross',\n",
       " 'VIXY_MACD',\n",
       " 'VIXY_MACD_sig',\n",
       " 'VIXY_MACD_hist',\n",
       " 'VIXY_RSI_14',\n",
       " 'VIXY_StochK',\n",
       " 'VIXY_StochD',\n",
       " 'VIXY_Vol_20',\n",
       " 'VIXY_Parkinson_20',\n",
       " 'VIXY_GK_20',\n",
       " 'VIXY_BB_width',\n",
       " 'VIXY_Volume_ROC',\n",
       " 'VIXY_Volume_Z',\n",
       " 'VIXY_OBV',\n",
       " 'VIXY_Entropy_20',\n",
       " 'VIXY_HMM',\n",
       " 'XBI_Return_1d',\n",
       " 'XBI_Return_5d',\n",
       " 'XBI_Return_10d',\n",
       " 'XBI_Return_20d',\n",
       " 'XBI_SMA_10',\n",
       " 'XBI_SMA_50',\n",
       " 'XBI_EMA_20',\n",
       " 'XBI_EMA_50',\n",
       " 'XBI_MA_Cross',\n",
       " 'XBI_MACD',\n",
       " 'XBI_MACD_sig',\n",
       " 'XBI_MACD_hist',\n",
       " 'XBI_RSI_14',\n",
       " 'XBI_StochK',\n",
       " 'XBI_StochD',\n",
       " 'XBI_Vol_20',\n",
       " 'XBI_Parkinson_20',\n",
       " 'XBI_GK_20',\n",
       " 'XBI_BB_width',\n",
       " 'XBI_Volume_ROC',\n",
       " 'XBI_Volume_Z',\n",
       " 'XBI_OBV',\n",
       " 'XBI_Entropy_20',\n",
       " 'XBI_HMM',\n",
       " 'XLV_Return_1d',\n",
       " 'XLV_Return_5d',\n",
       " 'XLV_Return_10d',\n",
       " 'XLV_Return_20d',\n",
       " 'XLV_SMA_10',\n",
       " 'XLV_SMA_50',\n",
       " 'XLV_EMA_20',\n",
       " 'XLV_EMA_50',\n",
       " 'XLV_MA_Cross',\n",
       " 'XLV_MACD',\n",
       " 'XLV_MACD_sig',\n",
       " 'XLV_MACD_hist',\n",
       " 'XLV_RSI_14',\n",
       " 'XLV_StochK',\n",
       " 'XLV_StochD',\n",
       " 'XLV_Vol_20',\n",
       " 'XLV_Parkinson_20',\n",
       " 'XLV_GK_20',\n",
       " 'XLV_BB_width',\n",
       " 'XLV_Volume_ROC',\n",
       " 'XLV_Volume_Z',\n",
       " 'XLV_OBV',\n",
       " 'XLV_Entropy_20',\n",
       " 'XLV_HMM',\n",
       " 'XPH_Return_1d',\n",
       " 'XPH_Return_5d',\n",
       " 'XPH_Return_10d',\n",
       " 'XPH_Return_20d',\n",
       " 'XPH_SMA_10',\n",
       " 'XPH_SMA_50',\n",
       " 'XPH_EMA_20',\n",
       " 'XPH_EMA_50',\n",
       " 'XPH_MA_Cross',\n",
       " 'XPH_MACD',\n",
       " 'XPH_MACD_sig',\n",
       " 'XPH_MACD_hist',\n",
       " 'XPH_RSI_14',\n",
       " 'XPH_StochK',\n",
       " 'XPH_StochD',\n",
       " 'XPH_Vol_20',\n",
       " 'XPH_Parkinson_20',\n",
       " 'XPH_GK_20',\n",
       " 'XPH_BB_width',\n",
       " 'XPH_Volume_ROC',\n",
       " 'XPH_Volume_Z',\n",
       " 'XPH_OBV',\n",
       " 'XPH_Entropy_20',\n",
       " 'XPH_HMM',\n",
       " 'IBB_Ratio_PPH',\n",
       " 'IBB_RS_20',\n",
       " 'IBB_Corr_PPH_60',\n",
       " 'IHE_Ratio_PPH',\n",
       " 'IHE_RS_20',\n",
       " 'IHE_Corr_PPH_60',\n",
       " 'PPH_Ratio_PPH',\n",
       " 'PPH_RS_20',\n",
       " 'PPH_Corr_PPH_60',\n",
       " 'SPY_Ratio_PPH',\n",
       " 'SPY_RS_20',\n",
       " 'SPY_Corr_PPH_60',\n",
       " 'VHT_Ratio_PPH',\n",
       " 'VHT_RS_20',\n",
       " 'VHT_Corr_PPH_60',\n",
       " 'VIXY_Ratio_PPH',\n",
       " 'VIXY_RS_20',\n",
       " 'VIXY_Corr_PPH_60',\n",
       " 'XBI_Ratio_PPH',\n",
       " 'XBI_RS_20',\n",
       " 'XBI_Corr_PPH_60',\n",
       " 'XLV_Ratio_PPH',\n",
       " 'XLV_RS_20',\n",
       " 'XLV_Corr_PPH_60',\n",
       " 'XPH_Ratio_PPH',\n",
       " 'XPH_RS_20',\n",
       " 'XPH_Corr_PPH_60',\n",
       " 'PCA_1',\n",
       " 'PCA_2',\n",
       " 'PCA_3',\n",
       " 'day_of_week',\n",
       " 'day_of_month',\n",
       " 'month',\n",
       " 'quarter',\n",
       " 'is_month_end',\n",
       " 'is_month_start',\n",
       " 'is_year_end',\n",
       " 'is_holiday_adjacent',\n",
       " 'is_opex_week',\n",
       " 'is_fomc_week',\n",
       " 'is_cpi_day',\n",
       " 'is_cpi_week',\n",
       " 'is_nfp_day',\n",
       " 'is_nfp_week',\n",
       " 'is_ppi_day',\n",
       " 'is_ppi_week',\n",
       " 'is_gdp_day',\n",
       " 'is_gdp_week',\n",
       " 'PPH_vol_20',\n",
       " 'PPH_vol_60',\n",
       " 'PPH_vol_120',\n",
       " 'PPH_trend_20',\n",
       " 'PPH_trend_60',\n",
       " 'PPH_trend_120',\n",
       " 'PPH_autocorr_20',\n",
       " 'PPH_autocorr_60',\n",
       " 'PPH_drawdown_60',\n",
       " 'PPH_max_drawdown_120',\n",
       " 'SPY_Return_1d_lag1',\n",
       " 'SPY_Vol_20_lag1',\n",
       " 'SPY_Return_1d_lag2',\n",
       " 'SPY_Vol_20_lag2',\n",
       " 'SPY_Return_1d_lag3',\n",
       " 'SPY_Vol_20_lag3',\n",
       " 'VIXY_Return_1d_lag1',\n",
       " 'VIXY_Vol_20_lag1',\n",
       " 'VIXY_Return_1d_lag2',\n",
       " 'VIXY_Vol_20_lag2',\n",
       " 'VIXY_Return_1d_lag3',\n",
       " 'VIXY_Vol_20_lag3',\n",
       " 'XLV_Return_1d_lag1',\n",
       " 'XLV_Vol_20_lag1',\n",
       " 'XLV_Return_1d_lag2',\n",
       " 'XLV_Vol_20_lag2',\n",
       " 'XLV_Return_1d_lag3',\n",
       " 'XLV_Vol_20_lag3',\n",
       " 'XBI_Return_1d_lag1',\n",
       " 'XBI_Vol_20_lag1',\n",
       " 'XBI_Return_1d_lag2',\n",
       " 'XBI_Vol_20_lag2',\n",
       " 'XBI_Return_1d_lag3',\n",
       " 'XBI_Vol_20_lag3',\n",
       " 'IBB_Return_1d_lag1',\n",
       " 'IBB_Vol_20_lag1',\n",
       " 'IBB_Return_1d_lag2',\n",
       " 'IBB_Vol_20_lag2',\n",
       " 'IBB_Return_1d_lag3',\n",
       " 'IBB_Vol_20_lag3',\n",
       " 'PPH_vol_zscore_60',\n",
       " 'PPH_trend_regime',\n",
       " 'PPH_volume_zscore_60']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32febd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3749, 353)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3731b357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n✔ SHIFT(1) correctly maps f → f(t-1)\\n✔ Calendar features create f(t+k) for k up to horizon\\n✔ Target remains untouched\\n✔ Already-lagged features remain untouched\\n✔ Open features remain unshifted\\n✔ Original columns that should be replaced are dropped\\n✔ Defensive: skip missing columns safely\\n✔ Uses clean functions for clarity\\n✔ Fully commented\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7fca0a04",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'feature_transformations.md'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m horizon = \u001b[32m30\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_transformed = \u001b[43mapply_all_feature_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature_transformations.md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mapply_all_feature_transformations\u001b[39m\u001b[34m(df, feature_table_path, horizon)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_all_feature_transformations\u001b[39m(df, feature_table_path=\u001b[33m\"\u001b[39m\u001b[33mfeature_transformations.md\u001b[39m\u001b[33m\"\u001b[39m, horizon=\u001b[32m63\u001b[39m):\n\u001b[32m    125\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m    Main pipeline:\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m    - Loads transformation rules\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    130\u001b[39m \u001b[33;03m    - Maintains target untouched\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     transform_df = \u001b[43mload_feature_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_table_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m     \u001b[38;5;66;03m# Apply transformations\u001b[39;00m\n\u001b[32m    135\u001b[39m     updated = update_dataframe_with_transformations(df, transform_df, horizon)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mload_feature_table\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03mLoads the markdown table produced earlier into a clean DataFrame.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Pandas can read markdown via read_table with the proper sep\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Skip markdown header lines\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskipinitialspace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Clean column names (markdown artifacts)\u001b[39;00m\n\u001b[32m     22\u001b[39m df.columns = [c.strip() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/Trend-Surgeon/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1405\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1392\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1393\u001b[39m     dialect,\n\u001b[32m   1394\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1401\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1402\u001b[39m )\n\u001b[32m   1403\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/Trend-Surgeon/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/Trend-Surgeon/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/Trend-Surgeon/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/Trend-Surgeon/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'feature_transformations.md'"
     ]
    }
   ],
   "source": [
    "horizon = 30\n",
    "df_transformed = apply_all_feature_transformations(data, \"feature_transformations.md\", horizon=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94432e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ============================================================\n",
    "# 1. LOAD TRANSFORMATION TABLE FROM MARKDOWN\n",
    "# ============================================================\n",
    "\n",
    "def load_feature_rules(path=\"feature_transformations.md\"):\n",
    "    \"\"\"\n",
    "    Loads the markdown table you generated earlier and converts it into\n",
    "    a clean dict: rules[feature] = {\"transformation\":..., \"new_name\":...}\n",
    "    \"\"\"\n",
    "    # Read markdown table (pipes)\n",
    "    df = pd.read_table(\n",
    "        path,\n",
    "        sep=\"|\",\n",
    "        engine=\"python\",\n",
    "        skiprows=2,          # Skip the markdown header separator lines\n",
    "        skipinitialspace=True\n",
    "    )\n",
    "\n",
    "    # Clean up junk markdown columns\n",
    "    df = df.drop(columns=[\"\"], errors=\"ignore\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    rules = {}\n",
    "    for _, row in df.iterrows():\n",
    "        feature = row[\"feature\"]\n",
    "        transformation = row[\"transformation\"]\n",
    "        new_name = row[\"new_name\"]\n",
    "\n",
    "        rules[feature] = {\n",
    "            \"transformation\": transformation,\n",
    "            \"new_name\": new_name\n",
    "        }\n",
    "\n",
    "    return rules\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. APPLY TRANSFORMATIONS USING THE RULE DICT\n",
    "# ============================================================\n",
    "\n",
    "def apply_rules_to_dataframe(df, rules, horizon=30):\n",
    "    \"\"\"\n",
    "    Applies the transformation rules exactly as specified in the markdown table.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    to_drop = []\n",
    "\n",
    "    for col in df.columns:\n",
    "\n",
    "        if col not in rules:\n",
    "            # If the column was not in the markdown table, we leave it unchanged\n",
    "            continue\n",
    "\n",
    "        rule = rules[col]\n",
    "        trans = rule[\"transformation\"]\n",
    "        new_name = rule[\"new_name\"]\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # TARGET — leave untouched\n",
    "        # ----------------------------------------------------\n",
    "        if trans == \"target\":\n",
    "            continue\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # NO SHIFT\n",
    "        # ----------------------------------------------------\n",
    "        if trans == \"no shift\":\n",
    "            if new_name != col:\n",
    "                df[new_name] = df[col]\n",
    "                to_drop.append(col)\n",
    "            continue\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # SHIFT(1)\n",
    "        # ----------------------------------------------------\n",
    "        if trans == \"shift(1)\" or trans == \"shift(1)\":\n",
    "            df[new_name] = df[col].shift(1)\n",
    "            to_drop.append(col)\n",
    "            continue\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # CALENDAR FUTURE HORIZON RULE\n",
    "        # ----------------------------------------------------\n",
    "        if \"future horizon rule\" in trans:\n",
    "            # base name (col(t))\n",
    "            df[new_name] = df[col]\n",
    "\n",
    "            # generate deterministic future versions\n",
    "            for k in range(1, horizon + 1):\n",
    "                df[f\"{col}(t+{k})\"] = df[col].shift(-k)\n",
    "\n",
    "            to_drop.append(col)\n",
    "            continue\n",
    "\n",
    "    # Drop original columns that were replaced\n",
    "    df = df.drop(columns=[c for c in to_drop if c in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. FULL PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "def apply_all_feature_transformations(df, feature_table_path=\"feature_transformations.md\", horizon=30):\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "    - loads rules from markdown file (ground truth)\n",
    "    - applies them to df\n",
    "    \"\"\"\n",
    "    rules = load_feature_rules(feature_table_path)\n",
    "    updated = apply_rules_to_dataframe(df, rules, horizon=horizon)\n",
    "    return updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d09123b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qc/49jwnztd1_n2k4yss03b1lkh0000gn/T/ipykernel_98756/1670282100.py:25: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/Trend-Surgeon/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'feature'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m horizon = \u001b[32m30\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m data = \u001b[43mapply_all_feature_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_table_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature_transformations.md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36mapply_all_feature_transformations\u001b[39m\u001b[34m(df, feature_table_path, horizon)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_all_feature_transformations\u001b[39m(df, feature_table_path=\u001b[33m\"\u001b[39m\u001b[33mfeature_transformations.md\u001b[39m\u001b[33m\"\u001b[39m, horizon=\u001b[32m30\u001b[39m):\n\u001b[32m    111\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    Full pipeline:\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    - loads rules from markdown file (ground truth)\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03m    - applies them to df\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     rules = \u001b[43mload_feature_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_table_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     updated = apply_rules_to_dataframe(df, rules, horizon=horizon)\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m updated\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mload_feature_rules\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     27\u001b[39m rules = {}\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df.iterrows():\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     feature = \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     30\u001b[39m     transformation = row[\u001b[33m\"\u001b[39m\u001b[33mtransformation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     31\u001b[39m     new_name = row[\u001b[33m\"\u001b[39m\u001b[33mnew_name\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/Trend-Surgeon/lib/python3.12/site-packages/pandas/core/series.py:1133\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/Trend-Surgeon/lib/python3.12/site-packages/pandas/core/series.py:1249\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1248\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/Trend-Surgeon/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'feature'"
     ]
    }
   ],
   "source": [
    "horizon = 30\n",
    "data = apply_all_feature_transformations(data, feature_table_path=\"feature_transformations.md\", horizon=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf9bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trend-Surgeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
